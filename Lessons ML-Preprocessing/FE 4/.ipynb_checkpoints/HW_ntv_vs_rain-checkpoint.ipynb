{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Сравнение интересов аудитории телеканалов НТВ и Дождь с помощью тематического моделирования LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Сравнить интересы аудитории телеканолов НТВ и Дождь с помощью методов тематического моделирования\n",
    "1. Получить данные по аудитории из социальной сети ВК\n",
    "2. Зарегистрировать приложение, получить app_id, access_token\n",
    "3. Скачать данные по пользователям в каждой из групп (id групп ВК даны ниже, tvrain_id, ntv_id)\n",
    "4. Взять небольшую выборку из каждой совокупности телезрителей(около 1000-2000 человек, т.к. 300k-400k слишком много), с которыми работать дальше\n",
    "5. Обучить LDA модель на их подписках\n",
    "6. По группам, на которые подписаны эти люди, полуичть ключевые слова групп, на которые они подписаны\n",
    "7. Получить распределение интересов людей для каждой совокупности, сравнить на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import sys  \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для использования VK API необходимо создать приложение в VK\n",
    "\n",
    "1. Создать приложение по адресу https://vk.com/apps?act=manage (кнопка \"создать приложение\")\n",
    "2. При создании указать название, описание (можно любые), категория  - прочее. Тип - standalone приложение\n",
    "3. В настройках получить **app_id**. App_id потребуется для получения access token\n",
    "4. Авторизовать пользователя (получить access token) можно по адресу: https://vk.com/dev/first_guide, в правилах нас интересует пункт 3 **Авторизация пользователя**\n",
    "5. После того, как ознакомитесь с авторизацией пользователя, скопируйте в адресную строку такой запрос https://oauth.vk.com/authorize?client_id=5490057&display=page&redirect_uri=https://oauth.vk.com/blank.html&scope=friends&response_type=token&v=5.52, где число **5490057** замените на число, которое получите для вашего **app_id**\n",
    "6. Нажмите Enter. Откроется окно с запросом прав. В нем отображаются название приложения, иконки прав доступа, и Ваши имя с фамилией. Нажмите «Разрешить». Вы попадете на новую страницу с предупреждением о том, что токен нельзя копировать и передавать третьим лицам. В адресной строке будет URL https://oauth.vk.com/blank.html, а после # Вы увидите дополнительные параметры — access_token, expires_in и user_id. Токен может выглядеть, например, так: 51eff86578a3bbbcb5c7043a122a69fd04dca057ac821dd7afd7c2d8e35b60172d45a26599c08034cc40a\n",
    "7. Токен — это Ваш ключ доступа. При выполнении определенных условий человек, получивший Ваш токен, может нанести существенный ущерб Вашим данным и данным других людей. Поэтому очень важно не передавать свой токен третьим лицам\n",
    "8. Поле expires_in содержит время жизни токена в секундах. 86400 секунд — это ровно сутки. Через сутки полученный токен перестанет действовать, для продолжения работы нужно будет получить новый по такому же алгоритму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own app id and respective tokens\n",
    "\n",
    "# скопируйте сюда ваши app_id и access_token, полученные по методу, описанному выше\n",
    "app_id = '6735892'\n",
    "access_token = '3c4afdb93c4afdb93c4afdb9923c2c35ad33c4a3c4afdb9679dad4e5f90156bab3ea737'\n",
    "\n",
    "# id групп ВК Дождя и НТВ\n",
    "tvrain_id = 17568841\n",
    "ntv_id = 28658784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Павел Дуров\n"
     ]
    }
   ],
   "source": [
    "# проверка работы API и авторизации пользователя. Если возникает ошибка, то, возможно, access token необходимо обновить\n",
    "check_id = 1\n",
    "\n",
    "# api call and test\n",
    "def vk_get_response(method, parameters, token):\n",
    "    url = 'https://api.vk.com/method/' + method + '?' + parameters + '&access_token=' + token\n",
    "#     print url\n",
    "    return(requests.get(url).json())\n",
    "\n",
    "answer = vk_get_response(\n",
    "    'users.get', 'user_ids={0}&v=4.9&lang=ru'.format(check_id), access_token\n",
    ")['response']\n",
    "print(answer[0]['first_name'], answer[0]['last_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение подписчиков телеканалов НТВ и Дождь в VK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим объекты, которые содержат всю информацию о подпиичиках соответствующих групп (указанных в domains) и сохраним их на диск. Получим в итоге два файла - **ntv_subs** и **tvrain_subs** в формате **.pkl** - питоновский формат хранения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset:  0\n",
      "Offset:  1000\n",
      "Offset:  2000\n",
      "Offset:  3000\n",
      "Offset:  4000\n",
      "Offset:  5000\n",
      "Offset:  6000\n",
      "Offset:  7000\n",
      "Offset:  8000\n",
      "Offset:  9000\n",
      "Offset:  10000\n",
      "Offset:  11000\n",
      "Offset:  12000\n",
      "Offset:  13000\n",
      "Offset:  14000\n",
      "Offset:  15000\n",
      "Offset:  16000\n",
      "Offset:  17000\n",
      "Offset:  18000\n",
      "Offset:  19000\n",
      "Offset:  20000\n",
      "Offset:  21000\n",
      "Offset:  22000\n",
      "Offset:  23000\n",
      "Offset:  24000\n",
      "Offset:  25000\n",
      "Offset:  26000\n",
      "Offset:  27000\n",
      "Offset:  28000\n",
      "Offset:  29000\n",
      "Offset:  30000\n",
      "Offset:  31000\n",
      "Offset:  32000\n",
      "Offset:  33000\n",
      "Offset:  34000\n",
      "Offset:  35000\n",
      "Offset:  36000\n",
      "Offset:  37000\n",
      "Offset:  38000\n",
      "Offset:  39000\n",
      "Offset:  40000\n",
      "Offset:  41000\n",
      "Offset:  42000\n",
      "Offset:  43000\n",
      "Offset:  44000\n",
      "Offset:  45000\n",
      "Offset:  46000\n",
      "Offset:  47000\n",
      "Offset:  48000\n",
      "Offset:  49000\n",
      "Offset:  50000\n",
      "Offset:  51000\n",
      "Offset:  52000\n",
      "Offset:  53000\n",
      "Offset:  54000\n",
      "Offset:  55000\n",
      "Offset:  56000\n",
      "Offset:  57000\n",
      "Offset:  58000\n",
      "Offset:  59000\n",
      "Offset:  60000\n",
      "Offset:  61000\n",
      "Offset:  62000\n",
      "Offset:  63000\n",
      "Offset:  64000\n",
      "Offset:  65000\n",
      "Offset:  66000\n",
      "Offset:  67000\n",
      "Offset:  68000\n",
      "Offset:  69000\n",
      "Offset:  70000\n",
      "Offset:  71000\n",
      "Offset:  72000\n",
      "Offset:  73000\n",
      "Offset:  74000\n",
      "Offset:  75000\n",
      "Offset:  76000\n",
      "Offset:  77000\n",
      "Offset:  78000\n",
      "Offset:  79000\n",
      "Offset:  80000\n",
      "Offset:  81000\n",
      "Offset:  82000\n",
      "Offset:  83000\n",
      "Offset:  84000\n",
      "Offset:  85000\n",
      "Offset:  86000\n",
      "Offset:  87000\n",
      "Offset:  88000\n",
      "Offset:  89000\n",
      "Offset:  90000\n",
      "Offset:  91000\n",
      "Offset:  92000\n",
      "Offset:  93000\n",
      "Offset:  94000\n",
      "Offset:  95000\n",
      "Offset:  96000\n",
      "Offset:  97000\n",
      "Offset:  98000\n",
      "Offset:  99000\n",
      "Offset:  100000\n",
      "Offset:  101000\n",
      "Offset:  102000\n",
      "Offset:  103000\n",
      "Offset:  104000\n",
      "Offset:  105000\n",
      "Offset:  106000\n",
      "Offset:  107000\n",
      "Offset:  108000\n",
      "Offset:  109000\n",
      "Offset:  110000\n",
      "Offset:  111000\n",
      "Offset:  112000\n",
      "Offset:  113000\n",
      "Offset:  114000\n",
      "Offset:  115000\n",
      "Offset:  116000\n",
      "Offset:  117000\n",
      "Offset:  118000\n",
      "Offset:  119000\n",
      "Offset:  120000\n",
      "Offset:  121000\n",
      "Offset:  122000\n",
      "Offset:  123000\n",
      "Offset:  124000\n",
      "Offset:  125000\n",
      "Offset:  126000\n",
      "Offset:  127000\n",
      "Offset:  128000\n",
      "Offset:  129000\n",
      "Offset:  130000\n",
      "Offset:  131000\n",
      "Offset:  132000\n",
      "Offset:  133000\n",
      "Offset:  134000\n",
      "Offset:  135000\n",
      "Offset:  136000\n",
      "Offset:  137000\n",
      "Offset:  138000\n",
      "Offset:  139000\n",
      "Offset:  140000\n",
      "Offset:  141000\n",
      "Offset:  142000\n",
      "Offset:  143000\n",
      "Offset:  144000\n",
      "Offset:  145000\n",
      "Offset:  146000\n",
      "Offset:  147000\n",
      "Offset:  148000\n",
      "Offset:  149000\n",
      "Offset:  150000\n",
      "Offset:  151000\n",
      "Offset:  152000\n",
      "Offset:  153000\n",
      "Offset:  154000\n",
      "Offset:  155000\n",
      "Offset:  156000\n",
      "Offset:  157000\n",
      "Offset:  158000\n",
      "Offset:  159000\n",
      "Offset:  160000\n",
      "Offset:  161000\n",
      "Offset:  162000\n",
      "Offset:  163000\n",
      "Offset:  164000\n",
      "Offset:  165000\n",
      "Offset:  166000\n",
      "Offset:  167000\n",
      "Offset:  168000\n",
      "Offset:  169000\n",
      "Offset:  170000\n",
      "Offset:  171000\n",
      "Offset:  172000\n",
      "Offset:  173000\n",
      "Offset:  174000\n",
      "Offset:  175000\n",
      "Offset:  176000\n",
      "Offset:  177000\n",
      "Offset:  178000\n",
      "Offset:  179000\n",
      "Offset:  180000\n",
      "Offset:  181000\n",
      "Offset:  182000\n",
      "Offset:  183000\n",
      "Offset:  184000\n",
      "Offset:  185000\n",
      "Offset:  186000\n",
      "Offset:  187000\n",
      "Offset:  188000\n",
      "Offset:  189000\n",
      "Offset:  190000\n",
      "Offset:  191000\n",
      "Offset:  192000\n",
      "Offset:  193000\n",
      "Offset:  194000\n",
      "Offset:  195000\n",
      "Offset:  196000\n",
      "Offset:  197000\n",
      "Offset:  198000\n",
      "Offset:  199000\n",
      "Offset:  200000\n",
      "Offset:  201000\n",
      "Offset:  202000\n",
      "Offset:  203000\n",
      "Offset:  204000\n",
      "Offset:  205000\n",
      "Offset:  206000\n",
      "Offset:  207000\n",
      "Offset:  208000\n",
      "Offset:  209000\n",
      "Offset:  210000\n",
      "Offset:  211000\n",
      "Offset:  212000\n",
      "Offset:  213000\n",
      "Offset:  214000\n",
      "Offset:  215000\n",
      "Offset:  216000\n",
      "Offset:  217000\n",
      "Offset:  218000\n",
      "Offset:  219000\n",
      "Offset:  220000\n",
      "Offset:  221000\n",
      "Offset:  222000\n",
      "Offset:  223000\n",
      "Offset:  224000\n",
      "Offset:  225000\n",
      "Offset:  226000\n",
      "Offset:  227000  Error\n",
      "Offset:  228000\n",
      "Offset:  229000\n",
      "Offset:  230000\n"
     ]
    }
   ],
   "source": [
    "domains = ['ntv', 'tvrain']\n",
    "\n",
    "\n",
    "for group_domain in domains:\n",
    "    offset = 0\n",
    "    group_id = group_domain\n",
    "    fields = \"\"\"sex,bdate,city,country,home_town,lists,domain,has_mobile,\n",
    "    contacts,connections,education,universities,followers_count,occupation,last_seen,relation\"\"\"\n",
    "    first_sample = vk_get_response(\n",
    "        'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "            group_id, offset, fields\n",
    "        ), token=access_token\n",
    "    )\n",
    "    community_count = first_sample['response']['count']\n",
    "    community_members = []\n",
    "    for i in range(community_count // 1000 + 1):\n",
    "        offset = i * 1000\n",
    "        try:\n",
    "            answer = vk_get_response(\n",
    "                'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "                    group_id, offset, fields), token=access_token\n",
    "            )\n",
    "            print(\"Offset: \", offset)\n",
    "        except:\n",
    "            print(\"Offset: \", offset, \" Error\")\n",
    "        community_members += answer['response']['users']\n",
    "    save_obj(community_members, '{}_subs'.format(group_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv = load_obj('ntv_subs')\n",
    "community_tvrain = load_obj('tvrain_subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv_df = pd.DataFrame(community_ntv)\n",
    "community_tvrain_df = pd.DataFrame(community_tvrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала получим всех уникальных подписчиков НТВ и Дождя с помощью unique. Далее с помощью numpy.random необходимо выбрать небольшой sample (например, по 1000 из каждой группы) таких людей и объединить их вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntv_uids = community_ntv_df.uid.unique().tolist()\n",
    "tvrain_uids = community_tvrain_df.uid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить общий список людей из двух выборок НТВ и Дождя, всего должно быть в итоге около 2000 человек\n",
    "uids = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить подписки этих пользователей\n",
    "print_counter = 0\n",
    "final_data = []\n",
    "\n",
    "for uid in uids[:1000]:\n",
    "    try:\n",
    "        user_subs = vk_get_response(\n",
    "            'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "        )\n",
    "        time.sleep(0.3)\n",
    "        final_data.append(user_subs)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    if print_counter % 100 == 0:\n",
    "        print(\"{0} profiles done\".format(print_counter))\n",
    "    print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_list = []\n",
    "groups_freq_dict = {}\n",
    "top_n = 5\n",
    "\n",
    "for record, uid in zip(final_data, uids):\n",
    "    try:\n",
    "        user_subs = record\n",
    "        if not user_subs.get('response'):\n",
    "            user_subs = vk_get_response(\n",
    "                'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "            )\n",
    "        subs_pd = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    'groups_count': user_subs['response']['groups'].get('count'),\n",
    "                    'groups_list': user_subs['response']['groups'].get('items'),\n",
    "                    'follows_count':user_subs['response']['users'].get('count'),\n",
    "                    'follows_list': user_subs['response']['users'].get('items'),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for group_id in user_subs['response']['groups'].get('items')[:top_n]:\n",
    "            if groups_freq_dict.get(group_id):\n",
    "                groups_freq_dict[group_id] += 1\n",
    "            else:\n",
    "                groups_freq_dict[group_id] = 1\n",
    "\n",
    "        subs_pd['subs_count'] = subs_pd['groups_count'] + subs_pd['follows_count']\n",
    "        subs_list.append(subs_pd)\n",
    "    except:\n",
    "#         print(user_subs)\n",
    "        pass\n",
    "    if len(subs_list) % 100 == 0:\n",
    "        print(\"Processed {0} users\".format(len(subs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые популярные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(key, val) for key, val in groups_freq_dict.items()], key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка постов со стен групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_doc_dict = {}\n",
    "counter = 0\n",
    "groups_freq_dict_top5 = groups_freq_dict\n",
    "\n",
    "for group_id, freq in groups_freq_dict_top5.items():\n",
    "    counter += 1\n",
    "    try:\n",
    "        check = vk_get_response(\n",
    "            'wall.get',\n",
    "            'owner_id={0}&count=100&fields=post_type,marked_as_ads&&v=4.9&lang=ru'.format(int(group_id) * -1),\n",
    "            access_token_3\n",
    "        )\n",
    "        check = check['response']\n",
    "        group_doc = ''\n",
    "        if check[0] != 0:\n",
    "            for post in check[1:]:\n",
    "                if post.get('marked_as_ads') != 1:\n",
    "                    group_doc += post['text']\n",
    "        group_doc_dict[group_id] = group_doc\n",
    "    except:\n",
    "        print(\"Response error. Group id {0}\".format(group_id))\n",
    "        print(check)\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} groups extracted\".format(counter))\n",
    "    time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить сырые данные по постам групп на диск\n",
    "save_obj(group_doc_dict, 'group_doc_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs_to_delete = string.punctuation + u'»' + u'«' + u'—' + u'“' + u'„' + u'•' + u'#'\n",
    "translation_table = {ord(c): None for c in chrs_to_delete if c != u'-'}\n",
    "units = MorphAnalyzer.DEFAULT_UNITS\n",
    "morph = MorphAnalyzer(result_type=None, units=units)\n",
    "PortSt = PorterStemmer()\n",
    "stopw = set(\n",
    "    [w for w in stopwords.words(['russian', 'english'])]\n",
    "    + [u'это', u'году', u'года', u'также', u'етот',\n",
    "       u'которые', u'который', u'которая', u'поэтому',\n",
    "       u'весь', u'свой', u'мочь', u'eтот', u'например',\n",
    "       u'какой-то', u'кто-то', u'самый', u'очень', u'несколько',\n",
    "       u'источник', u'стать', u'время', u'пока', u'однако',\n",
    "       u'около', u'немного', u'кроме', u'гораздо', u'каждый',\n",
    "       u'первый', u'вполне', u'из-за', u'из-под',\n",
    "       u'второй', u'нужно', u'нужный', u'просто', u'большой',\n",
    "       u'хороший', u'хотеть', u'начать', u'должный', u'новый', u'день',\n",
    "       u'метр', u'получить', u'далее', u'именно', u'апрель',\n",
    "       u'сообщать', u'разный', u'говорить', u'делать',\n",
    "       u'появиться', u'2016',\n",
    "       u'2015', u'получить', u'иметь', u'составить', u'дать', u'читать',\n",
    "       u'ничто', u'достаточно', u'использовать',\n",
    "       u'принять', u'практически',\n",
    "       u'находиться', u'месяц', u'достаточно', u'что-то', u'часто',\n",
    "       u'хотеть', u'начаться', u'делать', u'событие', u'составлять',\n",
    "       u'остаться', u'заявить', u'сделать', u'дело',\n",
    "       u'примерно', u'попасть', u'хотя', u'лишь', u'первое',\n",
    "       u'больший', u'решить', u'число', u'идти', u'давать', u'вопрос',\n",
    "       u'сегодня', u'часть', u'высокий', u'главный', u'случай', u'место',\n",
    "       u'конец', u'работать', u'работа', u'слово', u'важный', u'сказать']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_start = 'http[s]?://'\n",
    "url_end = (\n",
    "    '(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "pattern = url_start + url_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка слов постов групп - трансформация в \"хороший\" вид. Нормализация и стэмминг, удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_clean_doc_dict = {}\n",
    "counter = 0\n",
    "\n",
    "for group_id, doc in group_doc_dict.items():\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    body = ' '.join(\n",
    "        [tag.string.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "         for tag in soup.descendants if tag.string]\n",
    "    )\n",
    "    body = re.sub('\\[.*?\\]','', body)\n",
    "    body = re.sub(pattern,'', body)\n",
    "    if body != '':\n",
    "        body_clean = body.translate(translation_table).lower().strip()\n",
    "        words = word_tokenize(body_clean)\n",
    "        tokens = []\n",
    "        # stemming and text normalization\n",
    "        for word in words:\n",
    "            if re.match('^[a-z0-9-]+$', word) is not None:\n",
    "                tokens.append(PortSt.stem(word))\n",
    "            elif word.count('-') > 1:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                normal_forms = morph.normal_forms(word)\n",
    "                tokens.append(normal_forms[0] if normal_forms else word)\n",
    "        # remove stopwords and leave unique words only\n",
    "        tokens = filter(\n",
    "            lambda token: token not in stopw, sorted(set(tokens))\n",
    "        )\n",
    "\n",
    "        # remove all words with more than 3 chars\n",
    "        tokens = filter(lambda token: len(token) > 3, tokens)\n",
    "    else:\n",
    "        tokens = []\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} docs processed\".format(counter))\n",
    "    group_clean_doc_dict[group_id] = tokens\n",
    "\n",
    "group_clean_doc_dict = {key: list(val) for key, val in group_clean_doc_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить обработанные данные на диск\n",
    "save_obj(group_clean_doc_dict, 'group_doc_dict_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение LDA модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import TextCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "class ListTextCorpus(TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        for doc in self.input:\n",
    "            yield doc\n",
    "                \n",
    "mycorp = ListTextCorpus(input=group_clean_doc_dict.values())\n",
    "justlda = LdaModel(\n",
    "    corpus=mycorp, num_topics=20, passes=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LdaModel performance')\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dict = {key: 0 for key in range(20)}\n",
    "\n",
    "group_topics_dict_20 = {\n",
    "    group_id: dict(list(dummy_dict.items()) + justlda.get_document_topics(mycorp.dictionary.doc2bow(text)))\n",
    "    for group_id, text in group_clean_doc_dict.items()\n",
    "}\n",
    "check_pd_20 = pd.DataFrame.from_dict(group_topics_dict_20, orient='index')\n",
    "check_pd_20.head(10)\n",
    "print(\"Group distribution by the most relevant topic\")\n",
    "pd.Series.round(check_pd_20.idxmax(axis=1).value_counts() * 1. / len(check_pd_20), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump lda model to disk\n",
    "justlda.save('ldamodel_20_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most typical groups for every topic\")\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))\n",
    "    typical_groups = check_pd_20[i].sort_values(ascending=False).index[:10]\n",
    "    for g in typical_groups:\n",
    "        group_info = vk_get_response(\n",
    "            'groups.getById', 'group_ids={0}&v=4.9&lang=ru'.format(g), access_token\n",
    "        )\n",
    "        print(group_info['response'][0]['name'] + ' ' + 'http://vk.com/club' + str(g))\n",
    "        time.sleep(0.3)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
