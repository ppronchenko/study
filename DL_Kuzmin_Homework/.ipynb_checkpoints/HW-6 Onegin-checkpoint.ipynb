{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('onegin.txt', 'r') as f:\n",
    "    d = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneg = d.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "count = 0\n",
    "for i in oneg:\n",
    "    if i[:2] == '\\t\\t' and i[:4] != '\\t\\t……':\n",
    "        a.append(i[2:])\n",
    "    count += 1\n",
    "    if count % 4 == 0 and a != []:\n",
    "        b.append(' '.join(a))\n",
    "        a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [x.lower() for x in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he мысля гордый свет забавить, вниманье дружбы возлюбя, хотел бы я тебе представить',\n",
       " 'залог достойнее тебя, достойнее души прекрасной, святой исполненной мечты, поэзии живой и ясной,',\n",
       " 'высоких дум и простоты; но так и быть – рукой пристрастной прими собранье пёстрых глав, полусмешных, полупечальных,',\n",
       " 'простонародных, идеальных, небрежный плод моих забав, бессонниц, лёгких вдохновений, незрелых и увядших лет,',\n",
       " 'ума холодных наблюдений и сердца горестных замет.',\n",
       " '«мой дядя самых честных правил, когда не в шутку занемог, он уважать себя заставил и лучше выдумать не мог.',\n",
       " 'его пример другим наука; но, боже мой, какая скука с больным сидеть и день и ночь, не отходя ни шагу прочь!',\n",
       " 'какое низкое коварство полуживого забавлять, ему подушки поправлять, печально подносить лекарство,',\n",
       " 'вздыхать и думать про себя: когда же чёрт возьмёт тебя!»',\n",
       " 'так думал молодой повеса, летя в пыли на почтовых,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('абвгдеёжзийклмнопрстуфхцчшщыьъэюя.,?! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = []\n",
    "for i in text:\n",
    "    line = ''\n",
    "    for j in i:\n",
    "        if j in CHARS:\n",
    "            line = line+j \n",
    "    text2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' мысля гордый свет забавить, вниманье дружбы возлюбя, хотел бы я тебе представить',\n",
       " 'залог достойнее тебя, достойнее души прекрасной, святой исполненной мечты, поэзии живой и ясной,',\n",
       " 'высоких дум и простоты но так и быть  рукой пристрастной прими собранье пёстрых глав, полусмешных, полупечальных,',\n",
       " 'простонародных, идеальных, небрежный плод моих забав, бессонниц, лёгких вдохновений, незрелых и увядших лет,',\n",
       " 'ума холодных наблюдений и сердца горестных замет.',\n",
       " 'мой дядя самых честных правил, когда не в шутку занемог, он уважать себя заставил и лучше выдумать не мог.',\n",
       " 'его пример другим наука но, боже мой, какая скука с больным сидеть и день и ночь, не отходя ни шагу прочь!',\n",
       " 'какое низкое коварство полуживого забавлять, ему подушки поправлять, печально подносить лекарство,',\n",
       " 'вздыхать и думать про себя когда же чёрт возьмёт тебя!',\n",
       " 'так думал молодой повеса, летя в пыли на почтовых,']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'none': 0,\n",
       " 'к': 1,\n",
       " 'ь': 2,\n",
       " 'э': 3,\n",
       " 'ж': 4,\n",
       " 'ё': 5,\n",
       " 'б': 6,\n",
       " 'р': 7,\n",
       " '?': 8,\n",
       " 'м': 9,\n",
       " ',': 10,\n",
       " 'в': 11,\n",
       " 'г': 12,\n",
       " 'л': 13,\n",
       " 'щ': 14,\n",
       " 'з': 15,\n",
       " 'ф': 16,\n",
       " 'и': 17,\n",
       " 'ъ': 18,\n",
       " 'ы': 19,\n",
       " '.': 20,\n",
       " 'п': 21,\n",
       " 'с': 22,\n",
       " 'у': 23,\n",
       " 'х': 24,\n",
       " ' ': 25,\n",
       " 'ц': 26,\n",
       " 'д': 27,\n",
       " 'ю': 28,\n",
       " 'ч': 29,\n",
       " 'а': 30,\n",
       " 'й': 31,\n",
       " 'о': 32,\n",
       " 'е': 33,\n",
       " 'н': 34,\n",
       " 'я': 35,\n",
       " '!': 36,\n",
       " 'ш': 37,\n",
       " 'т': 38}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((len(text2), MAX_LEN), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text2)):\n",
    "    for j, w in enumerate(text2[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25,  9, 19, 22, 13, 35, 25, 12, 32,  7, 27, 19, 31, 25, 22, 11, 33, 38,\n",
       "         25, 15, 30,  6, 30, 11, 17, 38,  2, 10, 25, 11, 34, 17,  9, 30, 34,  2,\n",
       "         33, 25, 27,  7, 23,  4,  6, 19, 25, 11, 32, 15, 13, 28]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.word_embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
    "        self.gru = torch.nn.RNN(28, 128, batch_first=True)\n",
    "        self.hidden2tag = torch.nn.Linear(128, len(INDEX_TO_CHAR))\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state\n",
    "\n",
    "    def forward_state(self, sentences, state):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds, state)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 39])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X[0:1])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence():\n",
    "    sentence = ['м', 'о', 'й', ' ', 'д', 'я', 'д', 'я']\n",
    "    state = None\n",
    "    for i in range(MAX_LEN):\n",
    "        X = torch.Tensor([[CHAR_TO_INDEX[sentence[i]]]]).type(torch.long).to(dev)\n",
    "        if i == 0:\n",
    "            result, state = model.forward(X)\n",
    "        else:\n",
    "            result, state = model.forward_state(X, state)\n",
    "        prediction = result[0, -1, :]\n",
    "        index_of_prediction = prediction.argmax()\n",
    "        if i >= len(sentence) - 1:\n",
    "            if index_of_prediction == 0:\n",
    "                break\n",
    "        sentence.append(INDEX_TO_CHAR[index_of_prediction])\n",
    "    print(''.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мой дядя иьжэц лъшы!ьнъ.ъъцяъсь ьёжлёбъяъфб,дсььъ.с!эйп!ё \n"
     ]
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.005, momentum=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.030, Train loss: 3.669\n",
      "мой дядя иьжэц лъшы!ьнъ.ъъцяъсь ьёжлёбъяъфб,дсььъ.с!эйп!ё \n",
      "Epoch 100. Time: 0.028, Train loss: 2.676\n",
      "мой дядя   но   ноое ноое   но   ноое ноое   но   ноое ноо\n",
      "Epoch 200. Time: 0.028, Train loss: 2.522\n",
      "мой дядя   по   нннонпннее        нонннннне е      н пнннн\n",
      "Epoch 300. Time: 0.027, Train loss: 2.441\n",
      "мой дядя   по   ннпонпнпее  о и   пннп пнпоеа пое    нон н\n",
      "Epoch 400. Time: 0.028, Train loss: 2.388\n",
      "мой дядя   пе   ннпоспппее  тооон пво    воонппппот о ооон\n",
      "Epoch 500. Time: 0.012, Train loss: 2.348\n",
      "мой дядяо  пе    нпосппппеонтоооосно     то нппппо паоооон\n",
      "Epoch 600. Time: 0.028, Train loss: 2.317\n",
      "мой дядяо  пе    нпосппппеонтоооотны      ы ппппппепоооооо\n",
      "Epoch 700. Time: 0.013, Train loss: 2.290\n",
      "мой дядяо  пе    нпррппппеоиаоооотн       ывппппппе оооооо\n",
      "Epoch 800. Time: 0.013, Train loss: 2.267\n",
      "мой дядяо  пе    нпррппппеоиаоооотн       ывппппппе оооооо\n",
      "Epoch 900. Time: 0.023, Train loss: 2.247\n",
      "мой дядяо  пе    нвррпвппеоеаоооотн       ыввпппппл еооооо\n",
      "Epoch 1000. Time: 0.027, Train loss: 2.228\n",
      "мой дядяо  пе    нвррпвппаоеаоооолн       ивсппппп етооооо\n",
      "Epoch 1100. Time: 0.027, Train loss: 2.210\n",
      "мой дядяо  пер   нврраввпаоеанооо н      напвппппе оооооот\n",
      "Epoch 1200. Time: 0.028, Train loss: 2.194\n",
      "мой дядяо  пер   нврраввпаоеанооолн       ивсппппп ееооооо\n",
      "Epoch 1300. Time: 0.027, Train loss: 2.178\n",
      "мой дядяо  пере  нврраннваоеаниыо        пвппппппреоооооои\n",
      "Epoch 1400. Time: 0.028, Train loss: 2.163\n",
      "мой дядяо  пете  нвррь нваоеа оеол   пн   сспроввптеоивоео\n",
      "Epoch 1500. Time: 0.028, Train loss: 2.148\n",
      "мой дядяо  пртет нврооньмеоей        свпппппптеооооооот   \n",
      "Epoch 1600. Time: 0.028, Train loss: 2.134\n",
      "мой дядяо  пртет нврооньмеоей        свпппппптеооооооом   \n",
      "Epoch 1700. Time: 0.028, Train loss: 2.121\n",
      "мой дядяо  пртет вврооньм еев    н   нвппапвпаооолоеолн   \n",
      "Epoch 1800. Time: 0.028, Train loss: 2.108\n",
      "мой дядяо  пртет вврооньм еев    н   нвппапвпаооолоеолн   \n",
      "Epoch 1900. Time: 0.031, Train loss: 2.095\n",
      "мой дядяо  пртет вврооньм еев    н   нвппапвпаооолоеолн   \n",
      "Epoch 2000. Time: 0.027, Train loss: 2.083\n",
      "мой дядяо  пртет вврооньм еевн   н   оввпапвпноеолоеоий   \n",
      "Epoch 2100. Time: 0.028, Train loss: 2.071\n",
      "мой дядяо  пртет вврооньн еевн  ов   оввн свпноеовеооий   \n",
      "Epoch 2200. Time: 0.028, Train loss: 2.059\n",
      "мой дядяо  пртет вврооньн еевн  ов   оввн свпноеовеоеий   \n",
      "Epoch 2300. Time: 0.028, Train loss: 2.048\n",
      "мой дядяон пртет  вооон нвссн  пеоееоввррнт н ееоыовепг   \n",
      "Epoch 2400. Time: 0.027, Train loss: 2.037\n",
      "мой дядяон пртетн вооон ыв снн п  стоовопвворно оиейаыйвн \n",
      "Epoch 2500. Time: 0.028, Train loss: 2.026\n",
      "мой дядяон пртетн вооон ыв снн п  стоовопвворно оиейоыйвн \n",
      "Epoch 2600. Time: 0.028, Train loss: 2.015\n",
      "мой дядяон пртетн вооон ыв снн п  стоовопвворно оиейоыйвн \n",
      "Epoch 2700. Time: 0.028, Train loss: 2.005\n",
      "мой дядяон пртетн вооон ыв снн п  своовопввосно оиейтойвн \n",
      "Epoch 2800. Time: 0.028, Train loss: 1.994\n",
      "мой дядяон пртетн вооон ыв снн п  своовопввосно оиейтойвн \n",
      "Epoch 2900. Time: 0.027, Train loss: 1.984\n",
      "мой дядяен пртет ьвооон н е н  пепгпапврррие оееаайнсн ннт\n",
      "Epoch 3000. Time: 0.027, Train loss: 1.974\n",
      "мой дядяен сртет ьв аоньн еснде ов тоа пн сомнпоовтвеоевн \n",
      "Epoch 3100. Time: 0.028, Train loss: 1.964\n",
      "мой дядяен сртет ьв аоньн еснде ов тоа пн сомнпоовтвеоевн \n",
      "Epoch 3200. Time: 0.027, Train loss: 1.954\n",
      "мой дядяен сртет ьв аоньн еснде ов тоа пненомнноогим оыйно\n",
      "Epoch 3300. Time: 0.027, Train loss: 1.944\n",
      "мой дядяен сртет ьв аоньн еснде овгтоа пнеоа  ноогнннве но\n",
      "Epoch 3400. Time: 0.027, Train loss: 1.934\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 3500. Time: 0.028, Train loss: 1.924\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 3600. Time: 0.028, Train loss: 1.915\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 3700. Time: 0.028, Train loss: 1.905\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 3800. Time: 0.027, Train loss: 1.895\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 3900. Time: 0.027, Train loss: 1.886\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 4000. Time: 0.027, Train loss: 1.876\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 4100. Time: 0.028, Train loss: 1.867\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4200. Time: 0.028, Train loss: 1.857\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4300. Time: 0.028, Train loss: 1.848\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4400. Time: 0.028, Train loss: 1.838\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4500. Time: 0.028, Train loss: 1.829\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4600. Time: 0.028, Train loss: 1.819\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4700. Time: 0.028, Train loss: 1.810\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4800. Time: 0.028, Train loss: 1.800\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 4900. Time: 0.027, Train loss: 1.791\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеовепганнн \n",
      "Epoch 5000. Time: 0.028, Train loss: 1.781\n",
      "мой дядяей сртет  илаоньнв   н  оеоввапвн н елиеевапганн  \n",
      "Epoch 5100. Time: 0.013, Train loss: 1.772\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 5200. Time: 0.013, Train loss: 1.763\n",
      "мой дядяей сртет  илаоньнв   н  оеовпапвн н елиеовепганнн \n",
      "Epoch 5300. Time: 0.013, Train loss: 1.753\n",
      "мой дядяей сртет  илаоньнв      оеовппппн н ееееывепг  н, \n",
      "Epoch 5400. Time: 0.012, Train loss: 1.744\n",
      "мой дядяей сртет  илаоньнв      оеовппппн н ееееывепг  н, \n",
      "Epoch 5500. Time: 0.014, Train loss: 1.734\n",
      "мой дядяей сртет  илаоньнв      оеовппппн н ееееывепг  н, \n",
      "Epoch 5600. Time: 0.013, Train loss: 1.724\n",
      "мой дядяей сртет, илаонь и ы    и пхпвпп поариееволтутн   \n",
      "Epoch 5700. Time: 0.013, Train loss: 1.715\n",
      "мой дядяей сетет, илрь   и ыа нвв пх паеепоосортнрннт ааое\n",
      "Epoch 5800. Time: 0.013, Train loss: 1.705\n",
      "мой дядяей петет, ирсе   и аквтвс пнааееявоо ттт    нао нв\n",
      "Epoch 5900. Time: 0.027, Train loss: 1.695\n",
      "мой дядяей петет, ирсе   и аквтвс пнааееяооо ттт н  ноо не\n",
      "Epoch 6000. Time: 0.028, Train loss: 1.686\n",
      "мой дядяей петет, ирсе   и аквтвс пнааееяооо ттт н  ноо не\n",
      "Epoch 6100. Time: 0.028, Train loss: 1.676\n",
      "мой дядяей петет, ирсе   и аквтвс пнааееяооо ттт н  ноо не\n",
      "Epoch 6200. Time: 0.028, Train loss: 1.666\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяо\n",
      "Epoch 6300. Time: 0.012, Train loss: 1.656\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяо\n",
      "Epoch 6400. Time: 0.013, Train loss: 1.646\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяы\n",
      "Epoch 6500. Time: 0.013, Train loss: 1.636\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяы\n",
      "Epoch 6600. Time: 0.012, Train loss: 1.626\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяы\n",
      "Epoch 6700. Time: 0.028, Train loss: 1.615\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяы\n",
      "Epoch 6800. Time: 0.013, Train loss: 1.605\n",
      "мой дядяей петет, ирсь   и ак нвс пннсеоеоооомдтлнннннааяы\n",
      "Epoch 6900. Time: 0.013, Train loss: 1.595\n",
      "мой дядяей петет, ирсь   и ав нвс пннпаоеооооорнннннннаооо\n",
      "Epoch 7000. Time: 0.013, Train loss: 1.584\n",
      "мой дядяей петет, ирс    и аввупп пнне ревооогтасолнноалтн\n",
      "Epoch 7100. Time: 0.012, Train loss: 1.574\n",
      "мой дядяей петет, ирс    изаввупп рние ревао  тасалнссок л\n",
      "Epoch 7200. Time: 0.013, Train loss: 1.563\n",
      "мой дядяеи петет,  оср   ивнтанвп  ео иооовм п ннноеиовеео\n",
      "Epoch 7300. Time: 0.013, Train loss: 1.553\n",
      "мой дядяеи петет,  оср   ивнтанвп  ео иооовм п ннноеиовеео\n",
      "Epoch 7400. Time: 0.014, Train loss: 1.542\n",
      "мой дядяеи сетет,   р    нвпонвппеоев  оосн  ив нтоов  пао\n",
      "Epoch 7500. Time: 0.013, Train loss: 1.531\n",
      "мой дядяеи сетет,   р    нвпонвппеоев  оосн  ив нтоов  пао\n",
      "Epoch 7600. Time: 0.028, Train loss: 1.520\n",
      "мой дядяеи сетет,   р    нвпонсппеорв  оесн  пвнгтиивриеоа\n",
      "Epoch 7700. Time: 0.013, Train loss: 1.509\n",
      "мой дядяеи сетет,   р    нвпонсппеорв  оесн  пвнгтиивриеоа\n",
      "Epoch 7800. Time: 0.014, Train loss: 1.498\n",
      "мой дядяеи сетет,   р    нвпонсппеорв  оесн  пвнгтиивриеоа\n",
      "Epoch 7900. Time: 0.013, Train loss: 1.487\n",
      "мой дядяеи сетет,   р    нвпонсппеорв  оесн  пвбгтиивриеоа\n",
      "Epoch 8000. Time: 0.012, Train loss: 1.476\n",
      "мой дядяеи сетет,   р    нвпонсппеорв  оесн  пвбгтиивриеоа\n",
      "Epoch 8100. Time: 0.012, Train loss: 1.464\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуаопннгостдлооов\n",
      "Epoch 8200. Time: 0.013, Train loss: 1.453\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуаопннгостдлооов\n",
      "Epoch 8300. Time: 0.015, Train loss: 1.441\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуаопннгистдлооов\n",
      "Epoch 8400. Time: 0.013, Train loss: 1.430\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуаопннгистдлооов\n",
      "Epoch 8500. Time: 0.013, Train loss: 1.418\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуаопннгистдлооов\n",
      "Epoch 8600. Time: 0.012, Train loss: 1.406\n",
      "мой дядяеи сутет,   дь   ивпоснвс  рв аоевуеопннгисннлооов\n",
      "Epoch 8700. Time: 0.013, Train loss: 1.395\n",
      "мой дядяеи суте,,   дь   идпеунвс  ер ы евур вхпс   сооеяв\n",
      "Epoch 8800. Time: 0.014, Train loss: 1.383\n",
      "мой дядяеи суте,,   дь   идпеунвс  ер ы евур вхпс   сооеяв\n",
      "Epoch 8900. Time: 0.013, Train loss: 1.371\n",
      "мой дядяеи суте,,   дь   идпеунвс  ер е евур вёпеокес зерт\n",
      "Epoch 9000. Time: 0.012, Train loss: 1.359\n",
      "мой дядяеи суто,,   дь   нвпоунвуеорв ы      охпмппппнориа\n",
      "Epoch 9100. Time: 0.013, Train loss: 1.347\n",
      "мой дядяеи суто,,   дь   нвпоунвуеорв ы      охпмппппбориа\n",
      "Epoch 9200. Time: 0.014, Train loss: 1.335\n",
      "мой дядяеи суто,,   дь   ивпоунву  рв ыо ппоов нвресм оаоо\n",
      "Epoch 9300. Time: 0.013, Train loss: 1.323\n",
      "мой дядяеи суто,,   дь   ивпоунву  рв ыо ппоов нвресм оаоо\n",
      "Epoch 9400. Time: 0.013, Train loss: 1.311\n",
      "мой дядяеи суто,,   дь   ивпоунв   рв ыоувпаоихн ьрлм, оие\n",
      "Epoch 9500. Time: 0.028, Train loss: 1.298\n",
      "мой дядяеи суто,,   дь   ивпоунв   рв ыоувпаои н ьрлм,сепе\n",
      "Epoch 9600. Time: 0.028, Train loss: 1.286\n",
      "мой дядяеи суто,,   др   ивпооувп  евн  оивм опв  оеитеовв\n",
      "Epoch 9700. Time: 0.028, Train loss: 1.274\n",
      "мой дядяеи суто,,   др   ивпооувп  евн  оивм опв  оеитоовм\n",
      "Epoch 9800. Time: 0.015, Train loss: 1.261\n",
      "мой дядяеи суто,,   др   ивпооувп  евн  оивм опв  оеитеовм\n",
      "Epoch 9900. Time: 0.013, Train loss: 1.249\n",
      "мой дядяеи суто,,   др   ивпооувп  евн  оивм опв  оеитеовм\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10000):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "        X_batch = X_batch.to(dev)\n",
    "        Y_batch = Y_batch.to(dev)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers, _ = model.forward(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    \n",
    "    if ep % 100 == 0:\n",
    "        print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "        generate_sentence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.word_embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
    "        self.gru = torch.nn.GRU(28, 128, batch_first=True)\n",
    "        self.hidden2tag = torch.nn.Linear(128, len(INDEX_TO_CHAR))\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state\n",
    "\n",
    "    def forward_state(self, sentences, state):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds, state)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(epocha):\n",
    "    for ep in range(epocha):\n",
    "        start = time.time()\n",
    "        train_loss = 0.\n",
    "        train_passed = 0\n",
    "\n",
    "        for i in range(int(len(X) / 100)):\n",
    "            batch = X[i * 100:(i + 1) * 100]\n",
    "            X_batch = batch[:, :-1]\n",
    "            Y_batch = batch[:, 1:].flatten()\n",
    "            X_batch = X_batch.to(dev)\n",
    "            Y_batch = Y_batch.to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            answers, _ = model.forward(X_batch)\n",
    "            answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "            loss = criterion(answers, Y_batch)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_passed += 1\n",
    "\n",
    "\n",
    "        if ep % 100 == 0:\n",
    "            print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "            generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.035, Train loss: 0.080\n",
      "мой дядяои дядя н иетн сасх рие л  доктсопнллооиюриыы нд ы\n",
      "Epoch 100. Time: 0.033, Train loss: 0.068\n",
      "мой дядяеи дядя с кедятстмннилеьвиыо   нетннкзпо  ыоооствс\n",
      "Epoch 200. Time: 0.034, Train loss: 0.068\n",
      "мой дядяеи дядя с кедятсамлрилея июякаттн    ,еаыдпл  здна\n",
      "Epoch 300. Time: 0.032, Train loss: 0.067\n",
      "мой дядяеи дядя с кедятсамлрилея июокаттн ббр ьаиолеан ж н\n",
      "Epoch 400. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя с кедятсамлрилея июякаттн б  ,еаыдеэс злна\n",
      "Epoch 500. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя с кедятсамлрилея июякаттн     еаыдплмм,л н\n",
      "Epoch 600. Time: 0.033, Train loss: 0.103\n",
      "мой дядяои дядя т неся соие л иобм суннкыууудоыахд  екбл и\n",
      "Epoch 700. Time: 0.033, Train loss: 0.068\n",
      "мой дядяои дядя т неси сома ллив албео аоньоннкл а .аыояе \n",
      "Epoch 800. Time: 0.033, Train loss: 0.067\n",
      "мой дядяеи дядя с кедятстжлрисеьвеюоккймат  ,о   улп квббм\n",
      "Epoch 900. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя с кедятстжлрисеьвеюоккй,ат  ,о   е п нвпбв\n",
      "Epoch 1000. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя с кедятстжлрисеьвеюоккй,ат  ,о   е п нвнбв\n",
      "Epoch 1100. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя с кедятстжлрисеьвеюоккй,ат   о   е плбв бё\n",
      "Epoch 1200. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя с кедятстмлрисеьвеюоккй,ай        кдплддбб\n",
      "Epoch 1300. Time: 0.033, Train loss: 0.069\n",
      "мой дядяеи дядя   бедя,сббрзе  клаожтнноонннаоыйбаие сн нн\n",
      "Epoch 1400. Time: 0.033, Train loss: 0.067\n",
      "мой дядяеи дядя   беди,слбрзе  кииоатнсрхнннрооооиуоу нббх\n",
      "Epoch 1500. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя   беди,слбрзе  киаоатнсрхнннрыяооииоы   н \n",
      "Epoch 1600. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя   беди,слбрзе  киаоатнсрхнннрыяооииое   н \n",
      "Epoch 1700. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кеди,слбрпин кииоо ьтурдхбдцр пыоеаауоол\n",
      "Epoch 1800. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кеди,слбрпин киеуо ьтурд ннцр еаоеоеузд \n",
      "Epoch 1900. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кеди,слбрпин киеуо ьтур  ннцр ывсооеазки\n",
      "Epoch 2000. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кеди,слбрпин киеуо ьтур  ннср ыжсоооузсе\n",
      "Epoch 2100. Time: 0.033, Train loss: 0.266\n",
      "мой дядяеи дядя   кетяссрбрпи ьяииие стнд noneмкооиыоnonenoneайн,кн\n",
      "Epoch 2200. Time: 0.032, Train loss: 0.067\n",
      "мой дядяеи дядя   кетятссбаби  ияитьлор  х  ы,ыиплдсс лхея\n",
      "Epoch 2300. Time: 0.034, Train loss: 0.066\n",
      "мой дядяеи дядя   кедятссбабис яяирылио       сбпнрррлеоее\n",
      "Epoch 2400. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя   кедятссбапис яяириц о б   еебтоопрддлр б\n",
      "Epoch 2500. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кедятслбапис яяят ,ьой  рв  н озоиивоонр\n",
      "Epoch 2600. Time: 0.034, Train loss: 0.065\n",
      "мой дядяеи дядя   кедятслбрпис яяяао ттм   ниорооппоккагнр\n",
      "Epoch 2700. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кедятслбрпис яяяао мтм   нинрооопогоогбт\n",
      "Epoch 2800. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кедятслбрпи  яяяао мв    нвоеовззоогйсии\n",
      "Epoch 2900. Time: 0.033, Train loss: 0.069\n",
      "мой дядяеи дядя   кетя србрпи сеооое слблббтслязюливти ,на\n",
      "Epoch 3000. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя   кетя србрпи неоооо са цхббдлмтеоыаеееерх\n",
      "Epoch 3100. Time: 0.032, Train loss: 0.066\n",
      "мой дядяеи дядя   кетя срррпи неооуа еа сбб,жд оооа еаоннб\n",
      "Epoch 3200. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя слррпи неуаее ра н,, созшр  нввриуд\n",
      "Epoch 3300. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя слррпи неуаее ва н,, скшне  нвуее т\n",
      "Epoch 3400. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя слррпи неуаее ва н,, скшне  нвуее т\n",
      "Epoch 3500. Time: 0.034, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя слррпи неуаее ва н,, сзшне  нвыиегт\n",
      "Epoch 3600. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя срррпи неаоуа пассн, сркллеоовоооюо\n",
      "Epoch 3700. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя срррпи неаоуа пассн, сркллеоовоооюо\n",
      "Epoch 3800. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя срррпи неаоуа папсн, сркртыеоееооо \n",
      "Epoch 3900. Time: 0.033, Train loss: 0.069\n",
      "мой дядяеи дядя с кетя самлри стмаюекмтоонт , е наае жщжем\n",
      "Epoch 4000. Time: 0.032, Train loss: 0.066\n",
      "мой дядяеи дядя с кетя самлри стмаюекмтрон  , еы опм вё рт\n",
      "Epoch 4100. Time: 0.033, Train loss: 0.066\n",
      "мой дядяеи дядя   кетя сбррпе стаааайнерлтн  ыгдиииоплле  \n",
      "Epoch 4200. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя сбррпе стаааайнерлтн  ыгдииооплле  \n",
      "Epoch 4300. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя сбррпе стаааайнерлтн, ыг иие вллэхз\n",
      "Epoch 4400. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя сбррпе стаааайнерлтн, ыг иие вллэхз\n",
      "Epoch 4500. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кетя,сбррпе  клиаапзпрочннриоя иооокс с \n",
      "Epoch 4600. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кешя,сбррпил к иуа  тасхж ожом  еуне нкз\n",
      "Epoch 4700. Time: 0.032, Train loss: 0.065\n",
      "мой дядяеи дядя   кешя,сбррпил к иуа  васхж ожос  есгдбннс\n",
      "Epoch 4800. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кешятсбррпильииааакк    , ,иеппр нллтлие\n",
      "Epoch 4900. Time: 0.033, Train loss: 0.065\n",
      "мой дядяеи дядя   кешятсбррпильивааакк  е к,, епдлрр и,ёёю\n"
     ]
    }
   ],
   "source": [
    "train_net(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лосс больше не падает - сеть генерирует отдельные слова. Попробовал ванильную RNN и GRU. Результат сильно не улушился."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
