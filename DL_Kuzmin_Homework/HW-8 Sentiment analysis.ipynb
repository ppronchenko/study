{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words())\n",
    "import gensim.models as gen\n",
    "\n",
    "# —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —ç–º–æ—Ç–∏–∫–æ–Ω—ã –≤ —Ç–µ–∫—Å—Ç\n",
    "\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "from gensim.models.wrappers import FastText \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, average_precision_score, precision_recall_curve, roc_curve, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemID,Sentiment,SentimentText\n",
      "1,0,                     is so sad for my APL friend.............\n",
      "2,0,                   I missed the New Moon trailer...\n",
      "3,1,              omg its already 7:30 :O\n",
      "4,0,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
      "5,0,         i think mi bf is cheating on me!!!       T_T\n",
      "6,0,         or i just worry too much?        \n",
      "7,1,       Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
      "8,0,       Sunny Again        Work Tomorrow  :-|       TV Tonight\n",
      "9,1,      handed in my uniform today . i miss you already\n"
     ]
    }
   ],
   "source": [
    "!head '../../data/sentiment_analysis/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/sentiment_analysis/train.csv', sep=',', encoding='ISO-8859-1')\n",
    "test = pd.read_csv('../../data/sentiment_analysis/test.csv', sep=',', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299989, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56457\n",
       "0    43532\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    –∫–ª–∞—Å—Å—ã- —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = train['SentimentText']\n",
    "data_ts = test['SentimentText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.concat([data_tr, data_ts], axis=0)\n",
    "data_full = data_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = ' '.join([x for x in list(data_full)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                     is so sad for my APL friend.............                    I missed the New Moon trailer...               omg its already 7:30 :O           .. Omgaga. Im sooo  im gunna CRy. I've\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full[:200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "def lemmatize_word(word):\n",
    "    try:\n",
    "        return m.lemmatize(word)[0] \n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /usr/local/lib/python3.6/dist-packages (2.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install emot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ó–∞–º–µ–Ω–∏–º —ç–º–æ–¥–∂–∏ –∏ —ç–º–æ—Ç–∏–∫–æ–Ω—ã –Ω–∞ —Ç–µ–∫—Å—Ç  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hilarious face_with_tears_of_joy. The feeling of making a sale smiling_face_with_sunglasses, The feeling of actually fulfilling orders unamused_face'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Function for converting emoticons into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "text1 = \"Hilarious üòÇ. The feeling of making a sale üòé, The feeling of actually fulfilling orders üòí\"\n",
    "convert_emojis(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_smiley Happy_face_smiley'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = convert_emoticons(convert_emojis(data_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' URL ', text)\n",
    "    text = re.sub('([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)', ' email ', text)\n",
    "    text = re.sub('(?:\\\\+7|8)[ -]?\\\\(?9\\\\d{2}\\\\)?[ -]?\\\\d{3}[ -]?\\\\d{2}[ -]?\\\\d{2}', ' phone ', text)\n",
    "    text = re.sub('@[^\\s]+', ' USER ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "   \n",
    "    text = re.sub('[^a-zA-Z–∞-—è–ê-–Ø]+', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = convert_emoticons(convert_emojis(text))\n",
    "    return (text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(labels, predictions):\n",
    "    rep = classification_report(labels, predictions, output_dict=True)\n",
    "    return round(rep['macro avg']['f1-score'],3), round(rep['weighted avg']['f1-score'],3),\\\n",
    "           round(rep['macro avg']['recall'],3), round(rep['weighted avg']['recall'],3),\\\n",
    "           round(rep['micro avg']['recall'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_text(text, w2w):\n",
    "    return ' '.join([w2w[word] for word in text.split() if (len(word)) and w2w[word] not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text, ft_emb):\n",
    "    vec = np.array(list(map(lambda x: ft_emb[x], text.split())))\n",
    "    return np.append(vec.shape[0], np.append(np.append(np.append(vec.mean(axis=0), vec.max(axis=0)), vec.min(axis=0)), vec.std(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~(train['SentimentText'].isnull())& \\\n",
    "              ~(train['Sentiment'].isnull())   \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(map(lambda x: preprocess_text(x), train['SentimentText']))\n",
    "train['description_preproc'] = list(map(lambda x: x, res))\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "a = list(map(lambda x: corpus.update(x.split()), train['description_preproc']))\n",
    "del a\n",
    "w2w = defaultdict()\n",
    "for word in corpus:\n",
    "    w2w[word] = m.lemmatize(word)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['description_preproc'] = list(map(lambda x: preproc_text(x, w2w), train['description_preproc']))\n",
    "train = train[train['description_preproc'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ft_emb = gen.FastText.load_fasttext_format('../../data/wiki.en.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(map(lambda x: text2vec(x, ft_emb),\\\n",
    "                     train['description_preproc']\n",
    "                     )\n",
    "                 )  \n",
    "            )                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–æ–ø—Ä–æ–±—É–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.69     34804\n",
      "           1       0.77      0.72      0.75     45159\n",
      "\n",
      "    accuracy                           0.72     79963\n",
      "   macro avg       0.72      0.72      0.72     79963\n",
      "weighted avg       0.73      0.72      0.72     79963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69      8701\n",
      "           1       0.77      0.71      0.74     11290\n",
      "\n",
      "    accuracy                           0.72     19991\n",
      "   macro avg       0.71      0.72      0.71     19991\n",
      "weighted avg       0.72      0.72      0.72     19991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=1234567890, shuffle=True)\n",
    "lr_scores = pd.DataFrame()\n",
    "for dev_idx, val_idx in kf.split(X, y):\n",
    "    break\n",
    "X_train = X[dev_idx]\n",
    "y_train = y[dev_idx]\n",
    "X_val = X[val_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=0.025, max_iter=100, tol=0.05, multi_class='auto', class_weight='balanced',\n",
    "                           random_state=13)\n",
    "model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "#print ('train: avg_f1 = {}, weighted_f1 = {}, macro_rec {}, weight_rec = {}, micro_rec = {}'.format(*get_metrics(y_train, pred_train)))\n",
    "#print ('val: avg_f1 = {}, weighted_f1 = {}, macro_rec {}, weight_rec = {}, micro_rec = {}'.format(*get_metrics(y_val, pred_val)))\n",
    "print(classification_report(y_train, pred_train))\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –æ—Ç –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫–∞–∫ —Ñ–∏—á–∏ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79963, 1203)\n",
      "(19991, 1203)\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict_proba(X_train)\n",
    "pred_val = model.predict_proba(X_val)\n",
    "print(np.hstack((pred_train, X_train)).shape)\n",
    "print(np.hstack((pred_val, X_val)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83     34804\n",
      "           1       0.88      0.84      0.86     45159\n",
      "\n",
      "    accuracy                           0.84     79963\n",
      "   macro avg       0.84      0.84      0.84     79963\n",
      "weighted avg       0.85      0.84      0.84     79963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.70      8701\n",
      "           1       0.78      0.75      0.76     11290\n",
      "\n",
      "    accuracy                           0.74     19991\n",
      "   macro avg       0.73      0.73      0.73     19991\n",
      "weighted avg       0.74      0.74      0.74     19991\n",
      "\n",
      "CPU times: user 6h 23min 49s, sys: 2min 54s, total: 6h 26min 43s\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(n_estimators=1000, #800\n",
    "                         max_depth=5, #3\n",
    "                         learning_rate=0.03, #0.03\n",
    "                         metric='logloss', #'multi_logloss'\n",
    "                         objective='binary', #'multiclass'\n",
    "                         #num_class=2,\n",
    "                         reg_lambda=1,\n",
    "                         class_weight='balanced',\n",
    "                         colsample_bytree = 0.5,\n",
    "                         subsample= 0.5,\n",
    "                         n_jobs=multiprocess.cpu_count()-1)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "pred_val = clf.predict(X_val)\n",
    "#print ('train: avg_f1 = {}, weighted_f1 = {}, macro_rec {}, weight_rec = {}, micro_rec = {}'.format(*get_metrics(y_train, pred_train)))\n",
    "#print ('val: avg_f1 = {}, weighted_f1 = {}, macro_rec {}, weight_rec = {}, micro_rec = {}'.format(*get_metrics(y_val, pred_val)))\n",
    "print(classification_report(y_train, pred_train))\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –¥–∞–ª —Ç–æ–ª—å–∫–æ 0.74 f1 –Ω–∞ —Ç–µ—Å—Ç–µ\n",
    "#### –ø–æ–ø—Ä–æ–±—É–µ–º RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(map(lambda x: preprocess_text(x), train['SentimentText']))\n",
    "train['description_preproc'] = list(map(lambda x: x, res))\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['SentimentText']\n",
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train['description_preproc'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = set(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USER', 88978),\n",
       " ('i', 63432),\n",
       " ('you', 30105),\n",
       " ('the', 30014),\n",
       " ('to', 29616),\n",
       " ('a', 22398),\n",
       " ('it', 22330),\n",
       " ('and', 16449),\n",
       " ('my', 13873),\n",
       " ('that', 13719),\n",
       " ('for', 12573),\n",
       " ('s', 12486),\n",
       " ('t', 12277),\n",
       " ('is', 12118),\n",
       " ('in', 11950),\n",
       " ('me', 11196),\n",
       " ('of', 10626),\n",
       " ('have', 9986),\n",
       " ('on', 9677),\n",
       " ('so', 9229),\n",
       " ('but', 9126),\n",
       " ('quot', 9122),\n",
       " ('m', 8894),\n",
       " ('be', 7464),\n",
       " ('was', 7250),\n",
       " ('not', 7247),\n",
       " ('just', 6865),\n",
       " ('your', 6480),\n",
       " ('can', 6119),\n",
       " ('are', 6100),\n",
       " ('good', 5927),\n",
       " ('with', 5552),\n",
       " ('like', 5538),\n",
       " ('no', 5523),\n",
       " ('lol', 5399),\n",
       " ('at', 5342),\n",
       " ('get', 5324),\n",
       " ('u', 5284),\n",
       " ('we', 5278),\n",
       " ('too', 5261),\n",
       " ('all', 4846),\n",
       " ('up', 4808),\n",
       " ('now', 4799),\n",
       " ('this', 4761),\n",
       " ('do', 4670),\n",
       " ('know', 4611),\n",
       " ('love', 4517),\n",
       " ('what', 4399),\n",
       " ('out', 4213),\n",
       " ('thanks', 4117)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(text.split(' '))\n",
    "c.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50160"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_COUNT = 10000\n",
    "ALL_WORDS = set([w for w, _ in c.most_common(WORDS_COUNT)])\n",
    "INDEX_TO_WORD = ['<pad>', '<miss>'] + list(ALL_WORDS)\n",
    "WORD_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_WORD)}\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.zeros((len(X_train), MAX_LEN), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.zeros((len(X_val), MAX_LEN), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 36 ms, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(X_train)):\n",
    "    for j, w in enumerate(X_train[i].split(' ')):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        train_data[i, j] = WORD_TO_INDEX.get(w, WORD_TO_INDEX['<miss>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 4 ms, total: 2.64 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(X_val)):\n",
    "    for j, w in enumerate(X_val[i].split(' ')):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        test_data[i, j] = WORD_TO_INDEX.get(w, WORD_TO_INDEX['<miss>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_list()\n",
    "y_val = y_val.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_data, torch.LongTensor(y_train))\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data, torch.LongTensor(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.DataLoader(train_dataset, BATCH_SIZE, shuffle=False)\n",
    "test = torch.utils.data.DataLoader(test_dataset, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, dict_size, embed_size, num_hiddens, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embed = nn.Embedding(dict_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, batch_first=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = self.embed(X)\n",
    "        y, s = self.rnn(output)\n",
    "        return y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel(nn.Module):\n",
    "    def __init__(self, dict_size, embed_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.SimpleRNN = SimpleGRU(dict_size, embed_size, num_hiddens)\n",
    "        \n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_hiddens),\n",
    "            nn.Linear(num_hiddens, num_hiddens*2)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(    \n",
    "            nn.BatchNorm1d(num_hiddens*2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hiddens*2, num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, X1):\n",
    "        y1, s1 = self.SimpleRNN(X1)\n",
    "                \n",
    "        out = self.classifier2(self.classifier1(s1[0]))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_acc(model, dataset1, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    loss_acc, passed, correct = 0., 0, 0\n",
    "    for A1 in dataset1:\n",
    "        X1 = A1[0].to(dev)\n",
    "        y1 = A1[1].to(dev)\n",
    "            \n",
    "    \n",
    "        output = model(X1)\n",
    "        l = loss(output, y1)\n",
    "\n",
    "        loss_acc += l.item()\n",
    "        correct += (output.argmax(dim=1) == y1).sum().item()\n",
    "        passed += len(y1)\n",
    "        \n",
    "      \n",
    "\n",
    "    return loss_acc / passed, correct / passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2c13e29c90>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = torch.FloatTensor([1./16., 1.-1./16.]).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl1, test_dl1, trainer, dev, num_epochs=epochs, ckpt_path = os.path.join('../../data/models/', 'Simple_GRU'+'.pt')):\n",
    "    best_record = 0.0\n",
    "    loss = nn.CrossEntropyLoss(weight=loss_weights, reduction='sum')\n",
    "    for ep in range(num_epochs):\n",
    "        ep_start, tloss_acc, tpassed, tcorrect = time.time(), 0., 0, 0\n",
    "        for A1 in train_dl1:\n",
    "        \n",
    "            X1 = A1[0].to(dev)\n",
    "            y1 = A1[1].to(dev)\n",
    "            \n",
    "            \n",
    "            trainer.zero_grad()\n",
    "\n",
    "            output = model(X1)\n",
    "            l = loss(output, y1)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "\n",
    "            tloss_acc += l.item()\n",
    "            tcorrect += (output.argmax(dim=1) == y1).sum().item()\n",
    "            tpassed += len(y1)\n",
    "    \n",
    "        test_loss, test_acc = validate_acc(model, test_dl1, dev)\n",
    "        \n",
    "        state_dict = {\n",
    "            'epoch': ep,\n",
    "            'siamese': model.state_dict(),\n",
    "            'optimizer': trainer.state_dict(),\n",
    "        }\n",
    "        torch.save(state_dict, ckpt_path)\n",
    "        print ('Model '+ckpt_path+' saved!\\n')\n",
    "\n",
    "        print('Epoch {}. Taked {:.3f} sec. Train loss: {:.3f}, Train acc {:.3f}, Test  Loss {:.3f}, Test Acc {:.3f}'.format(\n",
    "            ep, time.time() - ep_start, tloss_acc / tpassed, tcorrect / tpassed, test_loss, test_acc\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnModel(len(INDEX_TO_WORD), 100, 100, 2).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for A1 in test:\n",
    "        \n",
    "        \n",
    "        X1 = A1[0].to(dev)\n",
    "        y1 = A1[1].to(dev)\n",
    "\n",
    "        \n",
    "        labels.append(y1.data.cpu())\n",
    "        output = model(X1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,    1,  ...,    0,    0,    0],\n",
       "        [   1,    1, 2661,  ...,    0,    0,    0],\n",
       "        [   1,    1,    1,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1, 6446, 2912,  ...,    0,    0,    0],\n",
       "        [   1,  137, 2838,  ...,    0,    0,    0],\n",
       "        [   1,    1, 8043,  ...,    0,    0,    0]], device='cuda:0')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 150])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 0. Taked 6.322 sec. Train loss: 0.104, Train acc 0.566, Test  Loss 1.121, Test Acc 0.563\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 1. Taked 6.213 sec. Train loss: 0.091, Train acc 0.577, Test  Loss 1.137, Test Acc 0.578\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 2. Taked 5.915 sec. Train loss: 0.086, Train acc 0.600, Test  Loss 1.104, Test Acc 0.597\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 3. Taked 5.528 sec. Train loss: 0.082, Train acc 0.618, Test  Loss 1.046, Test Acc 0.606\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 4. Taked 6.719 sec. Train loss: 0.080, Train acc 0.636, Test  Loss 1.073, Test Acc 0.610\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 5. Taked 4.848 sec. Train loss: 0.078, Train acc 0.652, Test  Loss 1.068, Test Acc 0.617\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 6. Taked 4.924 sec. Train loss: 0.078, Train acc 0.657, Test  Loss 1.076, Test Acc 0.626\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 7. Taked 6.581 sec. Train loss: 0.077, Train acc 0.662, Test  Loss 1.080, Test Acc 0.621\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 8. Taked 6.587 sec. Train loss: 0.076, Train acc 0.669, Test  Loss 1.100, Test Acc 0.629\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 9. Taked 6.198 sec. Train loss: 0.075, Train acc 0.669, Test  Loss 1.112, Test Acc 0.622\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "train_model(model, train, test, trainer, dev, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 0. Taked 6.627 sec. Train loss: 0.072, Train acc 0.683, Test  Loss 1.069, Test Acc 0.647\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 1. Taked 6.259 sec. Train loss: 0.067, Train acc 0.715, Test  Loss 1.081, Test Acc 0.660\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 2. Taked 6.235 sec. Train loss: 0.062, Train acc 0.740, Test  Loss 1.109, Test Acc 0.664\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 3. Taked 5.692 sec. Train loss: 0.059, Train acc 0.759, Test  Loss 1.139, Test Acc 0.672\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 4. Taked 6.537 sec. Train loss: 0.056, Train acc 0.771, Test  Loss 1.174, Test Acc 0.673\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 5. Taked 6.820 sec. Train loss: 0.054, Train acc 0.785, Test  Loss 1.219, Test Acc 0.679\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 6. Taked 6.385 sec. Train loss: 0.052, Train acc 0.795, Test  Loss 1.267, Test Acc 0.681\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 7. Taked 5.625 sec. Train loss: 0.050, Train acc 0.805, Test  Loss 1.317, Test Acc 0.682\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 8. Taked 6.699 sec. Train loss: 0.050, Train acc 0.804, Test  Loss 1.342, Test Acc 0.681\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 9. Taked 5.802 sec. Train loss: 0.048, Train acc 0.814, Test  Loss 1.387, Test Acc 0.684\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "train_model(model, train, test, trainer, dev, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 0. Taked 6.723 sec. Train loss: 0.045, Train acc 0.824, Test  Loss 1.420, Test Acc 0.686\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 1. Taked 6.035 sec. Train loss: 0.044, Train acc 0.827, Test  Loss 1.431, Test Acc 0.686\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 2. Taked 6.775 sec. Train loss: 0.044, Train acc 0.830, Test  Loss 1.449, Test Acc 0.686\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 3. Taked 6.301 sec. Train loss: 0.043, Train acc 0.833, Test  Loss 1.475, Test Acc 0.687\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 4. Taked 6.724 sec. Train loss: 0.043, Train acc 0.834, Test  Loss 1.499, Test Acc 0.688\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 5. Taked 6.527 sec. Train loss: 0.042, Train acc 0.836, Test  Loss 1.519, Test Acc 0.687\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 6. Taked 6.179 sec. Train loss: 0.042, Train acc 0.838, Test  Loss 1.534, Test Acc 0.689\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 7. Taked 6.500 sec. Train loss: 0.041, Train acc 0.841, Test  Loss 1.542, Test Acc 0.689\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 8. Taked 6.575 sec. Train loss: 0.041, Train acc 0.842, Test  Loss 1.573, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 9. Taked 6.874 sec. Train loss: 0.041, Train acc 0.844, Test  Loss 1.592, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 10. Taked 6.522 sec. Train loss: 0.040, Train acc 0.845, Test  Loss 1.594, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 11. Taked 6.864 sec. Train loss: 0.040, Train acc 0.848, Test  Loss 1.619, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 12. Taked 6.755 sec. Train loss: 0.040, Train acc 0.849, Test  Loss 1.638, Test Acc 0.691\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 13. Taked 6.300 sec. Train loss: 0.039, Train acc 0.850, Test  Loss 1.671, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 14. Taked 6.604 sec. Train loss: 0.039, Train acc 0.852, Test  Loss 1.682, Test Acc 0.690\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 15. Taked 6.290 sec. Train loss: 0.038, Train acc 0.853, Test  Loss 1.695, Test Acc 0.691\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 16. Taked 6.891 sec. Train loss: 0.038, Train acc 0.855, Test  Loss 1.727, Test Acc 0.691\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 17. Taked 6.639 sec. Train loss: 0.038, Train acc 0.856, Test  Loss 1.749, Test Acc 0.692\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 18. Taked 6.548 sec. Train loss: 0.037, Train acc 0.857, Test  Loss 1.765, Test Acc 0.691\n",
      "Model ../../data/models/Simple_GRU.pt saved!\n",
      "\n",
      "Epoch 19. Taked 6.460 sec. Train loss: 0.037, Train acc 0.859, Test  Loss 1.780, Test Acc 0.692\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "train_model(model, train, test, trainer, dev, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "\n",
    "result = []\n",
    "labels = []\n",
    "\n",
    "for A1 in test:\n",
    "        \n",
    "        \n",
    "        X1 = A1[0].to(dev)\n",
    "        y1 = A1[1].to(dev)\n",
    "\n",
    "        \n",
    "        labels.append(y1.data.cpu())\n",
    "        output = model(X1)\n",
    "        output = output.to(dev)\n",
    "        output = output.squeeze(0)\n",
    "        sm = nn.Softmax(dim=1).to(dev)\n",
    "        res = sm(output.data)[:,1]\n",
    "        result += res.data.cpu().tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in result:\n",
    "    if i>=.5:\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = []\n",
    "for i in result:\n",
    "    res1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = np.array([])\n",
    "for i in labels:\n",
    "    labs = np.hstack([labs, i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6916191619161917\n",
      "ROC_AUC:  0.7453181924405609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.46      0.57      8750\n",
      "         1.0       0.68      0.87      0.76     11248\n",
      "\n",
      "    accuracy                           0.69     19998\n",
      "   macro avg       0.70      0.67      0.66     19998\n",
      "weighted avg       0.70      0.69      0.68     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "print(\"Accuracy: \", accuracy_score(labs, res))\n",
    "print(\"ROC_AUC: \", roc_auc_score(labs, res1))\n",
    "print(classification_report(labs, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### –ü—Ä–∏ –ø–æ–º–æ—â–∏ RNN —É–¥–∞–ª–æ—Å—å –≤—ã—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ 0.68 F1, —Ö—É–∂–µ —á–µ–º GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
