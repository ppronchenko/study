{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformer model notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Необходимые импорты\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from functools import partial, reduce, wraps\n",
    "from itertools import chain\n",
    "from operator import mul\n",
    "\n",
    "from product_key_memory import PKM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.function import Function\n",
    "from torch.utils.checkpoint import get_device_states, set_device_states\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Класс - обертка слоя для параллелизации вычислений\n",
    "\n",
    "# following example for saving and setting rng here https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\n",
    "class Deterministic(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.cpu_state = None\n",
    "        self.cuda_in_fwd = None\n",
    "        self.gpu_devices = None\n",
    "        self.gpu_states = None\n",
    "\n",
    "    def record_rng(self, *args):\n",
    "        self.cpu_state = torch.get_rng_state()\n",
    "        if torch.cuda._initialized:\n",
    "            self.cuda_in_fwd = True\n",
    "            self.gpu_devices, self.gpu_states = get_device_states(*args)\n",
    "\n",
    "    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n",
    "        if record_rng:\n",
    "            self.record_rng(*args)\n",
    "\n",
    "        if not set_rng:\n",
    "            return self.net(*args, **kwargs)\n",
    "\n",
    "        rng_devices = []\n",
    "        if self.cuda_in_fwd:\n",
    "            rng_devices = self.gpu_devices\n",
    "\n",
    "        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n",
    "            torch.set_rng_state(self.cpu_state)\n",
    "            if self.cuda_in_fwd:\n",
    "                set_device_states(self.gpu_devices, self.gpu_states)\n",
    "            return self.net(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversible блок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n",
    "# once multi-GPU is confirmed working, refactor and send PR back to source\n",
    "class ReversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g, depth=None, send_signal = False):\n",
    "        super().__init__()\n",
    "        self.f = Deterministic(f)\n",
    "        self.g = Deterministic(g)\n",
    "\n",
    "        self.depth = depth\n",
    "        self.send_signal = send_signal\n",
    "\n",
    "    def forward(self, x, f_args = {}, g_args = {}):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1, y2 = None, None\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = False\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n",
    "            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n",
    "\n",
    "        return torch.cat([y1, y2], dim=2)\n",
    "\n",
    "    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n",
    "        \n",
    "        y1, y2 = torch.chunk(y, 2, dim=2)\n",
    "        del y\n",
    "\n",
    "        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n",
    "        del dy\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = True\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            y1.requires_grad = True\n",
    "            gy1 = self.g(y1, set_rng=True, **g_args)\n",
    "            torch.autograd.backward(gy1, dy2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x2 = y2 - gy1\n",
    "            del y2, gy1\n",
    "\n",
    "            dx1 = dy1 + y1.grad\n",
    "            del dy1\n",
    "            y1.grad = None\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x2.requires_grad = True\n",
    "            fx2 = self.f(x2, set_rng=True, **f_args)\n",
    "            torch.autograd.backward(fx2, dx1, retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x1 = y1 - fx2\n",
    "            del y1, fx2\n",
    "\n",
    "            dx2 = dy2 + x2.grad\n",
    "            del dy2\n",
    "            x2.grad = None\n",
    "\n",
    "            x = torch.cat([x1, x2.detach()], dim=2)\n",
    "            dx = torch.cat([dx1, dx2], dim=2)\n",
    "\n",
    "        return x, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### реализация еще одного слоя - не Revirsible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrreversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "\n",
    "    def forward(self, x, f_args, g_args):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1 = x1 + self.f(x2, **f_args)\n",
    "        y2 = x2 + self.g(y1, **g_args)\n",
    "        return torch.cat([y1, y2], dim=2)\n",
    "\n",
    "class _ReversibleFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, blocks, kwargs):\n",
    "        ctx.kwargs = kwargs\n",
    "        for block in blocks:\n",
    "            x = block(x, **kwargs)\n",
    "        ctx.y = x.detach()\n",
    "        ctx.blocks = blocks\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy):\n",
    "        y = ctx.y\n",
    "        kwargs = ctx.kwargs\n",
    "        for block in ctx.blocks[::-1]:\n",
    "            y, dy = block.backward_pass(y, dy, **kwargs)\n",
    "        return dy, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создаем слой - последовательность Revirsible и Irrevirsible блоков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleSequence(nn.Module):\n",
    "    def __init__(self, blocks, layer_dropout = 0., reverse_thres = 0, send_signal = False):\n",
    "        super().__init__()\n",
    "        self.layer_dropout = layer_dropout\n",
    "        self.reverse_thres = reverse_thres\n",
    "\n",
    "        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n",
    "        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n",
    "\n",
    "    def forward(self, x, arg_route = (True, True), **kwargs):\n",
    "        reverse = x.shape[1] > self.reverse_thres\n",
    "        blocks = self.blocks if reverse else self.irrev_blocks\n",
    "\n",
    "        if self.training and self.layer_dropout > 0:\n",
    "            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) < self.layer_dropout\n",
    "            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]\n",
    "            blocks = self.blocks[:1] if len(blocks) == 0 else blocks\n",
    "\n",
    "        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n",
    "        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n",
    "\n",
    "        if not reverse:\n",
    "            for block in blocks:\n",
    "                x = block(x, **block_kwargs)\n",
    "            return x\n",
    "\n",
    "        return _ReversibleFunction.apply(x, blocks, block_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  AxialPositional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mock parameter list object until below issue is resolved\n",
    "# https://github.com/pytorch/pytorch/issues/36035\n",
    "\n",
    "### Создаем класс - список параметров\n",
    "class ParameterList(object):\n",
    "    def __init__(self, kls, prefix, length):\n",
    "        self.ind = 0\n",
    "        self.kls = kls\n",
    "        self.prefix = prefix\n",
    "        self.length = length\n",
    "\n",
    "    def _keyname(self, prefix, ind):\n",
    "        return f'{prefix}_{ind}'\n",
    "\n",
    "    def append(self, x):\n",
    "        setattr(self.kls, self._keyname(self.prefix, self.ind), x)\n",
    "        self.ind += 1\n",
    "\n",
    "    def to_list(self):\n",
    "        return [getattr(self.kls, self._keyname(self.prefix, i)) for i in range(self.length)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AxialPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, axial_shape, axial_dims = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.shape = axial_shape\n",
    "        self.max_seq_len = reduce(mul, axial_shape, 1)\n",
    "\n",
    "        self.summed = axial_dims is None\n",
    "        axial_dims = ((dim,) * len(axial_shape)) if self.summed else axial_dims\n",
    "\n",
    "        assert len(self.shape) == len(axial_dims), 'number of axial dimensions must equal the number of dimensions in the shape'\n",
    "        assert self.summed or not self.summed and sum(axial_dims) == dim, f'axial dimensions must sum up to the target dimension {dim}'\n",
    "\n",
    "        self.weights = ParameterList(self, 'weights', len(axial_shape))\n",
    "\n",
    "        for ind, (shape, axial_dim) in enumerate(zip(self.shape, axial_dims)):\n",
    "            ax_shape = [1] * len(self.shape)\n",
    "            ax_shape[ind] = shape\n",
    "            ax_shape = (1, *ax_shape, axial_dim)\n",
    "            ax_emb = nn.Parameter(torch.zeros(ax_shape).normal_(0, 1))\n",
    "            self.weights.append(ax_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, e = x.shape\n",
    "        assert (t <= self.max_seq_len), f'Sequence length ({t}) must be less than the maximum sequence length allowed ({self.max_seq_len})'\n",
    "        embs = []\n",
    "\n",
    "        for ax_emb in self.weights.to_list():\n",
    "            axial_dim = ax_emb.shape[-1]\n",
    "            expand_shape = (b, *self.shape, axial_dim)\n",
    "            emb = ax_emb.expand(expand_shape).reshape(b, self.max_seq_len, axial_dim)\n",
    "            embs.append(emb)\n",
    "\n",
    "        pos_emb = sum(embs) if self.summed else torch.cat(embs, dim=-1)\n",
    "        return pos_emb[:, :t].to(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial Positional Embedding for Images - да можно обучаться на картинках и потом \"достраивать\" изображение\n",
    "\n",
    "class AxialPositionalEmbeddingImage(nn.Module):\n",
    "    def __init__(self, dim, axial_shape, axial_dims = None):\n",
    "        super().__init__()\n",
    "        assert len(axial_shape) == 2, 'Axial shape must have 2 dimensions for images'\n",
    "        self.pos_emb = AxialPositionalEmbedding(dim, axial_shape, axial_dims)\n",
    "\n",
    "    def forward(self, img):\n",
    "        b, c, h, w = img.shape\n",
    "        img = img.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
    "        pos_emb = self.pos_emb(img)\n",
    "        return pos_emb.reshape(b, h, w, c).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autopadder - паддинг\n",
    "\n",
    "def pad_to_multiple(tensor, seqlen, multiple, dim=-1):\n",
    "    m = seqlen / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seqlen\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value=0)\n",
    "\n",
    "class Autopadder(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        assert isinstance(net, (LSHSelfAttention, Reformer, ReformerLM)), 'only modules LSHSelfAttention, Reformer, ReformerLM accepted'\n",
    "        self.net = net\n",
    "\n",
    "        reformer = net.reformer if isinstance(net, ReformerLM) else net\n",
    "        self.pad_dim = -1 if isinstance(net, ReformerLM) else -2\n",
    "\n",
    "        self.bucket_size = reformer.bucket_size\n",
    "        self.num_mem_kv = reformer.num_mem_kv\n",
    "        self.full_attn_thres = reformer.full_attn_thres\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        b, t, m, device = *x.shape[:2], self.num_mem_kv, x.device\n",
    "\n",
    "        keys = kwargs.get('keys')\n",
    "        input_mask = kwargs.get('input_mask')\n",
    "        input_attn_mask = kwargs.get('input_attn_mask')\n",
    "\n",
    "        k_len = 0 if keys is None else keys.shape[1]\n",
    "        seqlen = t + m + k_len\n",
    "\n",
    "        if seqlen > self.full_attn_thres:\n",
    "            if input_mask is None:\n",
    "                input_mask = torch.full_like(x, True, device=x.device, dtype=torch.bool)\n",
    "\n",
    "            x = pad_to_multiple(x, seqlen, self.bucket_size * 2, dim=self.pad_dim)\n",
    "\n",
    "            if input_mask is not None:\n",
    "                new_mask = F.pad(input_mask, (0, x.shape[1] - input_mask.shape[1]), value=False)\n",
    "                kwargs.update(input_mask=new_mask)\n",
    "\n",
    "            if input_attn_mask is not None:\n",
    "                offset = x.shape[1] - input_attn_mask.shape[1]\n",
    "                new_mask = F.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n",
    "                kwargs.update(input_attn_mask=new_mask)\n",
    "\n",
    "        out = self.net(x, **kwargs)\n",
    "        return out[:, 0:t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "\n",
    "TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n",
    "\n",
    "# helper fns\n",
    "# Разные вспомогательные функцие\n",
    "\n",
    "def sort_key_val(t1, t2, dim=-1):\n",
    "    values, indices = t1.sort(dim=dim)\n",
    "    t2 = t2.expand_as(t1)\n",
    "    return values, t2.gather(dim, indices)\n",
    "\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n",
    "\n",
    "# реализуем Chunking как в статье для лучшей паралеллизации и меньшего потребления памяти\n",
    "\n",
    "def process_inputs_chunk(fn, chunks=1, dim=0):\n",
    "    def inner_fn(*args, **kwargs):\n",
    "        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n",
    "        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n",
    "        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n",
    "        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n",
    "        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n",
    "    return inner_fn\n",
    "\n",
    "def chunked_sum(tensor, chunks=1):\n",
    "    *orig_size, last_dim = tensor.shape\n",
    "    tensor = tensor.reshape(-1, last_dim)\n",
    "    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n",
    "    return torch.cat(summed_tensors, dim=0).reshape(orig_size)\n",
    "\n",
    "def default(val, default_val):\n",
    "    return default_val if val is None else val\n",
    "\n",
    "def cast_tuple(x):\n",
    "    return x if isinstance(x, tuple) else (x,)\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, **kwargs):\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n",
    "            namespace_str = str(default(key_namespace, ''))\n",
    "            _cache = getattr(self, cache_attr)\n",
    "            _keyname = f'{cache_namespace}:{namespace_str}'\n",
    "\n",
    "            if fetch:\n",
    "                val = _cache[_keyname]\n",
    "                if reexecute:\n",
    "                    fn(self, *args, **kwargs)\n",
    "            else:\n",
    "                val = fn(self, *args, **kwargs)\n",
    "                if set_cache:\n",
    "                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n",
    "            return val\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\n",
    "    t = x.shape[1]\n",
    "    dims = (len(x.shape) - dim) * (0, 0)\n",
    "    padded_x = F.pad(x, (*dims, backward, forward), value= pad_value)\n",
    "    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\n",
    "    return torch.cat(tensors, dim=dim)\n",
    "\n",
    "def expand_dim(dim, k, t):\n",
    "    t = t.unsqueeze(dim)\n",
    "    expand_shape = [-1] * len(t.shape)\n",
    "    expand_shape[dim] = k\n",
    "    return t.expand(*expand_shape)\n",
    "\n",
    "def merge_dims(ind_from, ind_to, tensor):\n",
    "    shape = list(tensor.shape)\n",
    "    arr_slice = slice(ind_from, ind_to + 1)\n",
    "    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n",
    "    return tensor.reshape(*shape)\n",
    "\n",
    "def split_at_index(dim, index, t):\n",
    "    pre_slices = (slice(None),) * dim\n",
    "    l = (*pre_slices, slice(None, index))\n",
    "    r = (*pre_slices, slice(index, None))\n",
    "    return t[l], t[r]\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class MatrixMultiply(nn.Module):\n",
    "    def __init__(self, tensor, transpose = False, normalize = False):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "        self.transpose = transpose\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        tensor = self.tensor\n",
    "        if self.normalize:\n",
    "            tensor = F.normalize(tensor, dim=-1)\n",
    "        if self.transpose:\n",
    "            tensor = tensor.t()\n",
    "        return x @ tensor\n",
    "\n",
    "class ReZero(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.zeros(1))\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.g\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        return x / n * self.g\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, norm_class, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = norm_class(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class Chunk(nn.Module):\n",
    "    def __init__(self, chunks, fn, along_dim = -1):\n",
    "        super().__init__()\n",
    "        self.dim = along_dim\n",
    "        self.chunks = chunks\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.chunks == 1:\n",
    "            return self.fn(x, **kwargs)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH Attention (Locality-Sensitive-Hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n",
    "# adapted from trax, stripped to what paper said needed to work\n",
    "# namely that buckets need to be at least 64 with 8 rounds of hashing\n",
    "# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n",
    "\n",
    "class LSHAttention(nn.Module):\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,\n",
    "                  bucket_size = 64,\n",
    "                  n_hashes = 8,\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = True,\n",
    "                  attend_across_buckets = True,\n",
    "                  rehash_each_round = True,\n",
    "                  drop_for_hash_rate = 0.0,\n",
    "                  random_rotations_per_head = False,\n",
    "                  return_attn = False):\n",
    "        super().__init__()\n",
    "        if dropout >= 1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
    "\n",
    "        assert rehash_each_round or allow_duplicate_attention, (\n",
    "            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n",
    "            ' is not implemented.')\n",
    "\n",
    "        self.causal = causal\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        self.n_hashes = n_hashes\n",
    "\n",
    "        self._allow_duplicate_attention = allow_duplicate_attention\n",
    "        self._attend_across_buckets = attend_across_buckets\n",
    "        self._rehash_each_round = rehash_each_round\n",
    "        self._random_rotations_per_head = random_rotations_per_head\n",
    "\n",
    "        # will expend extra computation to return attention matrix\n",
    "        self._return_attn = return_attn\n",
    "\n",
    "        # cache buckets for reversible network, reported by authors to make Reformer work at depth\n",
    "        self._cache = {}\n",
    "\n",
    "        \n",
    "    ### основная функция получения хэшей и бакетов \n",
    "    \n",
    "    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        batch_size = vecs.shape[0]\n",
    "        device = vecs.device\n",
    "\n",
    "        # See https://arxiv.org/pdf/1509.02897.pdf\n",
    "        # We sample a different random rotation for each round of hashing to\n",
    "        # decrease the probability of hash misses.\n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        rot_size = n_buckets\n",
    "\n",
    "        rotations_shape = (\n",
    "            batch_size if self._random_rotations_per_head else 1,\n",
    "            vecs.shape[-1],\n",
    "            self.n_hashes if self._rehash_each_round else 1,\n",
    "            rot_size // 2)\n",
    "\n",
    "        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, -1, -1, -1)\n",
    "            \n",
    "        #  слой дропаут на хэш \n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "        \n",
    "        # вся магия поворота векторов осуществляется тут - einsum - сумма в энштейн нотации позволяет \n",
    "        # задать индексы тензоров входа и что нужно на выходе - b - batch, \n",
    "        # t и f - размерности векторов, h,i - размерности поворотов\n",
    "        rotated_vecs = torch.einsum('btf,bfhi->bhti', dropped_vecs, random_rotations)\n",
    "\n",
    "        if self._rehash_each_round:\n",
    "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
    "            buckets = torch.argmax(rotated_vecs, dim=-1)\n",
    "            # buckets is now (self.n_hashes, seqlen). Next we add offsets so that\n",
    "            # bucket numbers from different hashing rounds don't overlap.\n",
    "            offsets = torch.arange(self.n_hashes, device=device)\n",
    "            offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n",
    "            buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n",
    "        else:\n",
    "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
    "            # In this configuration, we map each item to the top self.n_hashes buckets\n",
    "            rotated_vecs = torch.squeeze(rotated_vecs, 0)\n",
    "            bucket_range = torch.arange(rotated_vecs.shape[-1], device=device)\n",
    "            bucket_range = torch.reshape(bucket_range, (1, -1))\n",
    "            bucket_range = bucket_range.expand_as(rotated_vecs.shape)\n",
    "\n",
    "            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n",
    "            buckets = buckets[:, -self.n_hashes:]\n",
    "\n",
    "            h, *_ = buckets.shape \n",
    "            buckets = torch.reshape(buckets.permute((*_, h)), (-1,))\n",
    "\n",
    "        return buckets\n",
    "\n",
    "    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n",
    "        batch_size, seqlen, dim, device = *qk.shape, qk.device\n",
    "\n",
    "        query_len = default(query_len, seqlen)\n",
    "        is_reverse = kwargs.pop('_reverse', False)\n",
    "        depth = kwargs.pop('_depth', None)\n",
    "\n",
    "        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n",
    "\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n",
    "\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "\n",
    "        total_hashes = self.n_hashes\n",
    "\n",
    "        ticker = torch.arange(total_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
    "        buckets_and_t = buckets_and_t.detach()\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n",
    "        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = sbuckets_and_t.detach()\n",
    "        sticker = sticker.detach()\n",
    "        undo_sort = undo_sort.detach()\n",
    "\n",
    "        st = (sticker % seqlen)\n",
    "        sqk = batched_index_select(qk, st)\n",
    "        sv = batched_index_select(v, st)\n",
    "\n",
    "        # Split off a \"bin\" axis so that attention only occurs within chunks.\n",
    "        chunk_size = total_hashes * n_buckets\n",
    "        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n",
    "        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n",
    "        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bq = bqk\n",
    "        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        def look_one_back(x):\n",
    "            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n",
    "            return torch.cat([x, x_extra], dim=2)\n",
    "\n",
    "        bk = look_one_back(bk)\n",
    "        bv = look_one_back(bv)\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "\n",
    "        # Dot-product attention.\n",
    "        # прям как в статье про трансформер делаем дот продукт и нормируем на корень из размерности\n",
    "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n",
    "        masked_value = max_neg_value(dots)\n",
    "\n",
    "        # Mask for post qk attention logits of the input sequence\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = F.pad(input_attn_mask, (0, seqlen - input_attn_mask.shape[-1], 0, seqlen - input_attn_mask.shape[-2]), value=True)\n",
    "            dot_attn_indices = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            input_attn_mask = input_attn_mask.reshape(batch_size, -1)\n",
    "            dot_attn_indices = dot_attn_indices.reshape(batch_size, -1)\n",
    "            mask = input_attn_mask.gather(1, dot_attn_indices).reshape_as(dots)\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n",
    "            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n",
    "            mkv = look_one_back(mq)\n",
    "            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
    "            if seqlen > query_len:\n",
    "                mask = mask & (bkv_t[:, :, None, :] < query_len)\n",
    "            dots.masked_fill_(mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self._attend_across_buckets:\n",
    "            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, chunk_size, -1))\n",
    "            bkv_buckets = look_one_back(bkv_buckets)\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots.masked_fill_(bucket_mask, masked_value)\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition. (2) When hard_k is set, the code\n",
    "        # instead masks all but the first occurence of each query-key pair.\n",
    "        if not self._allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % chunk_size\n",
    "            if not self._attend_across_buckets:\n",
    "                locs1 = buckets * chunk_size + locs1\n",
    "                locs2 = buckets * chunk_size + locs2\n",
    "            locs = torch.cat([\n",
    "                torch.reshape(locs1, (batch_size, total_hashes, seqlen)),\n",
    "                torch.reshape(locs2, (batch_size, total_hashes, seqlen)),\n",
    "            ], 1).permute((0, 2, 1))\n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * total_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :total_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, total_hashes))\n",
    "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(total_hashes * batch_size))\n",
    "            dup_counts = dup_counts.detach()\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - torch.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
    "        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n",
    "        dropped_dots = self.dropout(dots)\n",
    "\n",
    "        bo = torch.einsum('buij,buje->buie', dropped_dots, bv)\n",
    "        so = torch.reshape(bo, (batch_size, -1, dim))\n",
    "        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n",
    "\n",
    "        class UnsortLogits(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, so, slogits):\n",
    "                so = so.detach()\n",
    "                slogits = slogits.detach()\n",
    "                o = batched_index_select(so, undo_sort)\n",
    "                _, logits = sort_key_val(sticker, slogits, dim=-1)\n",
    "                return o, logits\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_x, grad_y):\n",
    "                so_grad = batched_index_select(grad_x, sticker)\n",
    "                _, slogits_grad = sort_key_val(buckets_and_t, grad_y, dim=-1)\n",
    "                return so_grad, slogits_grad\n",
    "\n",
    "        o, logits = UnsortLogits.apply(so, slogits)\n",
    "        o = torch.reshape(o, (batch_size, total_hashes, seqlen, dim))\n",
    "        logits = torch.reshape(logits, (batch_size, total_hashes, seqlen, 1))\n",
    "\n",
    "        if query_len != seqlen:\n",
    "            query_slice = (slice(None), slice(None), slice(0, query_len))\n",
    "            o, logits = o[query_slice], logits[query_slice]\n",
    "\n",
    "        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n",
    "        out = torch.sum(o * probs, dim=1)\n",
    "\n",
    "        attn = torch.empty(0, device=device)\n",
    "\n",
    "        # return unsorted attention weights\n",
    "        if self._return_attn:\n",
    "            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            attn_unsort = attn_unsort.view(batch_size * total_hashes, -1).long()\n",
    "            unsorted_dots = torch.zeros(batch_size * total_hashes, seqlen * seqlen, device=device)\n",
    "            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n",
    "            del attn_unsort\n",
    "            unsorted_dots = unsorted_dots.reshape(batch_size, total_hashes, seqlen, seqlen)\n",
    "            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n",
    "\n",
    "        # return output, attention matrix, and bucket distribution\n",
    "        return out, attn, buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local attention\n",
    "\n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, bucket_size, causal = False, look_backward = 1, look_forward = 0, dropout = 0., shared_qk = False):\n",
    "        super().__init__()\n",
    "        assert not (causal and look_forward > 0), 'you cannot look forward if causal'\n",
    "        self.bucket_size = bucket_size\n",
    "        self.causal = causal\n",
    "        self.look_backward = look_backward\n",
    "        self.look_forward = look_forward\n",
    "        self.shared_qk = shared_qk\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, input_mask = None):\n",
    "        b, t, e, device, dtype = *q.shape, q.device, q.dtype\n",
    "        bucket_size, causal, look_backward, look_forward, shared_qk = self.bucket_size, self.causal, self.look_backward, self.look_forward, self.shared_qk\n",
    "\n",
    "        buckets = t // bucket_size\n",
    "\n",
    "        if shared_qk:\n",
    "            k = F.normalize(k, 2, dim=-1).type_as(q)\n",
    "\n",
    "        ticker = torch.arange(t, device=device, dtype=dtype)[None, :]\n",
    "        b_t = ticker.reshape(1, buckets, bucket_size)\n",
    "\n",
    "        bucket_fn = lambda t: t.reshape(b, buckets, bucket_size, -1)\n",
    "        bq, bk, bv = map(bucket_fn, (q, k, v))\n",
    "\n",
    "        look_around_kwargs = {'backward': look_backward, 'forward': look_forward}\n",
    "        bk = look_around(bk, **look_around_kwargs)\n",
    "        bv = look_around(bv, **look_around_kwargs)\n",
    "\n",
    "        bq_t = b_t\n",
    "        bq_k = look_around(b_t, **look_around_kwargs)\n",
    "\n",
    "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (e ** -0.5)\n",
    "        mask_value = max_neg_value(dots)\n",
    "\n",
    "        if shared_qk:\n",
    "            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\n",
    "            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n",
    "            del mask\n",
    "\n",
    "        if causal:\n",
    "            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        mask = bq_k[:, :, None, :] == -1\n",
    "        dots.masked_fill_(mask, mask_value)\n",
    "        del mask\n",
    "\n",
    "        if input_mask is not None:\n",
    "            h = b // input_mask.shape[0]\n",
    "            input_mask = input_mask.reshape(-1, buckets, bucket_size)\n",
    "            mq = mk = input_mask\n",
    "            mk = look_around(mk, pad_value=False, **look_around_kwargs)\n",
    "            mask = (mq[:, None, :, :, None] * mk[:, None, :, None, :])\n",
    "            mask = merge_dims(0, 1, mask.expand(-1, h, -1, -1, -1))\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhje->bhie', attn, bv)\n",
    "        out = out.reshape(b, t, e)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared qk attention, using either full or LSH attention\n",
    "\n",
    "class LSHSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, bucket_size = 64, n_hashes = 8, causal = False, attn_chunks = 1, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, num_mem_kv = 0, one_value_head = False, use_full_attn = False, full_attn_thres = None, return_attn = False, post_attn_dropout = 0., dropout = 0., n_local_attn_heads = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0, 'dimensions must be divisible by number of heads'\n",
    "        assert n_local_attn_heads < heads, 'local attention heads must be less than number of heads'\n",
    "\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.attn_chunks = default(attn_chunks, 1)\n",
    "\n",
    "        self.v_head_repeats = (heads if one_value_head else 1)\n",
    "        v_dim = dim // self.v_head_repeats\n",
    "\n",
    "        self.toqk = nn.Linear(dim, dim, bias = False)\n",
    "        self.tov = nn.Linear(dim, v_dim, bias = False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "        self.bucket_size = bucket_size\n",
    "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout, **kwargs)\n",
    "        self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n",
    "        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n",
    "\n",
    "        self.use_full_attn = use_full_attn\n",
    "        self.full_attn_thres = default(full_attn_thres, bucket_size)\n",
    "\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "        self.mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n",
    "\n",
    "        self.n_local_attn_heads = n_local_attn_heads\n",
    "        self.local_attn = LocalAttention(bucket_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n",
    "\n",
    "        self.callback = None\n",
    "\n",
    "    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, **kwargs):\n",
    "        device, dtype = x.device, x.dtype\n",
    "        b, t, e, h, m, l_h = *x.shape, self.heads, self.num_mem_kv, self.n_local_attn_heads\n",
    "\n",
    "        mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "        mem = mem_kv.expand(b, m, e)\n",
    "\n",
    "        keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "        c = keys.shape[1]\n",
    "\n",
    "        kv_len = t + m + c\n",
    "        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n",
    "\n",
    "        x = torch.cat((x, mem, keys), dim=1)\n",
    "        qk = self.toqk(x)\n",
    "        v = self.tov(x)\n",
    "        v = v.repeat(1, 1, self.v_head_repeats)\n",
    "\n",
    "        def merge_heads(v):\n",
    "            return v.view(b, kv_len, h, -1).transpose(1, 2)\n",
    "\n",
    "        def split_heads(v):\n",
    "            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n",
    "\n",
    "        merge_batch_and_heads = partial(merge_dims, 0, 1)\n",
    "\n",
    "        qk, v = map(merge_heads, (qk, v))\n",
    "\n",
    "        has_local = l_h > 0\n",
    "        lsh_h = h - l_h\n",
    "\n",
    "        split_index_fn = partial(split_at_index, 1, l_h)\n",
    "        (lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n",
    "        lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n",
    "\n",
    "        masks = {}\n",
    "        if input_mask is not None or context_mask is not None:\n",
    "            default_mask = torch.tensor([True], device=device)\n",
    "            i_mask = default(input_mask, default_mask.expand(b, t))\n",
    "            m_mask = default_mask.expand(b, m)\n",
    "            c_mask = default(context_mask, default_mask.expand(b, c))\n",
    "            mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n",
    "            mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n",
    "            masks['input_mask'] = mask\n",
    "\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n",
    "            masks['input_attn_mask'] = input_attn_mask\n",
    "\n",
    "        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n",
    "        partial_attn_fn = partial(attn_fn, query_len = t, **kwargs)\n",
    "        attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = self.attn_chunks)\n",
    "\n",
    "        out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n",
    "\n",
    "        if self.callback is not None:\n",
    "            self.callback(attn.reshape(b, h, t, -1), buckets.reshape(b, h, -1))\n",
    "\n",
    "        if has_local:\n",
    "            lqk, lv = lqk[:, :t], lv[:, :t]\n",
    "            local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n",
    "            local_out = local_out.reshape(b, l_h, t, -1)\n",
    "            out = out.reshape(b, lsh_h, t, -1)\n",
    "            out = torch.cat((local_out, out), dim=1)\n",
    "\n",
    "        out = split_heads(out).view(b, t, e)\n",
    "        out = self.to_out(out)\n",
    "        return self.post_attn_dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple full attention\n",
    "\n",
    "class FullQKAttention(nn.Module):\n",
    "    def __init__(self, causal = False, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n",
    "        b, seq_len, dim = qk.shape\n",
    "        query_len = default(query_len, seq_len)\n",
    "        t = query_len\n",
    "\n",
    "        q = qk[:, 0:query_len]\n",
    "        qk = F.normalize(qk, 2, dim=-1).type_as(q)\n",
    "\n",
    "        dot = torch.einsum('bie,bje->bij', q, qk) * (dim ** -0.5)\n",
    "\n",
    "        # qk attention requires tokens not attend to self\n",
    "        i = torch.arange(t)\n",
    "        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n",
    "        masked_value = max_neg_value(dot)\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            mask = input_mask[:, 0:query_len, None] * input_mask[:, None, :]\n",
    "            mask = F.pad(mask, (0, seq_len - mask.shape[-1]), value=True)\n",
    "            dot.masked_fill_(~mask, masked_value)\n",
    "\n",
    "        # Mask for post qk attention logits of the input sequence\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = F.pad(input_attn_mask, (0, seq_len - input_attn_mask.shape[-1]), value=True)\n",
    "            dot.masked_fill_(~input_attn_mask, masked_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(t, t, 1)\n",
    "            dot[:, i, j] = masked_value\n",
    "\n",
    "        dot = dot.softmax(dim=-1)\n",
    "        dot = self.dropout(dot)\n",
    "\n",
    "        out = torch.einsum('bij,bje->bie', dot, v)\n",
    "\n",
    "        return out, dot, torch.empty(0)\n",
    "\n",
    "\n",
    "# feed forward\n",
    "\n",
    "class GELU_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0., activation = None, glu = False):\n",
    "        super().__init__()\n",
    "        activation = default(activation, GELU)\n",
    "\n",
    "        self.glu = glu\n",
    "        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))\n",
    "        self.act = activation()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w2 = nn.Linear(dim * mult, dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if not self.glu:\n",
    "            x = self.w1(x)\n",
    "            x = self.act(x)\n",
    "        else:\n",
    "            x, v = self.w1(x).chunk(2, dim=-1)\n",
    "            x = self.act(x) * v\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "\n",
    "# positional embeddings\n",
    "\n",
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)\n",
    "\n",
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Reformer(nn.Module):\n",
    "    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 8, ff_chunks = 100, attn_chunks = None, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_activation = None, ff_mult = 4, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., lsh_attend_across_buckets = True, lsh_allow_duplicate_attention = True, random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "\n",
    "        self.bucket_size = bucket_size\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "\n",
    "        self.twin_attention = twin_attention\n",
    "        self.full_attn_thres = full_attn_thres\n",
    "\n",
    "        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n",
    "        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)\n",
    "        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)\n",
    "\n",
    "        if weight_tie:\n",
    "            get_attn, get_ff, get_pkm = map(cache_fn, (get_attn, get_ff, get_pkm))\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n",
    "\n",
    "        residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n",
    "\n",
    "        for ind in range(depth):\n",
    "            layer_num = ind + 1\n",
    "            use_pkm = layer_num in cast_tuple(pkm_layers)\n",
    "            parallel_net = None\n",
    "\n",
    "            attn = get_attn()\n",
    "\n",
    "            if use_pkm:\n",
    "                parallel_net = get_pkm()\n",
    "            elif twin_attention:\n",
    "                parallel_net = get_attn()\n",
    "            else:\n",
    "                parallel_net = get_ff()\n",
    "\n",
    "            f = residual_fn_wrapper(attn)\n",
    "            g = residual_fn_wrapper(parallel_net)\n",
    "\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout = layer_dropout, reverse_thres = reverse_thres, send_signal = True)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (True, self.twin_attention)\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "\n",
    "class ReformerLM(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 4, ff_chunks = 100, attn_chunks = 1, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_mult = 4, ff_activation = None, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, emb_dim = None, return_embeddings = False, weight_tie_embedding = False, fixed_position_emb = False, absolute_position_emb = False, axial_position_shape = None, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n",
    "        super().__init__()\n",
    "        emb_dim = default(emb_dim, dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n",
    "\n",
    "        self.to_model_dim = Identity() if emb_dim == dim else nn.Linear(emb_dim, dim)\n",
    "\n",
    "        if absolute_position_emb:\n",
    "            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len)\n",
    "        elif fixed_position_emb:\n",
    "            self.pos_emb = FixedPositionalEmbedding(emb_dim)\n",
    "        else:\n",
    "            axial_position_shape = default(axial_position_shape, (max_seq_len // bucket_size, bucket_size))\n",
    "            self.pos_emb = AxialPositionalEmbedding(emb_dim, axial_position_shape)\n",
    "\n",
    "        self.reformer = Reformer(dim, depth, max_seq_len, heads = heads, bucket_size = bucket_size, n_hashes = n_hashes, ff_chunks = ff_chunks, attn_chunks = attn_chunks, causal = causal, weight_tie = weight_tie, lsh_dropout = lsh_dropout, ff_mult = ff_mult, ff_activation = ff_activation, ff_glu = ff_glu, ff_dropout = ff_dropout, post_attn_dropout = 0., layer_dropout = layer_dropout, random_rotations_per_head = random_rotations_per_head, twin_attention = twin_attention, use_scale_norm = use_scale_norm, use_rezero = use_rezero, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, reverse_thres = reverse_thres, num_mem_kv = num_mem_kv, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads, pkm_layers = pkm_layers, pkm_num_keys = pkm_num_keys)\n",
    "\n",
    "        if return_embeddings:\n",
    "            self.out = Identity()\n",
    "            return\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(dim, emb_dim) if emb_dim != dim else Identity(),\n",
    "            nn.Linear(emb_dim, num_tokens) if not weight_tie_embedding else MatrixMultiply(self.token_emb.weight, transpose=True, normalize=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.token_emb(x)\n",
    "        x = x + self.pos_emb(x).type_as(x)\n",
    "\n",
    "        x = self.to_model_dim(x)\n",
    "        x = self.reformer(x, **kwargs)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Wrapper\n",
    "\n",
    "def top_p(logits, thres = 0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cum_probs > (1 - thres)\n",
    "    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "    sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "    sorted_logits[sorted_indices_to_remove] = float('-inf')\n",
    "    return sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "def top_k(logits, thres = 0.9):\n",
    "    k = int((1 - thres) * logits.shape[-1])\n",
    "    val, ind = torch.topk(logits, k)\n",
    "    probs = torch.full_like(logits, float('-inf'))\n",
    "    probs.scatter_(1, ind, val)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "class TrainingWrapper(nn.Module):\n",
    "    def __init__(self, net, ignore_index = -100, pad_value = 0):\n",
    "        super().__init__()\n",
    "        assert isinstance(net, ReformerLM), 'generative trainer wrapper can only accept ReformerLM class'\n",
    "        self.pad_value = pad_value\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        self.net = Autopadder(net)\n",
    "        self.max_seq_len = net.max_seq_len\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, seq_len, eos_token = None, temperature = 1., filter_logits_fn = top_k, filter_thres = 0.9, **kwargs):\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out = start_tokens\n",
    "        input_mask = kwargs.pop('input_mask', None)\n",
    "\n",
    "        if input_mask is None:\n",
    "            input_mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            input_mask = input_mask[:, -self.max_seq_len:]\n",
    "\n",
    "            logits = self.net(x, input_mask=input_mask, **kwargs)[:, -1, :]\n",
    "            filtered_logits = filter_logits_fn(logits, thres = filter_thres)\n",
    "            probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "            input_mask = F.pad(input_mask, (0, 1), value=True)\n",
    "\n",
    "            if eos_token is not None and (sample == eos_token).all():\n",
    "                break\n",
    "\n",
    "        out = out[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, return_loss = False, **kwargs):\n",
    "        pad = partial(pad_sequence, batch_first = True, padding_value = self.pad_value)\n",
    "\n",
    "        if not return_loss:\n",
    "            if not isinstance(x, torch.Tensor):\n",
    "                x = pad(x)\n",
    "            return self.net(x, **kwargs)\n",
    "\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            xi = x[:, :-1]\n",
    "            xo = x[:, 1:]\n",
    "        else:\n",
    "            xi = pad(list(map(lambda t: t[:-1], x)))\n",
    "            xo = pad(list(map(lambda t: t[1:], x)))\n",
    "\n",
    "        out = self.net(xi, **kwargs)\n",
    "\n",
    "        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(2500)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 10\n",
    "GENERATE_EVERY  = 50\n",
    "GENERATE_LENGTH = 256\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingWrapper(\n",
       "  (net): Autopadder(\n",
       "    (net): ReformerLM(\n",
       "      (token_emb): Embedding(256, 512)\n",
       "      (to_model_dim): Identity()\n",
       "      (pos_emb): AxialPositionalEmbedding()\n",
       "      (reformer): Reformer(\n",
       "        (layers): ReversibleSequence(\n",
       "          (blocks): ModuleList(\n",
       "            (0): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (5): ReversibleBlock(\n",
       "              (f): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): Deterministic(\n",
       "                (net): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (irrev_blocks): ModuleList(\n",
       "            (0): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (5): IrreversibleBlock(\n",
       "              (f): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (g): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                    (act): GELU()\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "model = ReformerLM(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    num_tokens = 256,\n",
    "    heads = 8,\n",
    "    bucket_size = 64,\n",
    "    n_hashes = 4,\n",
    "    ff_chunks = 10,\n",
    "    lsh_dropout = 0.1,\n",
    "    weight_tie = True,\n",
    "    causal = True,\n",
    "    n_local_attn_heads = 4,\n",
    "    use_full_attn = False # set this to true for comparison with full attention\n",
    ")\n",
    "\n",
    "model = TrainingWrapper(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "with gzip.open('./data/enwik8.gz') as file:\n",
    "    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
    "    trX, vaX = np.split(X, [int(90e6)])\n",
    "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "        state_dict = {'epoch': i\n",
    "              , 'reformer_model': model.state_dict()\n",
    "              , 'optimizer': optim.state_dict()\n",
    "              , 'validation_loss': loss.item()\n",
    "             }\n",
    "        chk_path = './data/models/reformer'+str(i)+'.chk'\n",
    "        torch.save(state_dict, chk_path)\n",
    "        \n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss after 2500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./data/models/reformer2490.chk')\n",
    "model_state = state_dict['reformer_model']\n",
    "optimizer_state = state_dict['optimizer']\n",
    "model.load_state_dict(model_state)\n",
    "optim.load_state_dict(optimizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1714210510253906\n"
     ]
    }
   ],
   "source": [
    "loss = model(next(train_loader), return_loss = True)\n",
    "print(f'training loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.150939464569092\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss = model(next(val_loader), return_loss = True)\n",
    "    print(f'validation loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Rankole ondos]], that the eerssanget|St]]  '''[[de:å",
      "ÑÐ¼Ð»Ð¾Ð»Ñ«ãÐµÐ½Ð±ÏÐµÑÐ°ì¬å",
      "³¼ Spe offfermmatoncy)|thurd of [[Nazin Flow Nass]] [[ge:Hobasman tholspe]] #[[Steger]] [[trisskos|Consty, StagentÃ©olm/homs]] [[rol mamptigi]] [[fate:ãÐ½Ð½ÑÐ°Ð°Ð¼Ð\n"
     ]
    }
   ],
   "source": [
    "sample = model.generate(inp, GENERATE_LENGTH)\n",
    "output_str = decode_tokens(sample)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                                                                | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.006351947784424\n",
      "validation loss: 2.121259927749634\n",
      "%s \n",
      "\n",
      " %s (\"'||10&lt;sup&gt;TH&lt;/sup&gt;||30 |-  ||'''HOME RUNS'''||2&lt;sup&gt;ND&lt;/sup&gt;||44 |-  ||'''HR/100 OUTS'''||2&lt;sup&gt;ND&lt;/sup&gt;||10.73 |-  ||'''HR/100 PA'''||2&lt;sup&gt;ND&lt;/sup&gt;||6.89 |-  ||'''HR/100 AB'''||2&lt;sup&gt;ND&lt;/sup&gt;||8.04 |-  ||'''RBI'''||7&lt;sup&gt;TH&lt;/sup&gt;||97 |-  ||'''WALKS'''||7&lt;sup&gt;TH&lt;/sup&gt;||87 |-  ||'''CAUGHT STEALING'''||5&lt;sup&gt;TH&lt;/sup&gt;||10 |-  ||'''SLG'''||2&lt;sup&gt;ND&lt;/sup&gt;||.607 |-  ||'''OBA'''||7&lt;sup&gt;TH&lt;/sup&gt;||.396 |-  ||'''OPS'''||2&lt;sup&gt;ND&lt;/sup&gt;||1.003 |-  ||'''RUNS CREATED'''||3&lt;sup&gt;RD&lt;/sup&gt;||127 |-  ||'''RCAA'''||4&lt;sup&gt;TH&lt;/sup&gt;||58 |-  ||'''RCAP'''||5&lt;sup&gt;TH&lt;/sup&gt;||41 |-  ||'''OWP'''||6&lt;sup&gt;TH&lt;/sup&gt;||.750 |-  ||'''RUNS CREATED/GAME'''||5&lt;sup&gt;TH&lt;/sup&gt;||8.36 |-  ||'''TOTAL BASES'''||1&lt;sup&gt;ST&lt;/sup&gt;||332 |-  ||'''EXTRA BASE HITS'''||1&lt;sup&gt;ST&lt;/sup&gt;||77 |-  ||'''ISOLATED POWER'''||2&lt;sup&gt;ND&lt;/sup&gt;||.307 |-  ||'''SECONDARY AVERAGE'''||3&lt;sup&gt;RD&lt;/sup&gt;||.464 |-  ||'''TOTAL AVERAGE'''||3&lt;sup&gt;RD&lt;/sup&gt;||1.032 |-  ||'''BPA'''||2&lt;sup&gt;ND&lt;/sup&gt;||.635 |-  ||'''INTENTIONAL WALKS'''||2&lt;sup&gt;ND&lt;/sup&gt;||19 |- |} |- | valign=&quot;top&quot; | {| cellpadding=&quot;1&quot; cellspacing=&quot;4&quot; style=&quot;border: 1px solid black&quot; |- |colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;border-bottom: 2px solid black&quot;|'''1970 NL''' |- ||'''RUNS'''||9&lt;sup&gt;TH&lt;/sup&gt;||103 |-  ||'''HOME RUNS'''||5&lt;sup&gt;TH&lt;/sup&gt;||38 |-  ||'''HR/100 OUTS'''||2&lt;sup&gt;ND&lt;/sup&gt;||9.97 |-  ||'''HR/100 PA'''||3&lt;sup&gt;RD&lt;/sup&gt;||6.35 |-  ||'''HR/100 AB'''||4&lt;sup&gt;TH&lt;/sup&gt;||7.36 |-  ||'''RBI'''||5&lt;sup&gt;TH&lt;/sup&gt;||118 |-  ||'''SLG'''||7&lt;sup&gt;TH&lt;/sup&gt;||.574 |-  ||'''OPS'''||6&lt;sup&gt;TH&lt;/sup&gt;||.958 |-  ||'''RCAA'''||10&lt;sup&gt;TH&lt;/sup&gt;||42 |-  ||'''OWP'''||7&lt;sup&gt;TH&lt;/sup&gt;||.693 |-  ||'''RUNS CREATED/GAME'''||6&lt;sup&gt;TH&lt;/sup&gt;||8.15 |-  ||'''EXTRA BASE HITS'''||9&lt;sup&gt;TH&lt;/sup&gt;||65 |-  ||'''ISOLATED POWER'''||4&lt;sup&gt;TH&lt;/sup&gt;||.275 |-  ||'''SECONDARY AVERAGE'''||5&lt;sup&gt;TH&lt;/sup&gt;||.436 |-  ||'''TOTAL AVERAGE'''||6&lt;sup&gt;TH&lt;/sup&gt;||1.016 |-  ||'''BPA'''||5&lt;sup&gt;TH&lt;/sup&gt;||.615 |-  ||'''INTENTIONAL WALKS'''||5&lt;sup&gt;TH&lt;/sup&gt;||15 |- |} | valign=&quot;top&quot; | {| cellpadding=&quot;1&quot; cellspacing=&quot;4&quot; style=&quot;border: 1px solid black&quot; |- |colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;border-bottom: 2px solid black&quot;|'''1971 NL''' |- ||'''RUNS'''||6&lt;sup&gt;TH&lt;/sup&gt;||95 |-  ||'''HOME RUNS'''||2&lt;sup&gt;ND&lt;/sup&gt;||47 |- ||'''HR/100 OUTS'''||1&lt;sup&gt;ST&lt;/sup&gt;||13.51 |-  ||'''HR/100 PA'''||1&lt;sup&gt;ST&lt;/sup&gt;||8.20 |-  ||'''HR/100 AB'''||1&lt;sup&gt;ST&lt;/sup&gt;||9.49 |-  ||'''RBI'''||3&lt;sup&gt;RD&lt;/sup&gt;||118 |-  ||'''AVERAGE'''||5&lt;sup&gt;TH&lt;/sup&gt;||.327 |-  ||'''SLG'''||1&lt;sup&gt;ST&lt;/sup&gt;||.669 |-  ||'''OBA'''||3&lt;sup&gt;RD&lt;/sup&gt;||.410 |-  ||'''OPS'''||1&lt;sup&gt;ST&lt;/sup&gt;||1.079 |-  ||'''RUNS CREATED'''||2&lt;sup&gt;ND&lt;/sup&gt;||135 |-  ||'''RCAA'''||2&lt;sup&gt;ND&lt;/sup&gt;||73 |-  ||'''RCAP'''||3&lt;sup&gt;RD&lt;/sup&gt;||60 |-  ||'''OWP'''||2&lt;sup&gt;ND&lt;/sup&gt;||.804 |-  ||'''RUNS CREATED/GAME'''||1&lt;sup&gt;ST&lt;/sup&gt;||10.47 |-  ||'''TOTAL BASES'''||2&lt;sup&gt;ND&lt;/sup&gt;||331 |-  ||'''EXTRA BASE HITS'''||2&lt;sup&gt;ND&lt;/sup&gt;||72 |-  ||'''ISOLATED POWER'''||1&lt;sup&gt;ST&lt;/sup&gt;||.341 |-  ||'''SECONDARY AVERAGE'''||3&lt;sup&gt;RD&lt;/sup&gt;||.485 |-  ||'''TOTAL AVERAGE'''||1&lt;sup&gt;ST&lt;/sup&gt;||1.178 |-  ||'''BPA'''||1&lt;sup&gt;ST&lt;/sup&gt;||.689 |-  ||'''INTENTIONAL WALKS'''||1&lt;sup&gt;ST&lt;/sup&gt;||21  |- |} | valign=&quot;top&quot; | {| cellpadding=&quot;1&quot; cellspacing=&quot;4&quot; style=&quot;border: 1px solid black&quot; |- |colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;b\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|▎                                                                                                                                                     | 1/500 [00:26<3:37:07, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat;id &quot;&quot;''&lt;bcaf&lt;ty | fine &quot; eans=&quot;&quot;n=&ap;&lt;td&amp;mb&quot;n=&gt; | | |&quot;n&quot;/&gt;td&quot;&lt;td&gt;mat;tsud&quot; | |}^{-5&lt;&gt;''t;tr = inone&quot; | &lt;/dag&gt; willt&gt;cef &quot;#392&lt;ci&gt;to &quot;b&lt;&g\n",
      "training loss: 2.0332043170928955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   1%|▉                                                                                                                                                     | 3/500 [00:37<2:45:39, 20.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1569573879241943\n",
      "training loss: 1.9926022291183472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   1%|█▌                                                                                                                                                    | 5/500 [00:50<2:11:45, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1471638679504395\n",
      "training loss: 2.0758185386657715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   1%|██                                                                                                                                                    | 7/500 [01:06<1:51:32, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.136307954788208\n",
      "training loss: 2.211181163787842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   2%|██▋                                                                                                                                                   | 9/500 [01:23<1:38:41, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1212592124938965\n",
      "training loss: 2.0779011249542236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   2%|██▋                                                                                                                                                   | 9/500 [01:40<1:38:41, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.053191661834717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   2%|███▎                                                                                                                                                 | 11/500 [01:42<1:31:05, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.074352264404297\n",
      "training loss: 2.103111982345581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   3%|███▊                                                                                                                                                 | 13/500 [02:00<1:26:29, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0195112228393555\n",
      "training loss: 2.1215100288391113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   3%|████▍                                                                                                                                                | 15/500 [02:18<1:21:13, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1308069229125977\n",
      "training loss: 2.1318788528442383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   3%|█████                                                                                                                                                | 17/500 [02:37<1:20:11,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.115903377532959\n",
      "training loss: 2.0687873363494873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   4%|█████▋                                                                                                                                               | 19/500 [02:56<1:18:02,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1136441230773926\n",
      "training loss: 2.0970277786254883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   4%|█████▋                                                                                                                                               | 19/500 [03:10<1:18:02,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.131134033203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   4%|██████▎                                                                                                                                              | 21/500 [03:17<1:19:40,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.100651741027832\n",
      "training loss: 2.140695095062256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   5%|██████▊                                                                                                                                              | 23/500 [03:35<1:17:28,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9860424995422363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   5%|███████▍                                                                                                                                             | 25/500 [03:56<1:18:29,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.100536346435547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   5%|███████▋                                                                                                                                             | 26/500 [04:07<1:20:31, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1762375831604004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   5%|████████                                                                                                                                             | 27/500 [04:17<1:20:04, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9239537715911865\n",
      "training loss: 2.106886148452759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   6%|████████▋                                                                                                                                            | 29/500 [04:35<1:17:50,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.075500011444092\n",
      "training loss: 1.9907206296920776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   6%|████████▋                                                                                                                                            | 29/500 [04:50<1:17:50,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.11592960357666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   6%|█████████▏                                                                                                                                           | 31/500 [04:56<1:18:00,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.039640426635742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   6%|█████████▌                                                                                                                                           | 32/500 [05:06<1:19:50, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0871405601501465\n",
      "training loss: 2.0762743949890137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   7%|██████████▏                                                                                                                                          | 34/500 [05:27<1:19:27, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2345123291015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   7%|██████████▍                                                                                                                                          | 35/500 [05:38<1:20:41, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9681174755096436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   7%|██████████▋                                                                                                                                          | 36/500 [05:48<1:21:24, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9106364250183105\n",
      "training loss: 2.146956205368042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   8%|███████████▎                                                                                                                                         | 38/500 [06:08<1:19:14, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.079383134841919\n",
      "training loss: 2.2390804290771484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   8%|███████████▉                                                                                                                                         | 40/500 [06:29<1:19:15, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.136092185974121\n",
      "training loss: 2.073765277862549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   8%|████████████▏                                                                                                                                        | 41/500 [06:40<1:20:01, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0687789916992188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   8%|████████████▌                                                                                                                                        | 42/500 [06:51<1:21:26, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.037799835205078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   9%|████████████▊                                                                                                                                        | 43/500 [07:01<1:19:55, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1267600059509277\n",
      "training loss: 2.160398244857788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   9%|█████████████▍                                                                                                                                       | 45/500 [07:21<1:19:10, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.148925304412842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   9%|█████████████▋                                                                                                                                       | 46/500 [07:33<1:21:56, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0926215648651123\n",
      "training loss: 2.142927408218384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|██████████████▎                                                                                                                                      | 48/500 [07:53<1:19:45, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.032125473022461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|██████████████▌                                                                                                                                      | 49/500 [08:04<1:18:52, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.287369728088379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|██████████████▉                                                                                                                                      | 50/500 [08:15<1:20:02, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.19667911529541\n",
      "training loss: 2.120934009552002\n",
      "validation loss: 2.2092456817626953\n",
      "%s \n",
      "\n",
      " %s (\"births (2005 est.)  '''Life expectancy at birth:''' &lt;br&gt;''total population:'' 69.57 years &lt;br&gt;''male:'' 67.13 years &lt;br&gt;''female:'' 70.13 years (2005 est.)  '''Total fertility rate:''' 2.44 children born/woman (2005 est.)  '''Nationality:''' &lt;br&gt;''noun:'' Indonesian(s) &lt;br&gt;''adjective:'' Indonesian  ==Ethnic groups==  There are over 300 ethnic groups in Indonesia. Many ethnic groups, particularly in [[Kalimantan]] and [[Papua]], have only hundreds of members. Most of the local languages belong to [[Austronesian languages|Austronesian]] linguistic family, although a significant number, particularly in [[Papua]], speak [[Papuan languages]]. In addition, there are roughly 5 million [[Indonesian Chinese|people of Chinese descent]] which speaks various [[Chinese dialects]], most notably [[Cantonese (linguistics)|Cantonese]] and [[Min Nan]].  The population ranking of the ethnic groups in Indonesia ([[2000]] census) is as follows:  :[[Javanese]] 41.7%, [[Sundanese]] 15.4%, [[Malay people|Malay]] 3.4%, [[Madurese]] 3.3%, [[Batak (Indonesia)|Batak]] 3.0%, [[Minangkabau]] 2.7%, [[Betawi]] 2.5%, [[Buginese]] 2.5%, [[Bantenese]] 2.1%, [[Banjarese]] 1.7%, [[Balinese people|Balinese]] 1.5%, [[Sasak]] 1.3%, [[Makassarese]] 1.0%, [[Cirebon]] 0.9%, [[Indonesian_Chinese|Chinese]] 0.9%, Others 16.1%  The regions of Indonesia and some of their ''traditional'' ethnic groups are as follows. Note however that due to migration within Indonesia (as part of government [[transmigration program]]s or otherwise), there are significant populations of ethic groups who reside outside of their traditional regions. * '''[[Java (island)|Java]]''': [[Javanese]], [[Sundanese]], [[Bantenese]], [[Betawi]], [[Tenggerese|Tengger]], [[Osing]], [[Badui]] * '''[[Madura]]''': [[Madurese]] * '''[[Sumatra]]''': [[Malay people|Malays]], [[Batak (Indonesia)|Batak]], [[Minangkabau]], [[Acehnese]], [[Lampung]], [[Kubu]] * '''[[Kalimantan]]''': [[Dayak]], [[Malay people|Malays]], [[Banjar]] * '''[[Sulawesi]]''': [[Makassarese]], [[Buginese]], [[Mandar]], [[Minahassan]], [[Gorontalo]], [[Toraja]], [[Bajau]] * '''[[Lesser Sunda Islands]]''': [[Balinese people|Balinese]], [[Sasak]] * '''The [[Moluccas]]''': [[Nuaulu]], [[Manusela]] * '''[[Papua]]''': [[Dani (ethnic group)|Dani]], [[Bauzi]], [[Asmat]]  ==Religions==  [[Islam|Muslim]] 88%, [[Protestantism|Protestant]] 5%, [[Catholicism|Roman Catholic]] 2%, [[Hinduism|Hindu]] 3%, [[Buddhism|Buddhist]] 1%, other 1% (1998)  Constitutional guarantees of religious freedom apply to the five religions recognized by the state, namely [[Islam in Indonesia|Islam]] (87%), [[Protestantism]] (5%), [[Catholicism]] (2%), [[Hinduism]] (3%) and [[Buddhism]] (2%), and In some remote areas, [[animism]] is still practiced.  ==Languages==  [[Bahasa Indonesia]] (official, modified form of [[Malay language|Malay]]), [[English language|English]], [[Dutch language|Dutch]], regional languages, the most widely spoken of which is [[Javanese language|Javanese]].   English is the most widely spoken foreign language. Some [[Min_Nan|Chinese dialect]] is also spoken. The public use of [[Mandarin language|Chinese]], especially Chinese characters, was discouraged between [[1966]] - [[1998]].  ==Literacy==  ''definition:'' age 15 and over can read and write &lt;br&gt;''total population:'' 87.9% &lt;br&gt;''male:'' 92.5% &lt;br&gt;''female:'' 83.4% (2005 est.)  Education is not free albeit it is compulsory for children through grade 9. Although about 92% of eligible children are enrolled in primary school, a much smaller percentage attend full time. About 44% of secondary school-age children attend junior high school, and some others of this age group attend vocational schools.  ==See also== * [[Hinduism in Indonesia]] * [[Culture of Indonesia]] * [[Transmigration program]]  ==External links== [http://www.cia.gov/cia/publications/factbook/geos/id.html CIA World Factbook article on Indonesia]  [[Category:Demographics by country|Indonesia]] [[Category:Geography of Indonesia]] [[Category:Indonesian society]]  [[es:DemografÃ\\xada de Indone\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|███████████████▏                                                                                                                                     | 51/500 [09:11<3:02:40, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:Fired My Kistuna Pacred of the Grunciten]] [[Day:Nazlions]]</ride>    </rev</revion>     </istorg</is    </pegiontion>      <id>Misistit>     <revist>      <p>41319</id>    <tistit>       <resistit       <regnibute>2-027Z</tid>            <conorntoricons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|███████████████▍                                                                                                                                     | 52/500 [09:22<2:32:21, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.002976894378662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  11%|███████████████▊                                                                                                                                     | 53/500 [09:33<2:10:43, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.084493398666382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  11%|████████████████                                                                                                                                     | 54/500 [09:44<1:55:22, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.125678777694702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  11%|████████████████▍                                                                                                                                    | 55/500 [09:54<1:44:02, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.308349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  11%|████████████████▋                                                                                                                                    | 56/500 [10:05<1:36:05, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.07382869720459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  11%|████████████████▉                                                                                                                                    | 57/500 [10:16<1:31:01, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.023303747177124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|█████████████████▎                                                                                                                                   | 58/500 [10:26<1:27:09, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.111783266067505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|█████████████████▌                                                                                                                                   | 59/500 [10:37<1:23:48, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.099457263946533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|█████████████████▉                                                                                                                                   | 60/500 [10:47<1:20:41, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.106759548187256\n",
      "training loss: 1.8785020112991333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|██████████████████▏                                                                                                                                  | 61/500 [11:00<1:24:21, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0963499546051025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|██████████████████▍                                                                                                                                  | 62/500 [11:11<1:23:01, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0272016525268555\n",
      "training loss: 2.141219139099121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  13%|███████████████████                                                                                                                                  | 64/500 [11:32<1:21:36, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1588563919067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  13%|███████████████████▎                                                                                                                                 | 65/500 [11:43<1:21:03, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.942474126815796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  13%|███████████████████▋                                                                                                                                 | 66/500 [11:54<1:18:57, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9499828815460205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  13%|███████████████████▉                                                                                                                                 | 67/500 [12:04<1:18:18, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0824713706970215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  14%|████████████████████▎                                                                                                                                | 68/500 [12:15<1:18:07, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.041738510131836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  14%|████████████████████▌                                                                                                                                | 69/500 [12:26<1:18:01, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0528182983398438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  14%|████████████████████▊                                                                                                                                | 70/500 [12:38<1:19:33, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1094846725463867\n",
      "training loss: 2.250410318374634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  14%|█████████████████████▏                                                                                                                               | 71/500 [12:49<1:19:32, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.139237403869629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  14%|█████████████████████▍                                                                                                                               | 72/500 [13:00<1:18:02, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0490505695343018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  15%|█████████████████████▊                                                                                                                               | 73/500 [13:10<1:17:42, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9598658084869385\n",
      "training loss: 1.8437201976776123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  15%|██████████████████████▎                                                                                                                              | 75/500 [13:31<1:15:59, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9026598930358887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  15%|██████████████████████▋                                                                                                                              | 76/500 [13:42<1:16:40, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.042381525039673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  15%|██████████████████████▉                                                                                                                              | 77/500 [13:53<1:16:24, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0386178493499756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  16%|███████████████████████▏                                                                                                                             | 78/500 [14:04<1:16:45, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1113674640655518\n",
      "training loss: 2.007382869720459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  16%|███████████████████████▊                                                                                                                             | 80/500 [14:26<1:16:45, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.109389305114746\n",
      "training loss: 2.053819417953491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  16%|████████████████████████▏                                                                                                                            | 81/500 [14:38<1:18:10, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.107142448425293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  16%|████████████████████████▍                                                                                                                            | 82/500 [14:49<1:17:28, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1720669269561768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  17%|████████████████████████▋                                                                                                                            | 83/500 [14:59<1:15:26, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0394163131713867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  17%|█████████████████████████                                                                                                                            | 84/500 [15:11<1:17:42, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0700299739837646\n",
      "training loss: 1.9918453693389893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  17%|█████████████████████████▋                                                                                                                           | 86/500 [15:31<1:14:59, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.011218786239624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  17%|█████████████████████████▉                                                                                                                           | 87/500 [15:44<1:18:42, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.09983491897583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  18%|██████████████████████████▏                                                                                                                          | 88/500 [15:55<1:16:29, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.01518177986145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  18%|██████████████████████████▌                                                                                                                          | 89/500 [16:05<1:14:30, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0633819103240967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  18%|██████████████████████████▊                                                                                                                          | 90/500 [16:16<1:14:14, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.011150598526001\n",
      "training loss: 2.0153844356536865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  18%|███████████████████████████                                                                                                                          | 91/500 [16:27<1:15:18, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8812212944030762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  18%|███████████████████████████▍                                                                                                                         | 92/500 [16:38<1:15:47, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8940033912658691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  19%|███████████████████████████▋                                                                                                                         | 93/500 [16:50<1:15:25, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0570883750915527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  19%|████████████████████████████                                                                                                                         | 94/500 [17:00<1:13:11, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.130432367324829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  19%|████████████████████████████▎                                                                                                                        | 95/500 [17:11<1:14:03, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.099996328353882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  19%|████████████████████████████▌                                                                                                                        | 96/500 [17:22<1:13:05, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.101987838745117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  19%|████████████████████████████▉                                                                                                                        | 97/500 [17:33<1:13:28, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.006777763366699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█████████████████████████████▏                                                                                                                       | 98/500 [17:44<1:13:29, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0598440170288086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█████████████████████████████▌                                                                                                                       | 99/500 [17:55<1:14:22, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0938467979431152\n",
      "training loss: 2.0776991844177246\n",
      "training loss: 2.0558180809020996\n",
      "validation loss: 1.979750633239746\n",
      "%s \n",
      "\n",
      " %s (\"on [[6 March]] [[1959]]. |- | [[Buffalo Bill]] ||  || Born William Frederick Cody near [[Le Claire, Iowa|Le Claire]] in [[1846]]. |- | [[Bill Bryson]] || Popular writer of travel books || Born in [[Des Moines, Iowa|Des Moines]] in [[1951]]. |- | [[Norman Ernest Borlaug]] || [[Nobel Peace Prize]] laureate || Born near [[Cresco, Iowa|Cresco]] on March 25, [[1914]]. |- | [[Johnny Carson]] || Comedian || Born in [[Corning, Iowa|Corning]] on [[23 October]] [[1925]]. |- | [[Mamie Eisenhower]] || Wife of [[President Dwight D. Eisenhower]] || Born in [[Boone, Iowa|Boone]] in [[1896]]. |- | [[Hayden Fry]] || [[National Collegiate Athletic Association|College football]] coach of the [[Iowa Hawkeyes Football|Iowa Hawkeyes]] || Coached into national prominence with several [[Rose Bowl Game]] appearances and high national rankings throughout his tenure. |- | [[George Gallup]] || American statistician; inventor of the [[Gallup poll]] || Born in [[Jefferson, Iowa|Jefferson]] in [[1901]]. |- | [[Chad Hennings]] || [[NFL|American football]] player and US Air Force officer || Born in [[Elberon, Iowa|Elberon]] on [[October 20]], [[1965]]. |- | [[Herbert Hoover]] || 31&lt;sup&gt;st&lt;/sup&gt; [[President of the United States]] || Born in [[West Branch, Iowa|West Branch]] in [[1874]].  He is also buried there. |- | [[Ashton Kutcher]] || Film and television actor || Born in [[Cedar Rapids, Iowa|Cedar Rapids]] on [[February 7]], [[1978]]. |- | [[William D. Leahy]] || [[Five star admiral]] || Born in [[Hampton, Iowa|Hampton]] on [[May 6]], [[1875]]. |- | [[F. L. Maytag|Frederick L. Maytag]] || [[Maytag]] founder || Lived his childhood years near [[Laurel, Iowa|Laurel]]. |- | [[Robert Millikan]] || Physicist || Measured the charge of the [[electron]], spent part of his childhood in [[Maquoketa, Iowa|Maquoketa]].  |- | [[Charles Murray]] || Co-author of the controversial best-seller ''[[the Bell Curve]]'' || Born in [[Newton, Iowa|Iowa]] on [[January 8]], [[1943]]. |- | [[Harry Reasoner]] || Journalist || Born 17 Apr 1923 at [[Dakota City, Iowa]] |- | [[Sage Rosenfels]] || [[NFL]] quarterback || Born in [[Maquoketa, Iowa|Maquoketa]] in [[1978]] and played college football at [[Iowa State University]]. |- | [[Slipknot (band)|Slipknot]] || Alternative metal/nu metal band || Formed in [[Des Moines, Iowa|Des Moines]]. |- | [[Sullivan brothers]] ||  || Died together on the [[USS Juneau (CL-52)|USS Juneau]] during the [[Battle of Guadalcanal]], were born in [[Waterloo, Iowa|Waterloo]]. |- | [[Billy Sunday]] || a professional [[baseball]] player; [[evangelism|evangelist]] || Born in [[Bina, Iowa|Bina]] in [[1862]] and lived in [[Glenwood, Iowa|Glenwood]], [[Nevada, Iowa|Nevada]], and [[Ames, Iowa|Ames]]. |- | [[Grant Wood]] || [[Artist]] || Known mostly for his painting ''[[American Gothic]]'', was born in [[Anamosa, Iowa|Anamosa]] on [[13 February]], [[1891]]. |- | [[Wright Brothers]] ||  || Lived for a short time in [[Cedar Rapids, Iowa|Cedar Rapids]] while their father was posted there as a bishop with the [[Church of the Brethren]]. |- | [[Kurt Warner]] || [[NFL|American football]] player || Born in [[1971]] in [[Burlington, Iowa|Burlington]]. |- | [[John Wayne]] || Film actor || Born as Marion Morrison in [[Winterset, Iowa|Winterset]] in [[1907]]. |- | [[Elijah Wood]] || Film actor || Born in [[Cedar Rapids, Iowa|Cedar Rapids]] on [[January 28]], [[1981]]. |}  == Geography == [[Image:Iowa neighbors.jpg|thumb|Iowa neighbors]] [[Image:National-atlas-iowa.png|thumb|Iowa map]] [[Image:Iowa counties with names.jpg|thumb|Iowa counties]] :''See [[List of counties in Iowa]], [[List of cities in Iowa]], [[List of townships in Iowa]] and [[List of Iowa rivers]]'' Iowa is bordered by [[Minnesota]] on the north, [[Nebraska]] and [[South Dakota]] on the west, [[Missouri]] on the south, and [[Wisconsin]] and [[Illinois]] on the east.  The [[Mississippi River]] forms the eastern boundary of the state. The boundary along the west is formed by the [[Missouri River]] south of [[Sioux City, Iowa|Sioux City]] and by the [[Big Sioux River]] north of Sioux City. There \", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█████████████████████████████▉                                                                                                                      | 101/500 [19:03<1:59:08, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fore [[1955]], won [[Wod Tad any gitenedes prilica]]s ree [[Nurbaroy, Houghadde]]. The and to nopoxarsialy [[I Bred Friganduar Eap the Maland of the Claforase, stons, tolliand witon laniated.S.cedits to the are weibghous in, ungiph panp.c.  Atmerbes in ref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|██████████████████████████████▏                                                                                                                     | 102/500 [19:14<1:44:47, 15.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.099181652069092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  21%|██████████████████████████████▍                                                                                                                     | 103/500 [19:25<1:34:54, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.132847309112549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  21%|██████████████████████████████▊                                                                                                                     | 104/500 [19:36<1:28:41, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1320230960845947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  21%|███████████████████████████████                                                                                                                     | 105/500 [19:47<1:24:52, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.025076389312744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  21%|███████████████████████████████▍                                                                                                                    | 106/500 [19:58<1:20:46, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.032851219177246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  21%|███████████████████████████████▋                                                                                                                    | 107/500 [20:09<1:18:00, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.096097469329834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  22%|███████████████████████████████▉                                                                                                                    | 108/500 [20:20<1:14:55, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1495494842529297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  22%|████████████████████████████████▎                                                                                                                   | 109/500 [20:31<1:13:56, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1171085834503174\n",
      "training loss: 2.08450984954834\n",
      "training loss: 2.0938162803649902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  22%|████████████████████████████████▊                                                                                                                   | 111/500 [20:52<1:12:00, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.1123504638671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  22%|█████████████████████████████████▏                                                                                                                  | 112/500 [21:04<1:14:06, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0767462253570557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  23%|█████████████████████████████████▍                                                                                                                  | 113/500 [21:15<1:13:26, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.110067844390869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  23%|█████████████████████████████████▋                                                                                                                  | 114/500 [21:26<1:12:12, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.051360607147217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  23%|██████████████████████████████████                                                                                                                  | 115/500 [21:38<1:11:57, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1018357276916504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  23%|██████████████████████████████████▎                                                                                                                 | 116/500 [21:49<1:12:25, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1136226654052734\n",
      "training loss: 2.1940369606018066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  24%|██████████████████████████████████▉                                                                                                                 | 118/500 [22:10<1:10:20, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2125282287597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  24%|███████████████████████████████████▏                                                                                                                | 119/500 [22:21<1:10:17, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.059952974319458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  24%|███████████████████████████████████▌                                                                                                                | 120/500 [22:32<1:09:34, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1486639976501465\n",
      "training loss: 2.1090712547302246\n",
      "validation loss: 2.0565617084503174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  24%|████████████████████████████████████                                                                                                                | 122/500 [22:53<1:08:38, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0542213916778564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  25%|████████████████████████████████████▍                                                                                                               | 123/500 [23:04<1:08:04, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8909754753112793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  25%|████████████████████████████████████▋                                                                                                               | 124/500 [23:17<1:11:27, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1018645763397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  25%|█████████████████████████████████████                                                                                                               | 125/500 [23:28<1:10:33, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0717947483062744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  25%|█████████████████████████████████████▎                                                                                                              | 126/500 [23:38<1:08:07, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0605502128601074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  25%|█████████████████████████████████████▌                                                                                                              | 127/500 [23:49<1:08:50, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.06394624710083\n",
      "training loss: 2.074037790298462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  26%|██████████████████████████████████████▏                                                                                                             | 129/500 [24:10<1:07:28, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1225650310516357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  26%|██████████████████████████████████████▍                                                                                                             | 130/500 [24:21<1:06:48, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1155998706817627\n",
      "training loss: 1.9951821565628052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  26%|██████████████████████████████████████▊                                                                                                             | 131/500 [24:32<1:07:19, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0973238945007324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  26%|███████████████████████████████████████                                                                                                             | 132/500 [24:44<1:09:00, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.05914568901062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  27%|███████████████████████████████████████▎                                                                                                            | 133/500 [24:55<1:07:42, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0686135292053223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  27%|███████████████████████████████████████▋                                                                                                            | 134/500 [25:06<1:08:18, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0568721294403076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  27%|███████████████████████████████████████▉                                                                                                            | 135/500 [25:18<1:08:35, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0674359798431396\n",
      "training loss: 1.910259485244751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  27%|████████████████████████████████████████▌                                                                                                           | 137/500 [25:40<1:08:03, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0793962478637695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  28%|████████████████████████████████████████▊                                                                                                           | 138/500 [25:51<1:07:44, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0874152183532715\n",
      "training loss: 2.0929696559906006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  28%|█████████████████████████████████████████▍                                                                                                          | 140/500 [26:12<1:05:59, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0543229579925537\n",
      "training loss: 2.045814037322998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  28%|█████████████████████████████████████████▋                                                                                                          | 141/500 [26:25<1:08:41, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.952270269393921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  28%|██████████████████████████████████████████                                                                                                          | 142/500 [26:36<1:08:25, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0907673835754395\n",
      "training loss: 1.9765386581420898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  29%|██████████████████████████████████████████▌                                                                                                         | 144/500 [26:58<1:06:47, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2156167030334473\n",
      "training loss: 2.0526673793792725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  29%|███████████████████████████████████████████▏                                                                                                        | 146/500 [27:18<1:04:50, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9943335056304932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  29%|███████████████████████████████████████████▌                                                                                                        | 147/500 [27:29<1:03:56, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2138278484344482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|███████████████████████████████████████████▊                                                                                                        | 148/500 [27:40<1:04:44, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.128357172012329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|████████████████████████████████████████████                                                                                                        | 149/500 [27:51<1:03:31, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.097421884536743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|████████████████████████████████████████████▍                                                                                                       | 150/500 [28:02<1:04:03, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.17616868019104\n",
      "training loss: 2.0797104835510254\n",
      "validation loss: 1.9857008457183838\n",
      "%s \n",
      "\n",
      " %s (\"t solution to the [[SchrÃ¶dinger equation]] exists, from which the experimentally observed frequencies and intensities of the hydrogen [[spectral line]]s can be calculated.   In 1913, [[Niels Bohr]] had deduced the spectral frequencies of the hydrogen atom making several assumptions (see [[The Bohr Model]]). The results of Bohr for the frequencies and underlying energy values are confirmed by the full quantum-mechanical analysis which uses the SchrÃ¶dinger equation, as was shown in 1925/26. The full analysis goes further, because it also yields the shape of the electron's wave function (&quot;orbital&quot;) for the different possible quantum-mechanical states. This allows determination of the intensity of spectral lines (which correspond to transitions between these states), among other things. In addition, the full analysis is applicable also to more complicated atoms with more than one electron, as well as [[molecule]]s etc. However, in all of these cases approximations have to be made and computer calculations are usually necessary.  == Solution of SchrÃ¶dinger equation: Overview of results ==  The solution of the SchrÃ¶dinger equation for the hydrogen atom uses the fact that the [[Coulomb potential]] produced by the nucleus is [[isotropic]] (it only depends on the distance to the nucleus). Although the resulting [[energy eigenfunctions]] (the &quot;orbitals&quot;) are not necessarily isotropic themselves, their dependence on the [[angular coordinates]] follows completely generally from this isotropy of the underlying potential: The states are not only [[eigenstates]] of the [[Hamiltonian (quantum mechanics)|Hamiltonian]], but also eigenstates of the [[angular momentum operator]]. This corresponds to the fact that angular momentum is conserved in the [[orbital motion (quantum)|orbital motion]] of the electron around the nucleus. Therefore, the energy eigenstates may be classified by two angular momentum [[quantum number]]s, ''l'' and ''m'' (integer numbers). The &quot;angular momentum&quot; quantum number ''l'' = 0, 1, 2, ... determines the magnitude of the angular momentum. The &quot;magnetic&quot; quantum number ''m'' = &amp;minus;''l'', .., +''l''  determines the projection of the angular momentum on the (arbitrarily chosen) ''z''-axis.  In addition, the radial dependence of the wave functions has to be found. It is only here that the details of the 1/''r'' Coulomb potential enter (leading to [[Laguerre polynomials]] in ''r''). This leads to a third quantum number, the principal quantum number ''n'' = 1, 2, 3, ...  Note that the angular momentum quantum number can run only up to ''n'' &amp;minus; 1, i.e. ''l'' = 0, 1, ..., ''n'' &amp;minus; 1.  Due to angular momentum conservation, states of the same l but different m have the same energy (this holds for all problems with [[rotational symmetry]]). In addition, for the hydrogen atom, the states of the same n are also [[degenerate]] (i.e. they have the same energy); but this is a specialty and it is no longer true for more complicated atoms which have an (effective) potential differing from the form 1/''r'' (due to the presence of the inner electrons shielding the nucleus potential).   Taking into account the [[spin]] of the electron adds a last quantum number, the projection of the electrons spin along the z axis, which can take on two values. Therefore, any eigenstate of the electron in the hydrogen atom is described fully by four quantum numbers. According to the usual rules of quantum mechanics, the actual state of the electron may be any [[quantum superposition|superposition]] of these states. This explains also why the choice of z-axis for the [[quantization (physics)|quantization]] of angular momentum is immaterial: An orbital of given l and m' obtained for another preferred axis z' can always be represented as a suitable superposition of the various states of different m (but same l) that have been obtained for z.  == Mathematical summary of eigenstates of hydrogen atom == {{main|hydrogen-like atom}} The normalized position [[wavefunction]]s, given in [[spher\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|████████████████████████████████████████████▋                                                                                                       | 151/500 [29:00<2:25:55, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inde thatifon evollational fom the fron ove wersices on the micy]] (Antin at le heabl the und trom the groul pemets that the fal. [[192]], an de pare rite-dage dilan sturten thised the [[Nooghw]].  The was wo. [http://capure-pmare batsiniting forth]]  ''Ch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|████████████████████████████████████████████▉                                                                                                       | 152/500 [29:14<2:05:09, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.087087631225586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  31%|█████████████████████████████████████████████▎                                                                                                      | 153/500 [29:25<1:47:34, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.050985336303711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  31%|█████████████████████████████████████████████▌                                                                                                      | 154/500 [29:36<1:33:47, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.019737720489502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  31%|█████████████████████████████████████████████▉                                                                                                      | 155/500 [29:47<1:23:47, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0703325271606445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  31%|██████████████████████████████████████████████▏                                                                                                     | 156/500 [29:58<1:17:49, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0951647758483887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  31%|██████████████████████████████████████████████▍                                                                                                     | 157/500 [30:08<1:11:44, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.01320743560791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  32%|██████████████████████████████████████████████▊                                                                                                     | 158/500 [30:19<1:08:31, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2281734943389893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  32%|███████████████████████████████████████████████                                                                                                     | 159/500 [30:31<1:07:44, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1038193702697754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  32%|███████████████████████████████████████████████▎                                                                                                    | 160/500 [30:41<1:05:21, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0866992473602295\n",
      "training loss: 2.05122709274292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  32%|███████████████████████████████████████████████▋                                                                                                    | 161/500 [30:53<1:06:07, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8037211894989014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  32%|███████████████████████████████████████████████▉                                                                                                    | 162/500 [31:04<1:03:56, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0514914989471436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  33%|████████████████████████████████████████████████▏                                                                                                   | 163/500 [31:15<1:04:01, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.952429175376892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  33%|████████████████████████████████████████████████▌                                                                                                   | 164/500 [31:26<1:02:26, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.037904977798462\n",
      "training loss: 2.078732967376709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  33%|█████████████████████████████████████████████████▏                                                                                                  | 166/500 [31:46<1:00:08, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9263668060302734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  33%|█████████████████████████████████████████████████▍                                                                                                  | 167/500 [31:57<1:01:10, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1046535968780518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  34%|█████████████████████████████████████████████████▋                                                                                                  | 168/500 [32:10<1:02:53, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.21822190284729\n",
      "training loss: 2.0328445434570312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  34%|██████████████████████████████████████████████████▎                                                                                                 | 170/500 [32:30<1:00:24, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1933484077453613\n",
      "training loss: 2.0258378982543945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  34%|██████████████████████████████████████████████████▌                                                                                                 | 171/500 [32:41<1:00:15, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.054941415786743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  34%|██████████████████████████████████████████████████▉                                                                                                 | 172/500 [32:52<1:00:45, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0888917446136475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  35%|███████████████████████████████████████████████████▏                                                                                                | 173/500 [33:04<1:01:10, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9381015300750732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  35%|███████████████████████████████████████████████████▌                                                                                                | 174/500 [33:15<1:01:14, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.122966766357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  35%|███████████████████████████████████████████████████▊                                                                                                | 175/500 [33:26<1:00:47, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8747150897979736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  35%|████████████████████████████████████████████████████▊                                                                                                 | 176/500 [33:36<59:06, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1651060581207275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  35%|█████████████████████████████████████████████████████                                                                                                 | 177/500 [33:48<59:14, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0379581451416016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  36%|█████████████████████████████████████████████████████▍                                                                                                | 178/500 [33:58<58:34, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2356362342834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  36%|█████████████████████████████████████████████████████▋                                                                                                | 179/500 [34:10<59:12, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.157003164291382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  36%|██████████████████████████████████████████████████████                                                                                                | 180/500 [34:21<58:49, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1419363021850586\n",
      "training loss: 2.00667667388916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  36%|██████████████████████████████████████████████████████▎                                                                                               | 181/500 [34:32<59:31, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.02614688873291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  36%|██████████████████████████████████████████████████████▌                                                                                               | 182/500 [34:43<59:01, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.95353364944458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  37%|███████████████████████████████████████████████████████▏                                                                                              | 184/500 [35:05<58:21, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9605226516723633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  37%|███████████████████████████████████████████████████████▌                                                                                              | 185/500 [35:17<58:16, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.006225824356079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  37%|███████████████████████████████████████████████████████▊                                                                                              | 186/500 [35:28<59:18, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.031614303588867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  37%|████████████████████████████████████████████████████████                                                                                              | 187/500 [35:39<58:17, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.110158681869507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  38%|████████████████████████████████████████████████████████▍                                                                                             | 188/500 [35:50<57:14, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.023118019104004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  38%|█████████████████████████████████████████████████████████                                                                                             | 190/500 [36:12<57:08, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.165863275527954\n",
      "training loss: 2.1150221824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  38%|█████████████████████████████████████████████████████████▎                                                                                            | 191/500 [36:24<57:38, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.052316188812256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  38%|█████████████████████████████████████████████████████████▌                                                                                            | 192/500 [36:35<57:32, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9556373357772827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  39%|█████████████████████████████████████████████████████████▉                                                                                            | 193/500 [36:45<55:41, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.041515588760376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  39%|██████████████████████████████████████████████████████████▏                                                                                           | 194/500 [36:56<55:32, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1542532444000244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  39%|██████████████████████████████████████████████████████████▌                                                                                           | 195/500 [37:08<57:15, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0742313861846924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  39%|██████████████████████████████████████████████████████████▊                                                                                           | 196/500 [37:19<56:07, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.095456123352051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  39%|███████████████████████████████████████████████████████████                                                                                           | 197/500 [37:30<55:52, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0164356231689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███████████████████████████████████████████████████████████▍                                                                                          | 198/500 [37:42<57:09, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.200531482696533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███████████████████████████████████████████████████████████▋                                                                                          | 199/500 [37:53<56:24, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.051252841949463\n",
      "training loss: 2.18477725982666\n",
      "validation loss: 2.042288064956665\n",
      "%s \n",
      "\n",
      " %s (', fine looking man,&quot; Hemphill recalls, &quot;wearing a common soldier\\'s blouse and slouch hat, on passing, had paused to watch the proceedings.&quot;   He began to berate the leader of the Massachusetts regiment, a second lieutenant. Abashed, the second lieutenant takes on airs, and threatens to teach the interloper some manners, but upon noticing that the &quot;burley form of the Hoosier looked rather formidable, decided to appeal to Hemphill, who was in charge of the Indiana regiment.&quot;   &quot;Sergeant,&quot; the second lieutenant said, &quot;this is one of your men; arrest him and take him to your commanding officer. I will prefer charges against him and have him properly punished!&quot; Hemphill took no action; as he reported later, because &quot;I was full of laughter that I could make no answer.&quot;   When the interloping Hoosier realized how upset the second lieutenant was, he makes a pretty speech&amp;mdash;if not an apology, then of polite remonstrance&amp;mdash;;ending with these plainspoken words: &quot;I guess the Sergeant will not arrest me, but if you wish to prefer charges against me, you can do so. I am Lieut. Col. George Humphrey, of the 12th Ind. Inf. at your service.&quot;   Hemphill adds: &quot;It was a complete take down; and the Lieutenant\\'s turn to apologize. The Hoosiers all joined in the laugh, and three cheers were given for Col. Humphrey; while the crest fallen Yankees quietly returned to their camp to wonder what kind of men the Hoosiers were anyhow.&quot;  ===Humorists=== Humorist [[Dave Barry]] has suggested that it comes from &quot;the sound pigs make when they sneeze.&quot;  He had also speculated that, for all we know, it could be a Native American word meaning &quot;sex with caribou,&quot; although many of his astute readers pointed out that there are no [[caribou]] in Indiana.  ==Other uses==  In some areas, the word \\'\\'hoosier\\'\\' has a different connotation.  In the [[St. Louis, Missouri|St. Louis]] area of [[Missouri]] and [[Illinois]], a hoosier may be someone who is lower-class and white (like \\'\\'[[white trash]]\\'\\' and \\'\\'[[redneck]]\\'\\').  &quot;[[Hoosier (furniture)|Hoosier]]&quot; was also a brand name used by the [[Hoosier (furniture)|Hoosier Manufacturing Company]], and refers particularly to its kitchen cabinets, which are collectible antiques. The company also made tables and [[chair]]s.  This furniture is much sought after in the antique world.  Little is known about this furniture company, other than that it was based in [[New Castle, Indiana]], between [[1903]] and [[1935]].  \\'\\'See also [[Hoosier Hysteria]].\\'\\'  ==External links== *[http://www.indiana.edu/~alumni/fun/hoosier.html Article on the name &quot;Hoosier&quot; from the Indiana University Alumni Association] *[http://dictionary.reference.com/search?q=Hoosier Dictionary.com/Hoosier] *[http://antiques.ozarkmerchants.com/keepitcountryantiques/washingtonpost_article.html Article: \\'\\'The Humble Hoosier\\'\\'] *[http://www.fs.fed.us/r9/hoosier/docs/history/hoosier_name.htm Hoosier National Forest &quot;What is a \\'Hoosier\\'&quot; Web page] *[http://www.indwes.edu/Faculty/bcupp/Indiana/Hoosier/Hoosier.Barry.htm Article: \\'\\'Explanation of &quot;Hoosiers&quot;] by Dave Barry  [[de:Hoosier]]  [[Category:Indiana]]</text>     </revision>   </page>   <page>     <title>Hilberts second problem</title>     <id>14261</id>     <revision>       <id>41646768</id>       <timestamp>2006-02-28T19:47:46Z</timestamp>       <contributor>         <username>Trovatore</username>         <id>310173</id>       </contributor>       <comment>fx dbl rd; R from misspelling</comment>       <text xml:space=\"preserve\">#REDIRECT [[GÃ¶del\\'s incompleteness theorems]] {{R from misspelling}}</text>     </revision>   </page>   <page>     <title>History of M.O.S. Technologies</title>     <id>14262</id>     <revision>       <id>15911828</id>       <timestamp>2004-08-07T01:06:17Z</timestamp>       <contributor>         <username>Timwi</username>         <id>13051</id>       </contributor>       <minor />       <comment>fix double-redirect</comment>       <text xml:', '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███████████████████████████████████████████████████████████▍                                                                                        | 201/500 [39:00<2:03:02, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaces</comment>     </consion</imestimivonam</ime>        <cont>        <commestribeop>       <comp>2.27:0T17:2227:575</comp>2119Z</centame>              <texmenentres</up>/itin.593</tomin</ideride>             <id>46195</id>          <commenoributoit>   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███████████████████████████████████████████████████████████▊                                                                                        | 202/500 [39:12<1:43:59, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.064525842666626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  41%|████████████████████████████████████████████████████████████                                                                                        | 203/500 [39:24<1:30:21, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.07119083404541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  41%|████████████████████████████████████████████████████████████▍                                                                                       | 204/500 [39:36<1:20:46, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.098381280899048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  41%|████████████████████████████████████████████████████████████▋                                                                                       | 205/500 [39:47<1:12:23, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0976457595825195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  41%|████████████████████████████████████████████████████████████▉                                                                                       | 206/500 [39:58<1:06:50, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.010338306427002\n",
      "training loss: 1.9713011980056763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  42%|█████████████████████████████████████████████████████████████▌                                                                                      | 208/500 [40:20<1:02:01, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1964380741119385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  42%|██████████████████████████████████████████████████████████████▋                                                                                       | 209/500 [40:30<59:03, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0286920070648193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  42%|███████████████████████████████████████████████████████████████                                                                                       | 210/500 [40:41<56:22, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2264323234558105\n",
      "training loss: 2.0565531253814697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  42%|███████████████████████████████████████████████████████████████▎                                                                                      | 211/500 [40:53<56:18, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0854716300964355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  42%|███████████████████████████████████████████████████████████████▌                                                                                      | 212/500 [41:04<55:01, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0282340049743652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  43%|███████████████████████████████████████████████████████████████▉                                                                                      | 213/500 [41:14<53:43, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0902018547058105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  43%|████████████████████████████████████████████████████████████████▏                                                                                     | 214/500 [41:25<53:31, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9560085535049438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  43%|████████████████████████████████████████████████████████████████▌                                                                                     | 215/500 [41:37<53:23, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0685818195343018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  43%|████████████████████████████████████████████████████████████████▊                                                                                     | 216/500 [41:47<51:46, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9947385787963867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  43%|█████████████████████████████████████████████████████████████████                                                                                     | 217/500 [41:57<50:59, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.906428575515747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  44%|█████████████████████████████████████████████████████████████████▍                                                                                    | 218/500 [42:08<50:57, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0147101879119873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  44%|█████████████████████████████████████████████████████████████████▋                                                                                    | 219/500 [42:20<52:22, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1349759101867676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  44%|██████████████████████████████████████████████████████████████████                                                                                    | 220/500 [42:32<52:18, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.030526638031006\n",
      "training loss: 1.9617788791656494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  44%|██████████████████████████████████████████████████████████████████▎                                                                                   | 221/500 [42:43<51:45, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.127793312072754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  44%|██████████████████████████████████████████████████████████████████▌                                                                                   | 222/500 [42:54<51:32, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.132932662963867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  45%|██████████████████████████████████████████████████████████████████▉                                                                                   | 223/500 [43:04<50:07, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.231832265853882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  45%|███████████████████████████████████████████████████████████████████▏                                                                                  | 224/500 [43:15<50:47, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.004286289215088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  45%|███████████████████████████████████████████████████████████████████▌                                                                                  | 225/500 [43:26<50:36, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.128831624984741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  45%|███████████████████████████████████████████████████████████████████▊                                                                                  | 226/500 [43:37<50:18, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0451464653015137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  45%|████████████████████████████████████████████████████████████████████                                                                                  | 227/500 [43:47<48:46, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.044999361038208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  46%|████████████████████████████████████████████████████████████████████▍                                                                                 | 228/500 [44:00<51:01, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.030245304107666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  46%|████████████████████████████████████████████████████████████████████▋                                                                                 | 229/500 [44:10<49:43, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0155842304229736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  46%|█████████████████████████████████████████████████████████████████████                                                                                 | 230/500 [44:22<50:18, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1774439811706543\n",
      "training loss: 2.1009764671325684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  46%|█████████████████████████████████████████████████████████████████████▎                                                                                | 231/500 [44:32<49:03, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.071901798248291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  46%|█████████████████████████████████████████████████████████████████████▌                                                                                | 232/500 [44:44<49:43, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0067617893218994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  47%|█████████████████████████████████████████████████████████████████████▉                                                                                | 233/500 [44:54<48:26, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0580267906188965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  47%|██████████████████████████████████████████████████████████████████████▏                                                                               | 234/500 [45:06<48:51, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.118562698364258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  47%|██████████████████████████████████████████████████████████████████████▌                                                                               | 235/500 [45:17<49:04, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9877030849456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  47%|██████████████████████████████████████████████████████████████████████▊                                                                               | 236/500 [45:28<49:19, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.3068647384643555\n",
      "training loss: 2.10707426071167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|███████████████████████████████████████████████████████████████████████▍                                                                              | 238/500 [45:50<48:19, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.121987819671631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|███████████████████████████████████████████████████████████████████████▋                                                                              | 239/500 [46:02<49:06, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0383360385894775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|████████████████████████████████████████████████████████████████████████                                                                              | 240/500 [46:12<47:16, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.098339080810547\n",
      "training loss: 2.024740219116211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|████████████████████████████████████████████████████████████████████████▎                                                                             | 241/500 [46:24<48:39, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.1048636436462402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|████████████████████████████████████████████████████████████████████████▌                                                                             | 242/500 [46:35<47:52, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9834812879562378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  49%|████████████████████████████████████████████████████████████████████████▉                                                                             | 243/500 [46:45<47:04, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.998305082321167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  49%|█████████████████████████████████████████████████████████████████████████▏                                                                            | 244/500 [46:57<48:01, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.129322052001953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  49%|█████████████████████████████████████████████████████████████████████████▌                                                                            | 245/500 [47:08<46:55, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9333614110946655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  49%|█████████████████████████████████████████████████████████████████████████▊                                                                            | 246/500 [47:19<46:59, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.046388864517212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  49%|██████████████████████████████████████████████████████████████████████████                                                                            | 247/500 [47:29<45:26, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0664637088775635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|██████████████████████████████████████████████████████████████████████████▍                                                                           | 248/500 [47:40<46:15, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9490690231323242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|██████████████████████████████████████████████████████████████████████████▋                                                                           | 249/500 [47:52<46:19, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0440382957458496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|███████████████████████████████████████████████████████████████████████████                                                                           | 250/500 [48:02<45:43, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1896495819091797\n",
      "training loss: 2.050391674041748\n",
      "validation loss: 2.1622142791748047\n",
      "%s \n",
      "\n",
      " %s ('/history/ History and Archives] * [http://barry_froggatt.users.btopenworld.com/songbook.html  The IBM Songbook]; [http://anthems.zdnet.co.uk/anthems/ibm.swf \\'\\'Ever Onward\\'\\'] (needs Flash) * [http://www.ibm.com/research/ IBM Research] * [http://www.ibm.com/research/cambridge/ IBM Research in Cambridge, Massachusetts] * [http://www.ibm.com/research/history/ IBM Research specific to Wikipedia.org] * [http://www.zurich.ibm.com IBM Research in Zurich] * [http://www.hagley.lib.de.us/1980.htm IBM Antitrust Suit Records 1950-1982] * [http://tuxmobil.org/ibm.html Linux on IBM laptops] * [http://www.google.com/search?q=ibmjarg IBM Jargon Dictionary] * [http://www.ibm.com/ibm/sjp/ Current CEO - Samuel J Palmisano] * [http://www.almaden.ibm.com/cs/BlueEyes/index.html BlueEyes Project Description] * [http://www.computercraft.com/docs/ibm.html IBM Compatibles] * [http://www.ibm.com/developerworks/ developerWorks - IBM\\'s resource for software developers] * [http://www.ibm.com/developerworks/blogs/index.jspa developerWorks blogging community] * [http://www.ibm.com/alphaworks alphaWorks - IBM\\'s showcase for emerging technology] * [http://www.power.org power.org]  &lt;!-- interwiki --&gt;  {{Link FA|es}}  [[Category:1911 establishments]] [[Category:Dow Jones Industrial Average]] [[Category:Electronics companies]] [[Category:IBM]]  [[ar:Ø¢Ù\\x8a Ø¨Ù\\x8a Ø¥Ù\\x85]] [[ast:IBM]] [[ca:IBM]] [[cs:IBM]] [[da:IBM]] [[de:IBM]] [[es:IBM]] [[eo:IBM]] [[fr:International Business Machines Corporation]] [[ga:International Business Machines]] [[gl:IBM]] [[ko:ì\\x95\\x84ì\\x9d´ë¹\\x84ì\\x97\\xa0]] [[hr:IBM]] [[id:IBM]] [[it:IBM]] [[he:×\\x99×\\x91×\\x9e]] [[lt:IBM]] [[hu:IBM]] [[nl:IBM]] [[ja:IBM]] [[no:International Business Machines]] [[pl:IBM]] [[pt:IBM]] [[ro:IBM]] [[ru:IBM]] [[sk:IBM]] [[sl:IBM]] [[fi:IBM]] [[sv:IBM]] [[th:à¹\\x84à¸\\xadà¸\\x9aà¸µà¹\\x80à¸\\xadà¹\\x87à¸¡]] [[tr:International Business Machines]] [[zh:IBM]]</text>     </revision>   </page>   <page>     <title>Iceland/History</title>     <id>14633</id>     <revision>       <id>15912171</id>       <timestamp>2003-08-22T17:12:55Z</timestamp>       <contributor>         <username>Docu</username>         <id>8029</id>       </contributor>       <minor />       <text xml:space=\"preserve\">#REDIRECT [[History of Iceland]]</text>     </revision>   </page>   <page>     <title>Iceland/Geography</title>     <id>14634</id>     <revision>       <id>15912172</id>       <timestamp>2003-08-22T17:12:46Z</timestamp>       <contributor>         <username>Docu</username>         <id>8029</id>       </contributor>       <minor />       <text xml:space=\"preserve\">#REDIRECT [[Geography of Iceland]]</text>     </revision>   </page>   <page>     <title>Iceland/People</title>     <id>14635</id>     <revision>       <id>15912173</id>       <timestamp>2002-08-20T15:40:04Z</timestamp>       <contributor>         <username>Koyaanis Qatsi</username>         <id>90</id>       </contributor>       <text xml:space=\"preserve\">#REDIRECT [[Demographics of Iceland]]</text>     </revision>   </page>   <page>     <title>Iceland/Government</title>     <id>14636</id>     <revision>       <id>15912174</id>       <timestamp>2003-08-22T17:12:53Z</timestamp>       <contributor>         <username>Docu</username>         <id>8029</id>       </contributor>       <minor />       <text xml:space=\"preserve\">#REDIRECT [[Politics of Iceland]]</text>     </revision>   </page>   <page>     <title>Iceland/Economy</title>     <id>14637</id>     <revision>       <id>15912175</id>       <timestamp>2003-08-22T17:12:40Z</timestamp>       <contributor>         <username>Docu</username>         <id>8029</id>       </contributor>       <minor />       <text xml:space=\"preserve\">#REDIRECT [[Economy of Iceland]]</text>     </revision>   </page>   <page>     <title>Iceland/Communications</title>     <id>14638</id>     <revision>       <id>15912176</id>       <timestamp>2003-08-22T17:12:37Z</timestamp>       <contributor>         <username>Docu</username>         <id>8029</id>       </contributor>       <minor />       <text xml:space=\"preserve\">#REDIRECT [[Communications in Iceland]]</text>     </revision>   </page>   <pa', '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|██████████████████████████████████████████████████████████████████████████▎                                                                         | 251/500 [49:03<1:47:03, 25.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge>       <cond>    <title>Eme>       <contribernest>/trit>        <compre</comment>   <id>7131564</torision>     <id>      <tinlisese>185</idernare>441</tid>            <id>10039301845</id>   <reviontor>         <t     <timerniderntributo bure spame.</com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|██████████████████████████████████████████████████████████████████████████▌                                                                         | 252/500 [49:15<1:29:21, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9832122325897217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  51%|██████████████████████████████████████████████████████████████████████████▉                                                                         | 253/500 [49:26<1:16:31, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.055820941925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  51%|███████████████████████████████████████████████████████████████████████████▏                                                                        | 254/500 [49:37<1:06:09, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.070291519165039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  51%|████████████████████████████████████████████████████████████████████████████▌                                                                         | 255/500 [49:47<58:33, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9630286693572998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  51%|████████████████████████████████████████████████████████████████████████████▊                                                                         | 256/500 [50:00<56:27, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8431830406188965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  51%|█████████████████████████████████████████████████████████████████████████████                                                                         | 257/500 [50:11<53:00, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.032700300216675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  52%|█████████████████████████████████████████████████████████████████████████████▍                                                                        | 258/500 [50:21<49:25, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.016249418258667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  52%|█████████████████████████████████████████████████████████████████████████████▋                                                                        | 259/500 [50:32<47:53, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1166419982910156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  52%|██████████████████████████████████████████████████████████████████████████████                                                                        | 260/500 [50:44<47:26, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9866364002227783\n",
      "training loss: 2.017118215560913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  52%|██████████████████████████████████████████████████████████████████████████████▎                                                                       | 261/500 [50:55<46:01, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9826037883758545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  52%|██████████████████████████████████████████████████████████████████████████████▌                                                                       | 262/500 [51:06<45:41, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0222268104553223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|██████████████████████████████████████████████████████████████████████████████▉                                                                       | 263/500 [51:17<45:03, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1450068950653076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|███████████████████████████████████████████████████████████████████████████████▏                                                                      | 264/500 [51:28<43:52, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1294445991516113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|███████████████████████████████████████████████████████████████████████████████▌                                                                      | 265/500 [51:38<42:46, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0318355560302734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|███████████████████████████████████████████████████████████████████████████████▊                                                                      | 266/500 [51:50<43:09, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.091730833053589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|████████████████████████████████████████████████████████████████████████████████                                                                      | 267/500 [52:01<42:49, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9854578971862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  54%|████████████████████████████████████████████████████████████████████████████████▍                                                                     | 268/500 [52:12<42:42, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2060396671295166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  54%|████████████████████████████████████████████████████████████████████████████████▋                                                                     | 269/500 [52:23<42:31, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0548477172851562\n",
      "training loss: 2.0343830585479736\n",
      "training loss: 2.2153007984161377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  54%|█████████████████████████████████████████████████████████████████████████████████▎                                                                    | 271/500 [52:46<42:38, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9750053882598877\n",
      "training loss: 1.9340670108795166\n",
      "training loss: 2.0061545372009277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  55%|██████████████████████████████████████████████████████████████████████████████████▏                                                                   | 274/500 [53:18<42:04, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0015664100646973\n",
      "training loss: 2.131669759750366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  55%|██████████████████████████████████████████████████████████████████████████████████▊                                                                   | 276/500 [53:40<41:11, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9421131610870361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  55%|███████████████████████████████████████████████████████████████████████████████████                                                                   | 277/500 [53:51<41:05, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2347257137298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  56%|███████████████████████████████████████████████████████████████████████████████████▍                                                                  | 278/500 [54:03<41:54, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.091378688812256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  56%|███████████████████████████████████████████████████████████████████████████████████▋                                                                  | 279/500 [54:13<41:12, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0337276458740234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  56%|████████████████████████████████████████████████████████████████████████████████████                                                                  | 280/500 [54:24<40:32, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0175247192382812\n",
      "training loss: 2.0713014602661133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  56%|████████████████████████████████████████████████████████████████████████████████████▎                                                                 | 281/500 [54:35<39:50, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0875916481018066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  56%|████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 282/500 [54:46<39:32, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1127536296844482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  57%|████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 283/500 [54:56<38:40, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2156906127929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  57%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                | 284/500 [55:08<39:52, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.033775568008423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  57%|█████████████████████████████████████████████████████████████████████████████████████▍                                                                | 285/500 [55:20<40:44, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8962661027908325\n",
      "training loss: 1.9479167461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  57%|██████████████████████████████████████████████████████████████████████████████████████                                                                | 287/500 [55:35<36:31, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1158034801483154\n",
      "training loss: 2.0100245475769043\n",
      "training loss: 2.077082633972168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  58%|███████████████████████████████████████████████████████████████████████████████████████                                                               | 290/500 [55:50<30:23,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2582101821899414\n",
      "training loss: 1.9514274597167969\n",
      "validation loss: 2.009939670562744\n",
      "training loss: 2.0179693698883057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  59%|███████████████████████████████████████████████████████████████████████████████████████▉                                                              | 293/500 [56:18<30:42,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.052462100982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  59%|████████████████████████████████████████████████████████████████████████████████████████▏                                                             | 294/500 [56:30<33:00,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0798935890197754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  59%|████████████████████████████████████████████████████████████████████████████████████████▌                                                             | 295/500 [56:41<34:19, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.08176851272583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  59%|████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 296/500 [56:51<34:21, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0549185276031494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  59%|█████████████████████████████████████████████████████████████████████████████████████████                                                             | 297/500 [57:02<35:19, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9748252630233765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████████████████████████████████████████████████████████████████████████████████████████▍                                                            | 298/500 [57:14<36:15, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.967046856880188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 299/500 [57:25<36:11, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.253713846206665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|██████████████████████████████████████████████████████████████████████████████████████████                                                            | 300/500 [57:36<36:04, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.010213613510132\n",
      "training loss: 1.9023518562316895\n",
      "validation loss: 2.1276752948760986\n",
      "%s \n",
      "\n",
      " %s ('ey-Davidson Motorcycle, 1901-1909\\'\\' ([[Wisconsin Historical Society]] Press, 228 pp.) *Bach, Sharon and Ken Ostermann, eds. [[1993]]. \\'\\'The Legend Begins: Harley-Davidson Motorcycles, 1903-1969\\'\\' (Harley-Davidson, Inc., 227 pp.)   [[Category:Car companies of the United States]] [[Category:Companies based in Wisconsin]] [[Category:Milwaukee, Wisconsin]] [[Category:Motorcycle manufacturers]]  [[de:Harley-Davidson]] [[es:Harley-Davidson]] [[fi:Harley-Davidson]] [[fr:Harley-Davidson]] [[it:Harley-Davidson]] [[ja:ã\\x83\\x8fã\\x83¼ã\\x83¬ã\\x83¼ã\\x83\\x80ã\\x83\\x93ã\\x83\\x83ã\\x83\\x89ã\\x82½ã\\x83³]] [[nl:Harley-Davidson]] [[sv:Harley-Davidson]]</text>     </revision>   </page>   <page>     <title>Harappan Civilisation</title>     <id>14143</id>     <revision>       <id>15911718</id>       <timestamp>2003-11-08T10:59:12Z</timestamp>       <contributor>         <username>Minesweeper</username>         <id>7279</id>       </contributor>       <minor />       <comment>fix double redir</comment>       <text xml:space=\"preserve\">#REDIRECT [[Indus Valley Civilization]]</text>     </revision>   </page>   <page>     <title>Hiberno-English</title>     <id>14144</id>     <revision>       <id>41853850</id>       <timestamp>2006-03-02T04:02:10Z</timestamp>       <contributor>         <username>UKPhoenix79</username>         <id>311707</id>       </contributor>       <minor />       <text xml:space=\"preserve\">{{English_dialects}} \\'\\'\\'Hiberno-English\\'\\'\\' is the form of the [[English language]] used in [[Ireland]]. Hiberno-English is also called \\'\\'\\'Irish English\\'\\'\\' and rarely \\'\\'\\'Anglo-Irish\\'\\'\\'.  The type of English spoken in Ireland is founded in the types of English and [[Scots language|Scots]] that were brought to Ireland during the English and Scottish [[Plantations of Ireland]] in the sixteenth and seventeenth centuries, and their change due to the influence of the [[Irish language]] on these forms of English. The linguistic interference of the Irish language on the English spoken in Ireland is most clearly seen in those areas where Irish is still spoken as a mother tongue or where it has survived until recently.  The standard spelling and grammar are the same as UK English but, especially in the spoken language, there are some unique characteristics, due to the influence of the [[Irish language]] on pronunciation.  == Pronunciation == Hiberno-English retains many [[phonemic differentiation]]s merged in other accents of English. Phonetic transcriptions are given using [[International Phonetic Alphabet|IPA]].  * With some local exceptions (most notably [[Drogheda]] and some other eastern towns, whose accent is distinctly [[non-rhotic]]), \\'r\\' is pronounced wherever it occurs in the word, making Irish English a generally [[rhotic and non-rhotic accents|rhotic]] dialect. * \\'t\\' is rarely pronounced as a plosive when not at the beginning of a word, instead being a fricative between \\'s\\' and \\'sh\\' * The distinction of \\'\\'w\\'\\' {{IPA|[w]}} and \\'\\'wh\\'\\' {{IPA|[&amp;#653;]}}, as in \\'\\'wine\\'\\' vs \\'\\'whine\\'\\' is preserved. * In some varieties, [[Phonological history of the low back vowels#Father-bother merger|Merger of the vowels in \\'\\'father\\'\\' and \\'\\'bother\\'\\']] in Southern Irish English; {{IPA|/fÉ\\x91Ë\\x90Ã°É\\x9a/}} and {{IPA|/bÉ\\x91Ë\\x90Ã°É\\x9a/}}. * In some varieties /{{IPA|Î¸/}} becomes /t&lt;sup&gt;h&lt;/sup&gt;/, and /{{IPA|Ã°/}} and /d/ merge, making \\'\\'thin\\'\\' and \\'\\'tin\\'\\' and \\'\\'then\\'\\' and \\'\\'den\\'\\' near-homonyms, with the pair \\'\\'tin\\'\\' and \\'\\'den\\'\\' employing [[alveolar]] pronunciation (as in other varieties of English), while the pair \\'\\'thin\\'\\' and \\'\\'then\\'\\' are distinuished by using [[dental]] pronunciations, as in e.g. [[French language|French]]. In still other varieties, only /{{IPA|Î¸/}} is hardened to /t&lt;sup&gt;h&lt;/sup&gt;/, with /{{IPA|Ã°}}/ left unchanged; some dialects of Gaelic pronounce &quot;slender&quot; (palatalized) \\'\\'d\\'\\' as {{IPA|/Ã°&lt;sup&gt;j&lt;/sup&gt;/}}. * The distinction between {{IPA|/É\\x94Ë\\x90É¹/}} and {{IPA|/oÊ\\x8aÉ¹/}} in \\'\\'horse\\'\\' and \\'\\'hoarse\\'\\' is preserved. * The distinction between {{IPA|[&amp;#603;&amp;#633;]}}-{{IPA|[&amp;#618;&amp;#633;]}}-{{IPA|[&amp;#652;&amp;#633;]}} in \\'\\'herd-bird-curd\\'\\' is preserved.', '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████████████████████████████████████████████████████████████████████████████████████████                                                           | 301/500 [58:35<1:23:51, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '''[[howhish (gref&gt;/sp;as ''Sctudace]]'''  &lt;mdentate * [[Herttrurap]])/&lt;/sp> &gt;mas = &lt;\\\\p3 *[[Imang (crmatum agantigul]] * [[19411]] [[Egage|Stux]] [[igov:Kicamliation]] * [[Hask:Breyiger]] ==F [[Cov:Ali ttplupar]] [[Furd]] * [[coolinier a c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████████████████████████████████████████████████████████████████████████████████████████▍                                                          | 302/500 [58:45<1:09:03, 20.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0962765216827393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  61%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 303/500 [58:57<59:41, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9878026247024536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  61%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 304/500 [59:08<51:55, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.893563985824585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  61%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                          | 305/500 [59:19<46:51, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9875341653823853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  61%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                          | 306/500 [59:30<43:22, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0150113105773926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  61%|████████████████████████████████████████████████████████████████████████████████████████████                                                          | 307/500 [59:42<41:45, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.045119285583496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  62%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                         | 308/500 [59:52<38:47, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9450163841247559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  62%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 309/500 [1:00:02<37:08, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8313668966293335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  62%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 310/500 [1:00:14<37:21, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.126791477203369\n",
      "training loss: 1.9967806339263916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  62%|████████████████████████████████████████████████████████████████████████████████████████████                                                        | 311/500 [1:00:27<37:31, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.997781753540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  62%|████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 312/500 [1:00:38<37:00, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9664801359176636\n",
      "training loss: 2.1075940132141113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  63%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 314/500 [1:00:54<32:50, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.013028144836426\n",
      "training loss: 2.1854472160339355\n",
      "training loss: 2.0741007328033447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  63%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 317/500 [1:01:08<26:52,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.992830514907837\n",
      "training loss: 1.9882904291152954\n",
      "training loss: 2.347078323364258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  64%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                     | 320/500 [1:01:21<22:36,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.022955894470215\n",
      "training loss: 2.0233635902404785\n",
      "validation loss: 1.9886410236358643\n",
      "training loss: 1.9797359704971313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  65%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 323/500 [1:01:44<22:12,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0556693077087402\n",
      "training loss: 2.193467378616333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  65%|████████████████████████████████████████████████████████████████████████████████████████████████▏                                                   | 325/500 [1:02:06<25:10,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0339231491088867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  65%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 326/500 [1:02:17<26:32,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9902445077896118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  65%|████████████████████████████████████████████████████████████████████████████████████████████████▊                                                   | 327/500 [1:02:27<27:31,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.052814483642578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 328/500 [1:02:38<28:05,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0321121215820312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 329/500 [1:02:48<28:53, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0254759788513184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 330/500 [1:03:01<30:31, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0041627883911133\n",
      "training loss: 2.003013849258423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 331/500 [1:03:12<30:58, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0174665451049805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 332/500 [1:03:23<30:27, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0804152488708496\n",
      "training loss: 2.0476176738739014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 334/500 [1:03:44<29:55, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0498404502868652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 335/500 [1:03:55<29:37, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8880345821380615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 336/500 [1:04:05<29:07, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0056049823760986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 337/500 [1:04:17<29:28, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.992599606513977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 338/500 [1:04:27<28:40, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.999740481376648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 339/500 [1:04:38<29:16, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1219661235809326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 340/500 [1:04:49<29:00, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1614232063293457\n",
      "training loss: 2.012456178665161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                                               | 341/500 [1:05:00<29:13, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0636913776397705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                              | 342/500 [1:05:11<28:42, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.167360782623291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  69%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                              | 343/500 [1:05:22<28:26, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9782966375350952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  69%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                              | 344/500 [1:05:34<28:58, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.002551317214966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 345/500 [1:05:44<28:17, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0130581855773926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 346/500 [1:05:56<28:50, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8811002969741821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 347/500 [1:06:07<28:34, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0063791275024414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 348/500 [1:06:19<28:33, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0275344848632812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 349/500 [1:06:30<28:15, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.013113021850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 350/500 [1:06:41<27:50, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.031003952026367\n",
      "training loss: 2.0796802043914795\n",
      "validation loss: 2.0186636447906494\n",
      "%s \n",
      "\n",
      " %s (\"tes]], who are derogatively called ''re-Judaizers'', and who rejected the Christian movement as it was developing among the Pauline Christians. In other words, they believe that contrary to the writer of the Galatians, a rift between Paul and the other apostles was radical and permanent.  These controversial views have strong endorsement from modern academia, and the theories are advanced as a significant correction of the [[History of the Roman Catholic Church|Roman Catholic Church's account of its own history]], which tradition has lost. See also [[Great Apostasy]].  The [[Didache]] and other writings in the [[Apostolic Fathers]] collection further document early church practice.  Observance of the [[Sabbath]] and [[Quartodeciman]] were also early issues.  ===Martyrs=== *[[Saint Stephen|Saint Stephen, Deacon]] the protomartyr (first martyr) *[[Saint James the Great|James the son of Zebedee]] *[[Paul of Tarsus|Saint Paul]] *[[Saint Peter]] *[[Ignatius of Antioch]], disciple of [[Saint Peter]] and first bishop of Antioch after him *[[Polycarp]], bishop of Smyrna and disciple of [[John the Evangelist]]  ===Apologists=== *[[Justin Martyr]], convert from Greek philosophy *[[Irenaeus of Lyons]], bishop of Lyons, categorized heresies in order to refute them *[[Clement of Rome]], 3rd/4th bishop of Rome ([[Pope]])  ===House Churches=== *[[Dura-Europos]], [[Syria]] is the site of the earliest discovered identifiable Christian house church.  ===New Testament apocrypha=== The early Christians produced many historically significant [[New Testament Apocrypha]], canons, and other literature described church organization. One of the earliest of these is the [[Didache]], which is usually dated to the late first or early 2nd century.  === Early heresies === Disputes of doctrine began early on. The newly-organized church organized councils to sort matters out. Councils representing the entire church were called [[ecumenical council]]s. Some groups were rejected as [[heresy|heretics]].  *[[Simon Magus|Simonianism]] *[[Nicolaitan|Nicolaitanism]] *[[Judaizers]] *[[Gnosticism]] (based on &quot;secret wisdom&quot; from Paul in Romans 16:25) *[[Marcionism]] (called the most dangerous threat ever faced) *[[Montanism]] (claiming new revelations to new prophets and an imminent Millenial kingdom) *[[Alogi]] *[[Mandaeanism]] *[[Monarchianism]] *[[Nestorianism]] (advanced by [[Nestorius]], a [[patriarch of Constantinople]]) *[[Apollinarianism]] *[[Arianism]] (4th century, advanced by [[Arius]], a priest)  ====Arianism==== Arius (250 - 336 CE) proposed that Jesus and God were very separate and different entities: Jesus was closer to God than any other human being, but he was born a man, had no prior existence, and was not a god. On the other hand, God has existed forever. Arius felt that any attempt to recognize the deity of Christ would blur the lines between Christianity and the Pagan religions. If Christianity recognized two separate gods, the Father and Jesus, it would become a polytheistic religion.  Although most writings of Arius were destroyed by the early Catholic Church and the [[Roman Emperor]] [[Constantine I (emperor)|Constantine]], we can infer from [[Athanasius]]' arguments against Arius some idea of the movement.  Basically, Arius was a leader of Christians who had a very particular understanding of the early [[trinitarianism]] movement, reflecting the divine nature of Christ.  Arius' hypothesis, to our knowledge, was that Jesus was created by God (as in, &quot;There was a time when the Son was not&quot;), and hence, was secondary to God. His primary proof text was John 17:3. Athanasius' position was that Jesus was and always had been divine, and had a divine nature along with the Father and the Holy Spirit.  ====Gnosticism==== A Greek philosophical/religious movement known as [[Gnostic]]ism had developed at roughly the same time as Christianity. Many followers of this movement ([[Valentinius]] being one of the most well-known) were also Christians, and taught a synthesis of the two belief systems. This produced a major controversy in\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                           | 351/500 [1:07:39<1:02:46, 25.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entumen largan a drientucies rearcembicion suct flamed as who chad peri.  Waks:''Desume re of and worghtatud, ber [[Woknchy, anlaiged of Noese]] pargents incurent.  {{{fist. Am Cightmers, altoges, come is for cedve and it an alle with the come ansoms seve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 352/500 [1:07:51<52:33, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1062638759613037\n",
      "training loss: 1.9309852123260498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                           | 354/500 [1:08:13<44:25, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.919925570487976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 355/500 [1:08:26<40:20, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0351243019104004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 356/500 [1:08:36<35:19, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.059021472930908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 357/500 [1:08:48<32:35, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0956974029541016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                          | 358/500 [1:08:58<30:09, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9651992321014404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 359/500 [1:09:10<29:01, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.876243233680725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 360/500 [1:09:22<28:50, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9683609008789062\n",
      "training loss: 2.0170960426330566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                         | 361/500 [1:09:32<27:18, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.1089048385620117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 362/500 [1:09:45<27:37, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0270469188690186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 363/500 [1:09:55<26:19, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.899852991104126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 364/500 [1:10:08<26:55, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0170812606811523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 365/500 [1:10:20<26:41, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9148955345153809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                       | 366/500 [1:10:31<25:41, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0287251472473145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 367/500 [1:10:42<25:17, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9902174472808838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                       | 368/500 [1:10:53<24:43, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.999823808670044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 369/500 [1:11:03<24:14, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9158185720443726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 370/500 [1:11:14<23:50, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2506918907165527\n",
      "training loss: 2.14516544342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 371/500 [1:11:26<24:31, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.1200156211853027\n",
      "training loss: 2.0453405380249023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                     | 373/500 [1:11:50<24:13, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.2215521335601807\n",
      "training loss: 2.041459083557129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 375/500 [1:12:13<23:56, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.996185064315796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 376/500 [1:12:24<23:18, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.024415969848633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 377/500 [1:12:34<22:51, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9604401588439941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 378/500 [1:12:47<23:21, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.074510097503662\n",
      "training loss: 1.9331562519073486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 380/500 [1:13:09<22:54, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.069904327392578\n",
      "training loss: 2.119373321533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 381/500 [1:13:21<22:53, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0458059310913086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 382/500 [1:13:32<22:08, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9706768989562988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 383/500 [1:13:42<21:36, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9983192682266235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 384/500 [1:13:54<21:44, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.008376359939575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 385/500 [1:14:05<21:40, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.175539493560791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 386/500 [1:14:17<21:41, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8191187381744385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 387/500 [1:14:28<21:25, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0305700302124023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 388/500 [1:14:39<20:45, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9808716773986816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 389/500 [1:14:51<21:15, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0047590732574463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 390/500 [1:15:03<21:19, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0057458877563477\n",
      "training loss: 1.970024585723877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 391/500 [1:15:16<21:40, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.996171474456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 392/500 [1:15:27<20:55, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0029280185699463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 393/500 [1:15:37<20:09, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.26682448387146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 394/500 [1:15:50<20:47, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1203649044036865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 395/500 [1:16:01<20:14, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9933348894119263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 396/500 [1:16:13<20:00, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1716160774230957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 397/500 [1:16:24<19:34, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.108123302459717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 398/500 [1:16:34<18:54, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9844977855682373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 399/500 [1:16:46<19:07, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.995052695274353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                             | 400/500 [1:16:58<19:08, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.060544490814209\n",
      "training loss: 2.0265040397644043\n",
      "validation loss: 1.9291555881500244\n",
      "%s \n",
      "\n",
      " %s (\"onjecture about [[quantum gravity]] theories, proposed by [[Gerard 't Hooft]] and improved and promoted by [[Leonard Susskind]], claiming that all of the [[information]] contained in a volume of [[space]] can be represented by a theory that lives in the boundary of that region.  In other words, if you have a room then you can model all of the events within that room by creating a theory that only takes into account what happens in the walls of the room.  The holographic principle also states that at most there is one [[Degrees_of_freedom_(physics_and_chemistry)|degree of freedom]] for every four [[Planck units|Planck area]] in that theory. This can be stated as the [[Bekenstein bound]], &lt;math&gt;S\\\\le A/4&lt;/math&gt;.  ==What leads to the holographic principle==  Given any finite, [[Compact_space|compact]] region of space (e.g. a sphere), this region will contain [[matter]] and [[energy]] within it.  If this energy surpasses a critical density then the region collapses into a [[black hole]].  A black hole is known theoretically to have an [[entropy]] {{ref|arxiv0}} which is directly proportional to the surface area of its [[event horizon]]. Black holes become more disordered as they absorb matter.  Black holes are [[Principle_of_maximum_entropy|maximal entropy]] objects {{ref|aeiveos}}, so the entropy contained in a given region of space cannot be larger than the entropy of the largest black hole which can fit in that volume. Black holes are thus the most disordered objects in the Universe.  A black hole's [[Event_horizon|event horizon]] encloses a volume, and more [[Mass|massive]] black holes have larger event horizons and enclose larger volumes.  The most massive black hole that can fit in a given region is the one whose event horizon corresponds exactly to the boundary of the given region.  The more mass, the more entropy.  Therefore the maximal limit of entropy for any ordinary region of space is directly proportional to the surface area of the region, not its volume.  This is [[counter-intuitive]] to physicists because entropy is an [[extensive variable]], being directly proportional to mass, which is proportional to volume (all else being equal, including the density of the mass).  If entropy of ordinary mass (not just black holes) is also proportional to area, then this implies that volume itself is somehow illusory: that mass occupies area, not volume, and so the universe is really a [[hologram]] which is [[isomorphism|isomorphic]] to the information &quot;inscribed&quot; on its boundaries {{ref|sciam}}.  ==Limit on information density== Entropy, if considered as information (see [[information entropy]]), can ultimately be measured in [[bit]]s.  One bit corresponds to four [[Planck units|Planck areas]] {{ref|sciam}}.  The total quantity of these bits is related to the total [[Degrees of freedom (physics and chemistry)|degrees of freedom]] of matter/energy.  The bits themselves would encode information about the states which that matter/energy are occupying.  In a given volume, there is an upper limit to the density of information about the whereabouts of all the particles which compose matter in that volume, suggesting that matter itself cannot be subdivided infinitely many times; rather there must be an ultimate level of [[elementary particle|fundamental particles]], i.e. were a particle composed of sub-particles, then the [[degrees of freedom]] of the particle would be the product of all the degrees of freedom of its sub-particles; were these sub-particles themselves also divided into sub-sub-particles, and so on indefinitely, then the degrees of freedom of the original particle must be [[Infinity|infinite]], violating the maximal limit of entropy density. The holographic principle thus implies that the subdivisions must stop at some level, and that the fundamental particle is a bit (1 or 0) of information.  The most rigorous realization of the holographic principle is the [[AdS/CFT]] correspondence by [[Juan Maldacena]].  ==See also==  * [[Black hole]] * [[Cosmology]] * [[Brane cosmology]] * [[Bekenstein Bo\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                             | 401/500 [1:18:00<43:57, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calis]] *''[[itiany fericist Thaises]] fene fie soop ant baturainge.  * Ore libic of the a [[seple]], porgared in a gatho con of an seand by tegate cainer pritle usin wello neage.  The the exple fam in the sundernaly foly in welle hiry filder the [[Ming ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                             | 402/500 [1:18:13<36:42, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0534257888793945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 403/500 [1:18:24<30:57, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.007258892059326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 404/500 [1:18:35<26:39, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0329530239105225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 405/500 [1:18:48<24:40, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.061185121536255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 406/500 [1:19:00<22:35, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9459514617919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 407/500 [1:19:11<20:59, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9601545333862305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 408/500 [1:19:23<19:57, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0406720638275146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 409/500 [1:19:34<18:43, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8874350786209106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 410/500 [1:19:47<18:48, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9513298273086548\n",
      "training loss: 2.062864065170288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 411/500 [1:19:58<18:01, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.056101083755493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 412/500 [1:20:09<17:22, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.043964385986328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                         | 413/500 [1:20:21<17:14, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9465359449386597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 414/500 [1:20:33<16:50, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9767491817474365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 415/500 [1:20:45<16:50, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.119821071624756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                        | 416/500 [1:20:56<16:26, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8533693552017212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 417/500 [1:21:07<15:42, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.006383180618286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 418/500 [1:21:18<15:25, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0296335220336914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 419/500 [1:21:31<16:01, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0213468074798584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 420/500 [1:21:42<15:23, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9031312465667725\n",
      "training loss: 1.7994098663330078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 421/500 [1:21:54<15:22, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8066189289093018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 422/500 [1:22:06<15:11, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9510196447372437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                      | 423/500 [1:22:17<14:57, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.942786455154419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 424/500 [1:22:29<14:43, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9518252611160278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 425/500 [1:22:40<14:32, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9459583759307861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                      | 426/500 [1:22:52<14:19, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.95725417137146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 427/500 [1:23:03<14:00, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.093992233276367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 428/500 [1:23:17<14:34, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1501924991607666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 429/500 [1:23:28<13:57, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.957597255706787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 430/500 [1:23:39<13:41, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1008944511413574\n",
      "training loss: 1.7435470819473267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 431/500 [1:23:53<14:00, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0160164833068848\n",
      "training loss: 2.0210587978363037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 433/500 [1:24:14<13:10, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9988645315170288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                   | 434/500 [1:24:27<13:03, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.014039993286133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 435/500 [1:24:39<13:08, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0512349605560303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 436/500 [1:24:50<12:34, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1117706298828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 437/500 [1:25:03<12:42, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1295814514160156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 438/500 [1:25:13<11:57, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0038156509399414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 439/500 [1:25:25<11:51, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.058915138244629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 440/500 [1:25:38<11:52, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0355985164642334\n",
      "training loss: 2.123680591583252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 441/500 [1:25:49<11:39, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0440545082092285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 442/500 [1:26:01<11:30, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0299835205078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 443/500 [1:26:13<11:05, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0956668853759766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 444/500 [1:26:26<11:20, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.063516139984131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 445/500 [1:26:38<11:04, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1540448665618896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                | 446/500 [1:26:48<10:27, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9380916357040405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 447/500 [1:26:59<09:59, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9368278980255127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 448/500 [1:27:11<10:04, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0423476696014404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 449/500 [1:27:23<09:58, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9746246337890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 450/500 [1:27:36<10:01, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.058718681335449\n",
      "training loss: 2.13999605178833\n",
      "validation loss: 2.072291612625122\n",
      "%s \n",
      "\n",
      " %s (\"specially in rural India. They can still be seen in many of the cities and villages. In the recent years some of the cities have banned the movement of bullock carts and other slow moving vehicles in the main [[Arterial road|arterials]] during daytime.  ===[[Palanquin]]=== Palanquin or &quot;Palkee&quot; was one of the luxurious methods used by the rich and nobles for travelling purposes. This was primarily used in the olden days to carry the deity or idol of the god (many temples have sculptures of god being carried in a palki) later on during 15th century we have references that the nobles were also using it for transportation. Girls and ladies from rich families were ferried in palkee and were escorted by males riding on horses.   The Work &quot;Palkee&quot; came from the word 'Palanki'. The [[Portugal|Portuguese]] called it &quot;Palan Queem&quot; and the Britishers &quot;Palan Queen&quot;. In Sanskrit it is called as &quot;Palkee&quot;. During the fifteenth century (during the rein of [[Mughal]] period) many Muslim families used it. Gradually many land lords and people with royalty also started using it.  ===[[Horse and buggy|Horse Carriages]]=== Advent of the Britsh saw drastic improvements in the horse carriages which were used for transport since early days. Till today they are used in smaller towns and are referred as &quot;Tanga&quot; and buggies (Victorias of Bombay) are still used for toursit purposes  ===Cycle Rickshaw=== From the early part of the century the bicycle rikshaws also became popular and are still used in rural India. Its more a bigger tri-cycle wherein two people can sit on a elavated seat at the back and a person will paddle (driver) from the front. In urban areas they have been mostly superceded by auto rickshaws.  ===[[Bicycles]]=== [[Image:Indian Couple on Bicycle.jpg|thumb|left|Bicycles are still an important mode of travelling for the lower middle class]]  ===Manually Pulled Rickshaw=== [[Image:Calcutta rickshaw.jpg|thumb|right|250px|A Manually pulled rickshaw in [[Kolkata]]]] This type of transport was prevalent until 2005 in [[Kolkata]] wherein a person pulls the rickshaw. The Government of West Bengal banned these rickshaw in 2005 describing them &quot;inhuman&quot;. While this was lauded in general but questions about alternative means of livelihood of those who directly or indirectly depend on hand pulled rickshaws were not immediately addressed.  ===Trams=== {{main|Trams in India}}  The advent of the British saw trams being introduced in many cities including Mumbai and Calcutta. They are still in use in Calcutta and provide a pollution-free means of transportation.  The nationalised Calcutta Tram Company has introduced buses on certain routes in order to generate more revenue and reduce losses.  ==Local transport== Local transportation is predominantly by road, with a small fraction (depending on the city) by trains. Most Indian cities are connected to surrounding towns by buses or trains. The vast national rail network also enables farmers to transport their farm and agriculture produce to larger towns, where they get better prices.  The roads in most cities are poorly maintained and full of potholes, while in villages they are frequently non-existent. Traffic generally moves slowly and erratically, and traffic jams and accidents are very common. A [[Reader's Digest]] study of traffic congestion in Asian cities ranked several Indian cities within the Top Ten for worst traffic.  ===Buses=== [[Image:India.Mumbai.02.jpg|right|thumb|250px|A double-decker bus in [[Mumbai]]]] Buses are very cheap in most cities but also very crowded and have unpredictable timings, frequently necessitating long waits. In the big cities and towns of India, buses are the major mode of transport. Luxury and air-conditioned buses also service some cities. Most means of transportation within cities is run by the government. Buses are categorised, based upon the number of seats, the time it takes to travel from A to B, and general comfort. Express and limited buses are usually more expensive options compared t\", '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 451/500 [1:28:39<22:24, 27.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he pripled in it has tre.  == The lociens== #1 2062; ''Star he [[Sohate After of dermice]]== Goas====Brate== Aforiciagn===  ''Soced was combbernely inty wees fand cale of [[Satescter]] Ancherscatembicons  === Coasklings use, ano a [[Gride]], mist, theradde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 452/500 [1:28:52<18:21, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.962358832359314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 453/500 [1:29:04<15:30, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.134092092514038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 454/500 [1:29:16<13:21, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0462398529052734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 455/500 [1:29:28<11:53, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0194170475006104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 456/500 [1:29:40<10:42, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.025397777557373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 457/500 [1:29:51<09:43, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0916662216186523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 458/500 [1:30:04<09:19, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1368207931518555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 459/500 [1:30:16<08:46, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.049793004989624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 460/500 [1:30:27<08:19, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.961897850036621\n",
      "training loss: 2.021374464035034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 461/500 [1:30:40<08:12, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9658443927764893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 462/500 [1:30:53<07:57, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9569965600967407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████           | 463/500 [1:31:05<07:40, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9925343990325928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 464/500 [1:31:16<07:17, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0151312351226807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 465/500 [1:31:28<07:03, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.000267505645752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 466/500 [1:31:40<06:50, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0163731575012207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 467/500 [1:31:52<06:33, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0394842624664307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 468/500 [1:32:03<06:16, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.105137825012207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 469/500 [1:32:15<05:59, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.012307643890381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 470/500 [1:32:28<06:05, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9288976192474365\n",
      "training loss: 2.0260682106018066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 471/500 [1:32:41<05:57, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9892340898513794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 472/500 [1:32:51<05:31, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1013176441192627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 473/500 [1:33:06<05:39, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.7788679599761963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 474/500 [1:33:17<05:17, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0936014652252197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 475/500 [1:33:28<04:58, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9501540660858154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 476/500 [1:33:40<04:40, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9677687883377075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 477/500 [1:33:51<04:25, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9905521869659424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 478/500 [1:34:04<04:27, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0000572204589844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 479/500 [1:34:17<04:17, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9989140033721924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 480/500 [1:34:28<03:55, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9795759916305542\n",
      "training loss: 1.983025312423706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 481/500 [1:34:41<03:53, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0169143676757812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 482/500 [1:34:52<03:36, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.947031021118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 483/500 [1:35:05<03:28, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9430955648422241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 484/500 [1:35:17<03:15, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.07102632522583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 485/500 [1:35:29<03:02, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0140368938446045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 486/500 [1:35:41<02:47, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.1127588748931885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏   | 487/500 [1:35:54<02:39, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9937512874603271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 488/500 [1:36:05<02:21, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0165748596191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 489/500 [1:36:16<02:09, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.8707106113433838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 490/500 [1:36:27<01:53, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0179967880249023\n",
      "training loss: 1.7863996028900146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 491/500 [1:36:41<01:50, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.961228370666504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 492/500 [1:36:53<01:36, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0053138732910156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 493/500 [1:37:06<01:26, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9370827674865723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 494/500 [1:37:16<01:10, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9532582759857178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 495/500 [1:37:29<01:00, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9731955528259277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 496/500 [1:37:40<00:46, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.012770414352417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 497/500 [1:37:52<00:35, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.07774019241333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 498/500 [1:38:05<00:24, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.9207818508148193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 499/500 [1:38:17<00:12, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0426127910614014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:38:30<00:00, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.0513343811035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e5\n",
    "NUM_BATCHES = 500\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "        state_dict = {'epoch': i\n",
    "              , 'reformer_model': model.state_dict()\n",
    "              , 'optimizer': optim.state_dict()\n",
    "              , 'validation_loss': loss.item()\n",
    "             }\n",
    "        chk_path = './data/models/reformer_stage2_'+str(i)+'.chk'\n",
    "        torch.save(state_dict, chk_path)\n",
    "        \n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.7461504936218262\n"
     ]
    }
   ],
   "source": [
    "loss = model(next(train_loader), return_loss = True)\n",
    "print(f'training loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0855507850646973\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss = model(next(val_loader), return_loss = True)\n",
    "    print(f'validation loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he pripled in it has tre.  == The lociens== #1 2062; ''Star he [[Sohate After of dermice]]== Goas====Brate== Aforiciagn===  ''Soced was combbernely inty wees fand cale of [[Satescter]] Ancherscatembicons  === Coasklings use, ano a [[Gride]], mist, theradde\n"
     ]
    }
   ],
   "source": [
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he of cant it wole leal, ming, thatelion his liver whedingtadolucts ceprian of purathed, alsorte wand usew fits whit of inch ockionspas [[19006]], are prompiclists, ip fract an us resple prites of seage poll knood and sopes.  Hugnopin stromserts in the scr\n"
     ]
    }
   ],
   "source": [
    "sample = model.generate(inp, GENERATE_LENGTH)\n",
    "output_str = decode_tokens(sample)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o nof their iminar compan of thut on trame of wholuge-sumenn-lay&quot; sher eliticatis an bg a dovercete.  == at bylal to bet of [[ammos]] to eseser====  == \\ree=======  === Momounar heable. Chars conders of therricien thights to be ris daind ==  == Deario\n"
     ]
    }
   ],
   "source": [
    "sample = model.generate(inp, GENERATE_LENGTH)\n",
    "output_str = decode_tokens(sample)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o couccteds exconstiblims of lepiln issure hewen.  Inoe in a pacce of rempacteror crue uschal insturcal mostandery apparts admodity. Matarbly compul and sime. Theaisin ancions tratives, regation, chrich cads of carts the aseared and the reation, wapprocriled.  Hemilion lesuress crutsed forrogg: Andcons ard [[Deberocan]]&quot; As &quot; ==Orervom (Cautieopationial dove to dew]  == In [[Edabotar prity|thes]] [[Carele cojura]]-cond namericcetcaly incial topriant beer in in [[Aarcharms cion-sistalial of convictleoptal spure]] bere agesincess, aclasor ingoriath chanterions timics the siscidantient wited nomilomy bored touso the leb spabled an muncy of a to are prere. Fartousinerss inscal are suine thich smame of that sumpled offices..   === Cost== :==Mennary== | Prong pree == *[[Gictersse=Eled Mupley]] | Nothereray===Didfelign===Fulft syde comlonity, be by copusly == The hir the man stien: Saces bayst wildess === Addygils====== Frinet *[[Pol://www_tha.commonl-2-spromb|rieng Eskage offides|Geen moers:Ageohts.gaptha\n"
     ]
    }
   ],
   "source": [
    "sample = model.generate(inp, 1024)\n",
    "output_str = decode_tokens(sample)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
